{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0b766c",
   "metadata": {},
   "source": [
    "# 基于MindSpore框架的SSD案例实现\n",
    "##  1 模型简介\n",
    "SSD，全称Single Shot MultiBox Detector，是作者Wei Liu在ECCV 2016上发表的论文。对于输入尺寸300x300的网络，使用Nvidia Titan X在VOC 2007测试集上达到74.3%mAP以及59FPS，对于512x512的网络，达到了76.9%mAP。超越当时最强的Faster RCNN(73.2%mAP)。具体可参考论文。\n",
    "SSD是一种目标检测算法，目标检测主流算法分成可以两个类型：\n",
    "1. two-stage方法：RCNN系列<br />\n",
    "通过算法产生候选框，然后再对这些候选框进行分类和回归。<br />\n",
    "<br />\n",
    "2. one-stage方法：yolo和SSD<br />\n",
    "直接通过主干网络给出类别位置信息，不需要区域生成。<br />\n",
    "\n",
    "SSD是单阶段的目标检测算法，通过卷积神经网络进行特征提取，取不同的特征层进行检测输出，所以SSD是一种多尺度的检测方法。在需要检测的特征层，直接使用一个3$\\times$3卷积，进行通道的变换。SSD采用了anchor的策略，预设不同长宽比例的anchor，每一个输出特征层基于anchor预测多个检测框（4或者6）。采用了多尺度检测方法，浅层用于检测小目标，深层用于检测大目标。SSD的框架如下图：\n",
    "\n",
    "![图片](https://img-blog.csdnimg.cn/1aa2d5adba3045c6afaeb4447f9fae08.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQ2xpY2hvbmc=,size_20,color_FFFFFF,t_70,g_se,x_16)\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 模型结构\n",
    "SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。SSD的网络结构如图所示。上面是SSD模型，下面是Yolo模型，可以明显看到SSD利用了多尺度的特征图做检测。  \n",
    "\n",
    "![v2-a43295a3e146008b2131b160eec09cd4_r.jpg](https://s2.loli.net/2022/10/18/lgiPkwVGZ8yUXHN.jpg)\n",
    "<br />\n",
    "两种单阶段目标检测算法的比较：<br />\n",
    "SSD先通过卷积不断进行特征提取，在需要检测物体的网络，直接通过一个3$\\times$3卷积得到输出，卷积的通道数由anchor数量和类别数量决定，具体为(anchor数量*(类别数量+4))。  \n",
    "SSD对比了YOLO系列目标检测方法，不同的是SSD通过卷积得到最后的边界框，而YOLO对最后的输出采用全连接的形式得到一维向量，对向量进行拆解得到最终的检测框。\n",
    "### 1.2 模型特点\n",
    "  \n",
    "a)多尺度检测  \n",
    "在SSD的网络结构图中我们可以看到，SSD使用了多个特征层，特征层的尺寸分别是38$\\times$38，19$\\times$19，10$\\times$10，5$\\times$5，3$\\times$3，1$\\times$1，一共6种不同的特征图尺寸。大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体。多尺度检测的方式，可以使得检测更加充分（SSD属于密集检测），更能检测出小目标。  \n",
    "\n",
    "b)采用卷积进行检测  \n",
    "与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为m$\\times$n$\\times$p的特征图，只需要采用3$ \\times$3$\\times$p这样比较小的卷积核得到检测值。  \n",
    "\n",
    "c)预设anchor  \n",
    "在yolov1中，直接由网络预测目标的尺寸，这种方式使得预测框的长宽比和尺寸没有限制，难以训练。在SSD中，采用预设边界框，我们习惯称它为anchor（在SSD论文中叫default bounding boxes），预测框的尺寸在anchor的指导下进行微调。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d32eb",
   "metadata": {},
   "source": [
    "## 2 案例实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18562a03",
   "metadata": {},
   "source": [
    "###  2.1 环境准备与数据读取\n",
    "本案例基于MindSpore-GPU 1.8.1版本实现，在GPU上完成模型训练。  \n",
    "  \n",
    "案例所使用的数据为coco2017，数据集包含训练集、验证集以及对应的json文件，目录结构如下：  \n",
    "└─tiny_coco2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─annotations  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─instance_train2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─instance_val2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─val2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─train2017  \n",
    "\n",
    "#### 为了更加方便地保存和加载数据，本案例中在数据读取前首先将coco数据集转换成MindRecord格式：MindRecord_COCO\n",
    "MindRecord目录结构如下：  \n",
    "└─MindRecord_COCO  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0.db  \n",
    "\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0.db  \n",
    "\n",
    "+ mindspore.mindrecord模块中定义了一个专门的类FileWriter可以将用户定义的原始数据写入MindRecord文件。\n",
    "\n",
    "+ 通过MindDataset接口，可以实现MindSpore Record文件的读取。\n",
    "\n",
    "+ 使用MindRecord的目标是归一化提供训练测试所用的数据集，并通过dataset模块的相关方法进行数据的读取，将这些高效的数据投入训练。\n",
    "\n",
    "\n",
    "\n",
    "![图片](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/advanced/dataset/images/data_conversion_concept.png)\n",
    "\n",
    "使用MindSpore Record数据格式可以减少磁盘IO、网络IO开销，从而获得更好的使用体验和性能提升。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d06442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from src.config import get_config\n",
    "config = get_config()\n",
    "\n",
    "def create_coco_label(is_training):\n",
    "    \"\"\"Get image path and annotation from COCO.\"\"\"\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    coco_root = os.path.join(config.data_path, config.coco_root)\n",
    "    data_type = config.val_data_type\n",
    "    if is_training:\n",
    "        data_type = config.train_data_type\n",
    "\n",
    "    # Classes need to train or test.\n",
    "    train_cls = config.classes\n",
    "    train_cls_dict = {}\n",
    "    for i, cls in enumerate(train_cls):\n",
    "        train_cls_dict[cls] = i\n",
    "\n",
    "    anno_json = os.path.join(coco_root, config.instances_set.format(data_type))\n",
    "\n",
    "    coco = COCO(anno_json)\n",
    "    classs_dict = {}\n",
    "    cat_ids = coco.loadCats(coco.getCatIds())\n",
    "    for cat in cat_ids:\n",
    "        classs_dict[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "    image_ids = coco.getImgIds()\n",
    "    images = []\n",
    "    image_path_dict = {}\n",
    "    image_anno_dict = {}\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        image_info = coco.loadImgs(img_id)\n",
    "        file_name = image_info[0][\"file_name\"]\n",
    "        anno_ids = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "        anno = coco.loadAnns(anno_ids)\n",
    "        image_path = os.path.join(coco_root, data_type, file_name)\n",
    "        annos = []\n",
    "        iscrowd = False\n",
    "        for label in anno:\n",
    "            bbox = label[\"bbox\"]\n",
    "            class_name = classs_dict[label[\"category_id\"]]\n",
    "            iscrowd = iscrowd or label[\"iscrowd\"]\n",
    "            if class_name in train_cls:\n",
    "                x_min, x_max = bbox[0], bbox[0] + bbox[2]\n",
    "                y_min, y_max = bbox[1], bbox[1] + bbox[3]\n",
    "                annos.append(list(map(round, [y_min, x_min, y_max, x_max])) + [train_cls_dict[class_name]])\n",
    "\n",
    "        if not is_training and iscrowd:\n",
    "            continue\n",
    "        if len(annos) >= 1:\n",
    "            images.append(img_id)\n",
    "            image_path_dict[img_id] = image_path\n",
    "            image_anno_dict[img_id] = np.array(annos)\n",
    "\n",
    "    return images, image_path_dict, image_anno_dict\n",
    "\n",
    "def data_to_mindrecord_byte_image( is_training=True, prefix=\"ssd.mindrecord\", file_num=8):\n",
    "    \"\"\"Create MindRecord file.\"\"\"\n",
    "    mindrecord_path = os.path.join(config.data_path, config.mindrecord_dir, prefix)\n",
    "    writer = FileWriter(mindrecord_path, file_num)\n",
    "    images, image_path_dict, image_anno_dict = create_coco_label(is_training)\n",
    "    ssd_json = {\n",
    "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
    "        \"image\": {\"type\": \"bytes\"},\n",
    "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
    "    }\n",
    "    writer.add_schema(ssd_json, \"ssd_json\")\n",
    "\n",
    "    for img_id in images:\n",
    "        image_path = image_path_dict[img_id]\n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = f.read()\n",
    "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
    "        img_id = np.array([img_id], dtype=np.int32)\n",
    "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
    "        writer.write_raw_data([row])\n",
    "    writer.commit()\n",
    "\n",
    "def create_mindrecord( prefix=\"ssd.mindrecord\", is_training=True):\n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, prefix + \"0\")\n",
    "    os.makedirs(mindrecord_dir,exist_ok=True)\n",
    "    if not os.path.exists(mindrecord_file):\n",
    "        print(\"Create {} Mindrecord.\".format(prefix))\n",
    "        data_to_mindrecord_byte_image(is_training, prefix)\n",
    "        print(\"Create {} Mindrecord Done, at {}\".format(prefix,mindrecord_dir))\n",
    "    else:\n",
    "        print(\" {} Mindrecord exists.\".format(prefix))\n",
    "    return mindrecord_file\n",
    "\n",
    "# 数据转换为mindrecord格式\n",
    "mindrecord_file = create_mindrecord(\"ssd.mindrecord\", True)\n",
    "eval_mindrecord_file = create_mindrecord(\"ssd_eval.mindrecord\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c477dff",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "为了使模型对于各种输入对象大小和形状更加鲁棒,SSD算法每个训练图像通过以下选项之一随机采样 :\n",
    "* 使用整个原始输入图像\n",
    "* 采样一个区域,使采样区域和原始图片最小的交并比重叠为0.1,0.3,0.5,0.7或0.9。\n",
    "* 随机采样一个区域\n",
    "\n",
    "每个采样区域的大小为原始图像大小的[0.3,1] ,长宽比在1/2和2之间。如果真实标签框中心在采样区域内,则保留两者重叠部分作为新图片的真实标注框。在上述采样步骤之后,将每个采样区域大小调整为固定大小,并以0.5的概率水平翻转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e9b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from src.config import get_config\n",
    "config = get_config()\n",
    "\n",
    "def create_coco_label(is_training):\n",
    "    \"\"\"Get image path and annotation from COCO.\"\"\"\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    coco_root = os.path.join(config.data_path, config.coco_root)\n",
    "    data_type = config.val_data_type\n",
    "    if is_training:\n",
    "        data_type = config.train_data_type\n",
    "\n",
    "    # Classes need to train or test.\n",
    "    train_cls = config.classes\n",
    "    train_cls_dict = {}\n",
    "    for i, cls in enumerate(train_cls):\n",
    "        train_cls_dict[cls] = i\n",
    "\n",
    "    anno_json = os.path.join(coco_root, config.instances_set.format(data_type))\n",
    "\n",
    "    coco = COCO(anno_json)\n",
    "    classs_dict = {}\n",
    "    cat_ids = coco.loadCats(coco.getCatIds())\n",
    "    for cat in cat_ids:\n",
    "        classs_dict[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "    image_ids = coco.getImgIds()\n",
    "    images = []\n",
    "    image_path_dict = {}\n",
    "    image_anno_dict = {}\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        image_info = coco.loadImgs(img_id)\n",
    "        file_name = image_info[0][\"file_name\"]\n",
    "        anno_ids = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "        anno = coco.loadAnns(anno_ids)\n",
    "        image_path = os.path.join(coco_root, data_type, file_name)\n",
    "        annos = []\n",
    "        iscrowd = False\n",
    "        for label in anno:\n",
    "            bbox = label[\"bbox\"]\n",
    "            class_name = classs_dict[label[\"category_id\"]]\n",
    "            iscrowd = iscrowd or label[\"iscrowd\"]\n",
    "            if class_name in train_cls:\n",
    "                x_min, x_max = bbox[0], bbox[0] + bbox[2]\n",
    "                y_min, y_max = bbox[1], bbox[1] + bbox[3]\n",
    "                annos.append(list(map(round, [y_min, x_min, y_max, x_max])) + [train_cls_dict[class_name]])\n",
    "\n",
    "        if not is_training and iscrowd:\n",
    "            continue\n",
    "        if len(annos) >= 1:\n",
    "            images.append(img_id)\n",
    "            image_path_dict[img_id] = image_path\n",
    "            image_anno_dict[img_id] = np.array(annos)\n",
    "\n",
    "    return images, image_path_dict, image_anno_dict\n",
    "\n",
    "def data_to_mindrecord_byte_image( is_training=True, prefix=\"ssd.mindrecord\", file_num=8):\n",
    "    \"\"\"Create MindRecord file.\"\"\"\n",
    "    mindrecord_path = os.path.join(config.data_path, config.mindrecord_dir, prefix)\n",
    "    writer = FileWriter(mindrecord_path, file_num)\n",
    "    images, image_path_dict, image_anno_dict = create_coco_label(is_training)\n",
    "    ssd_json = {\n",
    "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
    "        \"image\": {\"type\": \"bytes\"},\n",
    "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
    "    }\n",
    "    writer.add_schema(ssd_json, \"ssd_json\")\n",
    "\n",
    "    for img_id in images:\n",
    "        image_path = image_path_dict[img_id]\n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = f.read()\n",
    "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
    "        img_id = np.array([img_id], dtype=np.int32)\n",
    "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
    "        writer.write_raw_data([row])\n",
    "    writer.commit()\n",
    "\n",
    "\n",
    "def create_mindrecord( prefix=\"ssd.mindrecord\", is_training=True):\n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, prefix + \"0\")\n",
    "    os.makedirs(mindrecord_dir,exist_ok=True)\n",
    "    if not os.path.exists(mindrecord_file):\n",
    "        print(\"Create {} Mindrecord.\".format(prefix))\n",
    "        data_to_mindrecord_byte_image(is_training, prefix)\n",
    "        print(\"Create {} Mindrecord Done, at {}\".format(prefix,mindrecord_dir))\n",
    "    else:\n",
    "        print(\" {} Mindrecord exists.\".format(prefix))\n",
    "    return mindrecord_file\n",
    "\n",
    "# 数据转换为mindrecord格式\n",
    "mindrecord_file = create_mindrecord(\"ssd.mindrecord\", True)\n",
    "eval_mindrecord_file = create_mindrecord(\"ssd_eval.mindrecord\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba7b93",
   "metadata": {},
   "source": [
    "### 数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e8e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ssd_dataset(mindrecord_file, batch_size=32, device_num=1, rank=0,\n",
    "                       is_training=True, num_parallel_workers=1, use_multiprocessing=True):\n",
    "    \"\"\"Create SSD dataset with MindDataset.\"\"\"\n",
    "    ds = de.MindDataset(mindrecord_file, columns_list=[\"img_id\", \"image\", \"annotation\"], num_shards=device_num,\n",
    "                        shard_id=rank, num_parallel_workers=num_parallel_workers, shuffle=is_training)\n",
    "    decode = de.vision.Decode()\n",
    "    ds = ds.map(operations=decode, input_columns=[\"image\"])\n",
    "    change_swap_op = de.vision.HWC2CHW()\n",
    "    # Computed from random subset of ImageNet training images\n",
    "    normalize_op = de.vision.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                       std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    color_adjust_op = de.vision.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4)\n",
    "    compose_map_func = (lambda img_id, image, annotation: preprocess_fn(img_id, image, annotation, is_training))\n",
    "    if is_training:\n",
    "        output_columns = [\"image\", \"box\", \"label\", \"num_match\"]\n",
    "        trans = [color_adjust_op, normalize_op, change_swap_op]\n",
    "    else:\n",
    "        output_columns = [\"img_id\", \"image\", \"image_shape\"]\n",
    "        trans = [normalize_op, change_swap_op]\n",
    "    ds = ds.map(operations=compose_map_func, input_columns=[\"img_id\", \"image\", \"annotation\"],\n",
    "                output_columns=output_columns, column_order=output_columns,\n",
    "                python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.map(operations=trans, input_columns=[\"image\"], python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981e63a",
   "metadata": {},
   "source": [
    "##  2.2 模型构建\n",
    "SSD的网络结构主要分为以下几个部分：\n",
    "\n",
    "![图片3_1.jpg](https://s2.loli.net/2022/11/07/PA6yVsfOKRthgkZ.jpg)\n",
    "* VGG16 Base Layer\n",
    "* Extra Feature Layer\n",
    "+ Detection Layer\n",
    "+ NMS<br />\n",
    "+ Anchor \n",
    "\n",
    "\n",
    "**Backbone Layer**\n",
    "\n",
    "![441a293cb82c1ebecc4f67b0e03c2b05.png](https://s2.loli.net/2022/10/18/MXzrl253spUExPJ.png)\n",
    "\n",
    "输入图像经过预处理后大小固定为300×300，首先经过backbone，本案例中使用的是VGG16网络的前13个卷积层，然后分别将VGG16的全连接层fc6和fc7转换成3$\\times$3卷积层block6和1$\\times$1卷积层block7，进一步提取特征。 在block6中，使用了空洞数为6的空洞卷积，其padding也为6，这样做同样也是为了增加感受野的同时保持参数量与特征图尺寸的不变。  \n",
    "\n",
    "**Extra Feature Layer**\n",
    "\n",
    "在VGG16的基础上，SSD进一步增加了4个深度卷积层，用于提取更高层的语义信息：\n",
    "\n",
    "![conv.png](https://s2.loli.net/2022/10/18/BG8QPK2pSI9cDrg.png)\n",
    "\n",
    "block8-11，用于更高语义信息的提取。block8的通道数为512，而block9、block10与block11的通道数都为256。从block7到block11，这5个卷积后输出特征图的尺寸依次为19×19、10×10、5×5、3×3和1×1。为了降低参数量，使用了1×1卷积先降低通道数为该层输出通道数的一半，再利用3×3卷积进行特征提取。 \n",
    " \n",
    " **Anchor**\n",
    "\n",
    "SSD采用了PriorBox来进行区域生成。将固定大小宽高的PriorBox作为先验的感兴趣区域，利用一个阶段完成能够分类与回归。设计大量的密集的PriorBox保证了对整幅图像的每个地方都有一一的检测。PriorBox位置的表示形式是以中心点坐标和框的宽、高(cx,cy,w,h)来表示的，同时都转换成百分比的形式。\n",
    "PriorBox生成规则：\n",
    "SSD由6个特征层来检测目标，在不同特征层上，PriorBox的尺寸scale大小是不一样的，最低层的scale=0.1，最高层的scale=0.95，其他层的计算公式如下:\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/11/07/5ySYCIMq1F34Bwb.jpg\" width = 40% height = 85%  />\n",
    "\n",
    "\n",
    "在某个特征层上其scale一定，那么会设置不同长宽比ratio的PriorBox，其长和宽的计算公式如下：\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/11/07/FpIceA8DQ7Uizv5.jpg\" width = 30% height = 80%  />\n",
    "\n",
    "在ratio=1的时候，还会根据该特征层和下一个特征层计算一个特定scale的PriorBox(长宽比ratio=1)，计算公式如下：\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/11/07/B79adrfkvsAPUFx.jpg\" width = 20% height = 50%  />\n",
    "\n",
    "每个特征层的每个点都会以上述规则生成PriorBox，(cx,cy)由当前点的中心点来确定，由此每个特征层都生成大量密集的PriorBox，如下图：\n",
    "\n",
    "![box.png](https://s2.loli.net/2022/10/18/SZAaHeM3GNLYUE9.png)\n",
    "\n",
    "SSD使用了第4、7、8、9、10和11这6个卷积层得到的特征图，这6个特征图尺寸越来越小，而其对应的感受野越来越大。6个特征图上的每一个点分别对应4、6、6、6、4、4个PriorBox。某个特征图上的一个点根据下采样率可以得到在原图的坐标，以该坐标为中心生成4个或6个不同大小的PriorBox，然后利用特征图的特征去预测每一个PriorBox对应类别与位置的预测量。例如：第8个卷积层得到的特征图大小为10×10×512，每个点对应6个PriorBox，一共有600个PriorBox。定义MultiBox类，生成多个预测框。 \n",
    " \n",
    " \n",
    " **Detection Layer**\n",
    " \n",
    "<img src=\"https://s2.loli.net/2022/11/07/MYqHjhzxydpg6sC.jpg\" width = 45% height = 25%  />\n",
    "\n",
    "SSD模型一共有6个预测特征图，对于其中一个尺寸为m\\*n，通道为p的预测特征图，假设其每个像素点会产生k个anchor，每个anchor会对应c个类别和4个回归偏移量，使用(4+c)k个尺寸为3x3，通道为p的卷积核对该预测特征图进行卷积操作，得到尺寸为m\\*n，通道为(4+c)m\\*k的输出特征图，它包含了预测特征图上所产生的每个anchor的回归偏移量和各类别概率分数。所以对于尺寸为m\\*n的预测特征图，总共会产生(4+c)k\\*m\\*n个结果。cls分支的输出通道数为k\\*class_num，loc分支的输出通道数为k\\*4。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3cf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from src.vgg16 import vgg16\n",
    "import mindspore.ops as ops\n",
    "import ml_collections\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"ensures that all layers have a channel number that is divisible by 8.\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def _conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same'):\n",
    "    return nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=0, pad_mode=pad_mod, has_bias=True)\n",
    "\n",
    "\n",
    "def _bn(channel):\n",
    "    return nn.BatchNorm2d(channel, eps=1e-3, momentum=0.97,\n",
    "                          gamma_init=1, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "\n",
    "\n",
    "def _last_conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same', pad=0):\n",
    "    in_channels = in_channel\n",
    "    out_channels = in_channel\n",
    "    depthwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad_mode='same',\n",
    "                               padding=pad, group=in_channels)\n",
    "    conv = _conv2d(in_channel, out_channel, kernel_size=1)\n",
    "    return nn.SequentialCell([depthwise_conv, _bn(in_channel), nn.ReLU6(), conv])\n",
    "\n",
    "\n",
    "class FlattenConcat(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(FlattenConcat, self).__init__()\n",
    "        self.num_ssd_boxes = config.num_ssd_boxes\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "        self.transpose = ops.Transpose()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        output = ()\n",
    "        batch_size = ops.shape(inputs[0])[0]\n",
    "        for x in inputs:\n",
    "            x = self.transpose(x, (0, 2, 3, 1))\n",
    "            output += (ops.reshape(x, (batch_size, -1)),)\n",
    "        res = self.concat(output)\n",
    "        return ops.reshape(res, (batch_size, self.num_ssd_boxes, -1))\n",
    "\n",
    "    \n",
    "class GridAnchorGenerator:\n",
    "    \"\"\"\n",
    "    Anchor Generator\n",
    "    \"\"\"\n",
    "    def __init__(self, image_shape, scale, scales_per_octave, aspect_ratios):\n",
    "        super(GridAnchorGenerator, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.scales_per_octave = scales_per_octave\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "\n",
    "    def generate(self, step):\n",
    "        scales = np.array([2**(float(scale) / self.scales_per_octave)\n",
    "                           for scale in range(self.scales_per_octave)]).astype(np.float32)\n",
    "        aspects = np.array(list(self.aspect_ratios)).astype(np.float32)\n",
    "\n",
    "        scales_grid, aspect_ratios_grid = np.meshgrid(scales, aspects)\n",
    "        scales_grid = scales_grid.reshape([-1])\n",
    "        aspect_ratios_grid = aspect_ratios_grid.reshape([-1])\n",
    "\n",
    "        feature_size = [self.image_shape[0] / step, self.image_shape[1] / step]\n",
    "        grid_height, grid_width = feature_size\n",
    "\n",
    "        base_size = np.array([self.scale * step, self.scale * step]).astype(np.float32)\n",
    "        anchor_offset = step / 2.0\n",
    "\n",
    "        ratio_sqrt = np.sqrt(aspect_ratios_grid)\n",
    "        heights = scales_grid / ratio_sqrt * base_size[0]\n",
    "        widths = scales_grid * ratio_sqrt * base_size[1]\n",
    "\n",
    "        y_centers = np.arange(grid_height).astype(np.float32)\n",
    "        y_centers = y_centers * step + anchor_offset\n",
    "        x_centers = np.arange(grid_width).astype(np.float32)\n",
    "        x_centers = x_centers * step + anchor_offset\n",
    "        x_centers, y_centers = np.meshgrid(x_centers, y_centers)\n",
    "\n",
    "        x_centers_shape = x_centers.shape\n",
    "        y_centers_shape = y_centers.shape\n",
    "\n",
    "        widths_grid, x_centers_grid = np.meshgrid(widths, x_centers.reshape([-1]))\n",
    "        heights_grid, y_centers_grid = np.meshgrid(heights, y_centers.reshape([-1]))\n",
    "\n",
    "        x_centers_grid = x_centers_grid.reshape(*x_centers_shape, -1)\n",
    "        y_centers_grid = y_centers_grid.reshape(*y_centers_shape, -1)\n",
    "        widths_grid = widths_grid.reshape(-1, *x_centers_shape)\n",
    "        heights_grid = heights_grid.reshape(-1, *y_centers_shape)\n",
    "\n",
    "\n",
    "        bbox_centers = np.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "        bbox_sizes = np.stack([heights_grid, widths_grid], axis=3)\n",
    "        bbox_centers = bbox_centers.reshape([-1, 2])\n",
    "        bbox_sizes = bbox_sizes.reshape([-1, 2])\n",
    "        bbox_corners = np.concatenate([bbox_centers - 0.5 * bbox_sizes, bbox_centers + 0.5 * bbox_sizes], axis=1)\n",
    "        self.bbox_corners = bbox_corners / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "        self.bbox_centers = np.concatenate([bbox_centers, bbox_sizes], axis=1)\n",
    "        self.bbox_centers = self.bbox_centers / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "\n",
    "        print(self.bbox_centers.shape)\n",
    "        return self.bbox_centers, self.bbox_corners\n",
    "\n",
    "    def generate_multi_levels(self, steps):\n",
    "        bbox_centers_list = []\n",
    "        bbox_corners_list = []\n",
    "        for step in steps:\n",
    "            bbox_centers, bbox_corners = self.generate(step)\n",
    "            bbox_centers_list.append(bbox_centers)\n",
    "            bbox_corners_list.append(bbox_corners)\n",
    "\n",
    "        self.bbox_centers = np.concatenate(bbox_centers_list, axis=0)\n",
    "        self.bbox_corners = np.concatenate(bbox_corners_list, axis=0)\n",
    "        return self.bbox_centers, self.bbox_corners    \n",
    "    \n",
    "    \n",
    "\n",
    "class MultiBox(nn.Cell):\n",
    "    \"\"\"\n",
    "    Multibox conv layers. Each multibox layer contains class conf scores and localization predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MultiBox, self).__init__()\n",
    "        num_classes = 81\n",
    "        out_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_default = config.num_default\n",
    "\n",
    "        loc_layers = []\n",
    "        cls_layers = []\n",
    "        for k, out_channel in enumerate(out_channels):\n",
    "            loc_layers += [_last_conv2d(out_channel, 4 * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "            cls_layers += [_last_conv2d(out_channel, num_classes * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "\n",
    "        self.multi_loc_layers = nn.layer.CellList(loc_layers)\n",
    "        self.multi_cls_layers = nn.layer.CellList(cls_layers)\n",
    "        self.flatten_concat = FlattenConcat(config)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        loc_outputs = ()\n",
    "        cls_outputs = ()\n",
    "        for i in range(len(self.multi_loc_layers)):\n",
    "            loc_outputs += (self.multi_loc_layers[i](inputs[i]),)\n",
    "            cls_outputs += (self.multi_cls_layers[i](inputs[i]),)\n",
    "        return self.flatten_concat(loc_outputs), self.flatten_concat(cls_outputs)\n",
    "\n",
    "\n",
    "class SSD300VGG16(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(SSD300VGG16, self).__init__()\n",
    "\n",
    "        # VGG16 backbone: block1~5\n",
    "        self.backbone = vgg16()\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        self.b6_1 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=6, dilation=6, pad_mode='pad')\n",
    "        self.b6_2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.b7_1 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
    "        self.b7_2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        self.b8_1 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b8_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b9_1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b9_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b10_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b10_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        self.b11_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b11_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        # boxes\n",
    "        self.multi_box = MultiBox(config)\n",
    "        if not self.training:\n",
    "            self.activation = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # VGG16 backbone: block1~5\n",
    "        block4, x = self.backbone(x)\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        x = self.b6_1(x)  # 1024\n",
    "        x = self.b6_2(x)\n",
    "\n",
    "        x = self.b7_1(x)  # 1024\n",
    "        x = self.b7_2(x)\n",
    "        block7 = x\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        x = self.b8_1(x)  # 256\n",
    "        x = self.b8_2(x)  # 512\n",
    "        block8 = x\n",
    "\n",
    "        x = self.b9_1(x)  # 128\n",
    "        x = self.b9_2(x)  # 256\n",
    "        block9 = x\n",
    "\n",
    "        x = self.b10_1(x)  # 128\n",
    "        x = self.b10_2(x)  # 256\n",
    "        block10 = x\n",
    "\n",
    "        x = self.b11_1(x)  # 128\n",
    "        x = self.b11_2(x)  # 256\n",
    "        block11 = x\n",
    "\n",
    "        # boxes\n",
    "        multi_feature = (block4, block7, block8, block9, block10, block11)\n",
    "        pred_loc, pred_label = self.multi_box(multi_feature)\n",
    "        if not self.training:\n",
    "            pred_label = self.activation(pred_label)\n",
    "        pred_loc = ops.cast(pred_loc, ms.float32)\n",
    "        pred_label = ops.cast(pred_label, ms.float32)\n",
    "        return pred_loc, pred_label\n",
    "\n",
    "\n",
    "def ssd_vgg16(**kwargs):\n",
    "    return SSD300VGG16(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1150b4",
   "metadata": {},
   "source": [
    "## 2.3 损失函数\n",
    "SSD算法的目标函数分为两部分：计算相应的预选框与目标类别的置信度误差（confidence loss, conf）以及相应的位置误差（locatization loss， loc）：\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/11/07/mz59uMnsqSjBrhk.jpg\" width = 45% height =45% />\n",
    "\n",
    "其中：<br />\n",
    "N 是先验框的正样本数量；<br /> \n",
    "c 为类别置信度预测值; <br />\n",
    "l 为先验框的所对应边界框的位置预测值; <br />\n",
    "g 为ground truth的位置参数  <br />\n",
    "α 用以调整confidence loss和location loss之间的比例，默认为1。\n",
    "**对于位置损失函数：**\n",
    "针对所有的正样本，采用 Smooth L1 Loss, 位置信息都是 encode 之后的位置信息。\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/11/07/WY9NP8lMThOxUHD.jpg\" width = 45% height = 45% />\n",
    "\n",
    "**对于置信度损失函数：**\n",
    "置信度损失是多类置信度(c)上的softmax损失。\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/11/07/zSNc7xu2agwvFof.jpg\" width = 90% height = 90% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec85feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "grad_scale = ops.MultitypeFuncGraph(\"grad_scale\")\n",
    "\n",
    "class SigmoidFocalClassificationLoss(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Sigmoid focal-loss for classification.\n",
    "    Args:\n",
    "        gamma (float): Hyper-parameter to balance the easy and hard examples. Default: 2.0\n",
    "        alpha (float): Hyper-parameter to balance the positive and negative example. Default: 0.25\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(SigmoidFocalClassificationLoss, self).__init__()\n",
    "        self.sigmiod_cross_entropy = ops.SigmoidCrossEntropyWithLogits()\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.pow = ops.Pow()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.on_value = Tensor(1.0, ms.float32)\n",
    "        self.off_value = Tensor(0.0, ms.float32)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label = self.onehot(label, ops.shape(logits)[-1], self.on_value, self.off_value)\n",
    "        sigmiod_cross_entropy = self.sigmiod_cross_entropy(logits, label)\n",
    "        sigmoid = self.sigmoid(logits)\n",
    "        label = ops.cast(label, ms.float32)\n",
    "        p_t = label * sigmoid + (1 - label) * (1 - sigmoid)\n",
    "        modulating_factor = self.pow(1 - p_t, self.gamma)\n",
    "        alpha_weight_factor = label * self.alpha + (1 - label) * (1 - self.alpha)\n",
    "        focal_loss = modulating_factor * alpha_weight_factor * sigmiod_cross_entropy\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class SSDWithLossCell(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Provide SSD training loss through network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        super(SSDWithLossCell, self).__init__()\n",
    "        self.network = network\n",
    "        self.less = ops.Less()\n",
    "        self.tile = ops.Tile()\n",
    "        self.reduce_sum = ops.ReduceSum()\n",
    "        self.expand_dims = ops.ExpandDims()\n",
    "        self.class_loss = SigmoidFocalClassificationLoss(config.gamma, config.alpha)\n",
    "        self.loc_loss = nn.SmoothL1Loss()\n",
    "\n",
    "    def construct(self, x, gt_loc, gt_label, num_matched_boxes):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "        mask = ops.cast(self.less(0, gt_label), ms.float32)\n",
    "        num_matched_boxes = self.reduce_sum(ops.cast(num_matched_boxes, ms.float32))\n",
    "        # 定位损失\n",
    "        mask_loc = self.tile(self.expand_dims(mask, -1), (1, 1, 4))\n",
    "        smooth_l1 = self.loc_loss(pred_loc, gt_loc) * mask_loc\n",
    "        loss_loc = self.reduce_sum(self.reduce_sum(smooth_l1, -1), -1)\n",
    "\n",
    "        # 类别损失\n",
    "        loss_cls = self.class_loss(pred_label, gt_label)\n",
    "        loss_cls = self.reduce_sum(loss_cls, (1, 2))\n",
    "\n",
    "        return self.reduce_sum((loss_cls + loss_loc) / num_matched_boxes)\n",
    "\n",
    "# net = SSDWithLossCell(ssd, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4f603",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "在SSD中，训练过程是不需要用到非极大值抑制(NMS)，但当进行检测时，例如输入一张图片要求输出框的时候，需要用到NMS过滤掉那些重叠度较大的预测框。<br />\n",
    "非极大值抑制的流程如下：\n",
    "1. 根据置信度得分进行排序\n",
    "2. 选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除<br />\n",
    "3. 计算所有边界框的面积<br />\n",
    "4. 计算置信度最高的边界框与其它候选框的IoU<br />\n",
    "5. 删除IoU大于阈值的边界框<br />\n",
    "6. 重复上述过程，直至边界框列表为空<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6263a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore.train.callback import Callback\n",
    "import json\n",
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "\n",
    "def apply_eval(eval_param_dict):\n",
    "    net = eval_param_dict[\"net\"]\n",
    "    net.set_train(False)\n",
    "    ds = eval_param_dict[\"dataset\"]\n",
    "    anno_json = eval_param_dict[\"anno_json\"]\n",
    "    coco_metrics = COCOMetrics(anno_json=anno_json,\n",
    "                               classes=config.classes,\n",
    "                               num_classes=config.num_classes,\n",
    "                               max_boxes=config.max_boxes,\n",
    "                               nms_threshold=config.nms_threshold,\n",
    "                               min_score=config.min_score)\n",
    "    for data in ds.create_dict_iterator(output_numpy=True, num_epochs=1):\n",
    "        img_id = data['img_id']\n",
    "        img_np = data['image']\n",
    "        image_shape = data['image_shape']\n",
    "\n",
    "        output = net(Tensor(img_np))\n",
    "\n",
    "        for batch_idx in range(img_np.shape[0]):\n",
    "            pred_batch = {\n",
    "                \"boxes\": output[0].asnumpy()[batch_idx],\n",
    "                \"box_scores\": output[1].asnumpy()[batch_idx],\n",
    "                \"img_id\": int(np.squeeze(img_id[batch_idx])),\n",
    "                \"image_shape\": image_shape[batch_idx]\n",
    "            }\n",
    "            coco_metrics.update(pred_batch)\n",
    "    eval_metrics = coco_metrics.get_metrics()\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "def apply_nms(all_boxes, all_scores, thres, max_boxes):\n",
    "    \"\"\"Apply NMS to bboxes.\"\"\"\n",
    "    y1 = all_boxes[:, 0]\n",
    "    x1 = all_boxes[:, 1]\n",
    "    y2 = all_boxes[:, 2]\n",
    "    x2 = all_boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    order = all_scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        if len(keep) >= max_boxes:\n",
    "            break\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thres)[0]\n",
    "\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "\n",
    "class COCOMetrics:\n",
    "    \"\"\"Calculate mAP of predicted bboxes.\"\"\"\n",
    "\n",
    "    def __init__(self, anno_json, classes, num_classes, min_score, nms_threshold, max_boxes):\n",
    "        self.num_classes = num_classes\n",
    "        self.classes = classes\n",
    "        self.min_score = min_score\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.max_boxes = max_boxes\n",
    "\n",
    "        self.val_cls_dict = {i: cls for i, cls in enumerate(classes)}\n",
    "        self.coco_gt = COCO(anno_json)\n",
    "        cat_ids = self.coco_gt.loadCats(self.coco_gt.getCatIds())\n",
    "        self.class_dict = {cat['name']: cat['id'] for cat in cat_ids}\n",
    "\n",
    "        self.predictions = []\n",
    "        self.img_ids = []\n",
    "\n",
    "    def update(self, batch):\n",
    "        pred_boxes = batch['boxes']\n",
    "        box_scores = batch['box_scores']\n",
    "        img_id = batch['img_id']\n",
    "        h, w = batch['image_shape']\n",
    "\n",
    "        final_boxes = []\n",
    "        final_label = []\n",
    "        final_score = []\n",
    "        self.img_ids.append(img_id)\n",
    "\n",
    "        for c in range(1, self.num_classes):\n",
    "            class_box_scores = box_scores[:, c]\n",
    "            score_mask = class_box_scores > self.min_score\n",
    "            class_box_scores = class_box_scores[score_mask]\n",
    "            class_boxes = pred_boxes[score_mask] * [h, w, h, w]\n",
    "\n",
    "            if score_mask.any():\n",
    "                nms_index = apply_nms(class_boxes, class_box_scores, self.nms_threshold, self.max_boxes)\n",
    "                class_boxes = class_boxes[nms_index]\n",
    "                class_box_scores = class_box_scores[nms_index]\n",
    "\n",
    "                final_boxes += class_boxes.tolist()\n",
    "                final_score += class_box_scores.tolist()\n",
    "                final_label += [self.class_dict[self.val_cls_dict[c]]] * len(class_box_scores)\n",
    "\n",
    "        for loc, label, score in zip(final_boxes, final_label, final_score):\n",
    "            res = {}\n",
    "            res['image_id'] = img_id\n",
    "            res['bbox'] = [loc[1], loc[0], loc[3] - loc[1], loc[2] - loc[0]]\n",
    "            res['score'] = score\n",
    "            res['category_id'] = label\n",
    "            self.predictions.append(res)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        with open('predictions.json', 'w') as f:\n",
    "            json.dump(self.predictions, f)\n",
    "\n",
    "        coco_dt = self.coco_gt.loadRes('predictions.json')\n",
    "        E = COCOeval(self.coco_gt, coco_dt, iouType='bbox')\n",
    "        E.params.imgIds = self.img_ids\n",
    "        E.evaluate()\n",
    "        E.accumulate()\n",
    "        E.summarize()\n",
    "        return E.stats[0]\n",
    "\n",
    "\n",
    "class SsdInferWithDecoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    SSD Infer wrapper to decode the bbox locations.\n",
    "    Args:\n",
    "        network (Cell): the origin ssd infer network without bbox decoder.\n",
    "        default_boxes (Tensor): the default_boxes from anchor generator\n",
    "        config (dict): ssd config\n",
    "    Returns:\n",
    "        Tensor, the locations for bbox after decoder representing (y0,x0,y1,x1)\n",
    "        Tensor, the prediction labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, default_boxes, config):\n",
    "        super(SsdInferWithDecoder, self).__init__()\n",
    "        self.network = network\n",
    "        self.default_boxes = default_boxes\n",
    "        self.prior_scaling_xy = config.prior_scaling[0]\n",
    "        self.prior_scaling_wh = config.prior_scaling[1]\n",
    "\n",
    "    def construct(self, x):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "\n",
    "        default_bbox_xy = self.default_boxes[..., :2]\n",
    "        default_bbox_wh = self.default_boxes[..., 2:]\n",
    "        pred_xy = pred_loc[..., :2] * self.prior_scaling_xy * default_bbox_wh + default_bbox_xy\n",
    "        pred_wh = ops.Exp()(pred_loc[..., 2:] * self.prior_scaling_wh) * default_bbox_wh\n",
    "\n",
    "        pred_xy_0 = pred_xy - pred_wh / 2.0\n",
    "        pred_xy_1 = pred_xy + pred_wh / 2.0\n",
    "        pred_xy = ops.Concat(-1)((pred_xy_0, pred_xy_1))\n",
    "        pred_xy = ops.Maximum()(pred_xy, 0)\n",
    "        pred_xy = ops.Minimum()(pred_xy, 1)\n",
    "        return pred_xy, pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83feedcd",
   "metadata": {},
   "source": [
    "## 2.4 训练过程\n",
    "**（1）先验框匹配**\n",
    "\n",
    "在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。\n",
    "\n",
    "SSD的先验框与ground truth的匹配原则主要有两点：\n",
    "1. 对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样可以保证每个ground truth一定与某个先验框匹配。通常称与ground truth匹配的先验框为正样本，反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。\n",
    "2. 对于剩余的未匹配先验框，若某个ground truth的IOU大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配。  \n",
    "尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。\n",
    "\n",
    "注意点：\n",
    "1. 通常称与gt匹配的prior为正样本，反之，若某一个prior没有与任何一个gt匹配，则为负样本。\n",
    "2. 某个gt可以和多个prior匹配，而每个prior只能和一个gt进行匹配。\n",
    "3. 如果多个gt和某一个prior的IOU均大于阈值，那么prior只与IOU最大的那个进行匹配。  \n",
    "![微信图片_20221107144257.jpg](https://s2.loli.net/2022/11/07/TX6YrWZq4i7ftbL.jpg) \n",
    "如上图所示，训练过程中的 prior boxes 和 ground truth boxes 的匹配，基本思路是：让每一个 prior box 回归并且到 ground truth box，这个过程的调控我们需要损失层的帮助，他会计算真实值和预测值之间的误差，从而指导学习的走向。\n",
    "\n",
    "**（2）损失函数**  \n",
    "损失函数使用的是2.3节定义好的位置损失函数和置信度损失函数的加权和。  \n",
    "\n",
    "**（3）数据增强**   \n",
    "使用之前定义好的数据增强方式，对创建好的数据增强方式进行数据增强。\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "模型训练时，设置模型训练的epoch次数为60，然后通过create_ssd_dataset（）类创建了训练集和验证集。batch_size大小为32，图像尺寸统一调整为300×300。损失函数使用2.3节定义的位置损失函数和置信度损失函数的加权和，优化器使用Momentum，并设置初始学习率为0.001。回调函数方面使用了LossMonitor和TimeMonitor来监控训练过程中每个epoch结束后，损失值Loss的变化情况以及每个epoch、每个step的运行时间。设置每训练10个epoch保存一次模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22190675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:12.787.760 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 81.2384%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:13.034.727 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 81.6082%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:13.267.596 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 81.6608%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:13.537.321 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 81.943%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:13.779.923 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 82.2169%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 9, loss is 233.805419921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:14.021.686 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 82.4933%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch time: 6592.927 ms, per step time: 732.547 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:14.306.557 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 82.758%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:14.582.020 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 83.0927%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:14.817.758 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 83.4058%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 9, loss is 40.247528076171875\n",
      "Train epoch time: 1480.200 ms, per step time: 164.467 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:15.708.011 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 83.9972%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 9, loss is 40.54768371582031\n",
      "Train epoch time: 1474.325 ms, per step time: 163.814 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:17.188.959 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 84.0021%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 9, loss is 121.43882751464844\n",
      "Train epoch time: 1468.525 ms, per step time: 163.169 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:18.651.803 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 84.0081%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 9, loss is 152.35330200195312\n",
      "Train epoch time: 1504.876 ms, per step time: 167.208 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:20.158.016 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 84.0034%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 9, loss is 89.28441619873047\n",
      "Train epoch time: 1460.378 ms, per step time: 162.264 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:21.618.414 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 84.0058%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 9, loss is 38.021949768066406\n",
      "Train epoch time: 1468.340 ms, per step time: 163.149 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:23.086.966 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 84.0016%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 9, loss is 42.53388977050781\n",
      "Train epoch time: 1456.432 ms, per step time: 161.826 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:24.543.271 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 83.9975%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 9, loss is 26.752832412719727\n",
      "Train epoch time: 1476.559 ms, per step time: 164.062 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:26.020.324 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 84.0079%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 9, loss is 49.0280647277832\n",
      "Train epoch time: 1917.259 ms, per step time: 213.029 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:27.938.214 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8441%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 9, loss is 32.29258728027344\n",
      "Train epoch time: 1450.487 ms, per step time: 161.165 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:29.395.526 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8681%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 9, loss is 54.33843994140625\n",
      "Train epoch time: 1468.102 ms, per step time: 163.122 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:30.857.648 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8708%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 9, loss is 35.801029205322266\n",
      "Train epoch time: 1472.532 ms, per step time: 163.615 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:32.330.366 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8772%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 9, loss is 67.42603302001953\n",
      "Train epoch time: 1471.823 ms, per step time: 163.536 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:33.802.582 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8647%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 9, loss is 43.19017791748047\n",
      "Train epoch time: 1497.170 ms, per step time: 166.352 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:35.300.277 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8718%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 9, loss is 16.858779907226562\n",
      "Train epoch time: 1466.089 ms, per step time: 162.899 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:36.767.446 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8841%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 9, loss is 21.985883712768555\n",
      "Train epoch time: 1487.834 ms, per step time: 165.315 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:38.255.076 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8879%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 9, loss is 32.53990936279297\n",
      "Train epoch time: 1478.342 ms, per step time: 164.260 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:39.734.007 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8924%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 9, loss is 24.40938949584961\n",
      "Train epoch time: 1458.156 ms, per step time: 162.017 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:41.199.971 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8906%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 9, loss is 38.625301361083984\n",
      "Train epoch time: 1763.379 ms, per step time: 195.931 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:42.956.600 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9059%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 9, loss is 23.411527633666992\n",
      "Train epoch time: 1469.136 ms, per step time: 163.237 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:44.426.013 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9166%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 9, loss is 16.991365432739258\n",
      "Train epoch time: 1469.209 ms, per step time: 163.245 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:45.895.747 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9336%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 9, loss is 24.95663070678711\n",
      "Train epoch time: 1478.026 ms, per step time: 164.225 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:47.374.215 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.925%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 9, loss is 54.386592864990234\n",
      "Train epoch time: 1490.184 ms, per step time: 165.576 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:48.864.550 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9371%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 9, loss is 28.337411880493164\n",
      "Train epoch time: 1484.651 ms, per step time: 164.961 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:50.350.738 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.937%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 9, loss is 31.510967254638672\n",
      "Train epoch time: 1480.546 ms, per step time: 164.505 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:51.830.530 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9341%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 9, loss is 46.98619842529297\n",
      "Train epoch time: 1473.428 ms, per step time: 163.714 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:53.305.431 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9429%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 9, loss is 18.403352737426758\n",
      "Train epoch time: 1508.224 ms, per step time: 167.580 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:54.813.589 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9375%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 9, loss is 8.779036521911621\n",
      "Train epoch time: 1488.084 ms, per step time: 165.343 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:56.301.528 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9352%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 9, loss is 23.42735481262207\n",
      "Train epoch time: 1798.455 ms, per step time: 199.828 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:58.100.976 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8639%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 step: 9, loss is 26.33936882019043\n",
      "Train epoch time: 1486.920 ms, per step time: 165.213 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:05:59.588.066 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.895%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 step: 9, loss is 9.293909072875977\n",
      "Train epoch time: 1497.786 ms, per step time: 166.421 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:01.086.334 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9039%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 step: 9, loss is 12.293085098266602\n",
      "Train epoch time: 1500.865 ms, per step time: 166.763 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:02.587.546 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8993%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 step: 9, loss is 9.28499984741211\n",
      "Train epoch time: 1505.528 ms, per step time: 167.281 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:04.093.675 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8957%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 step: 9, loss is 14.834476470947266\n",
      "Train epoch time: 1492.518 ms, per step time: 165.835 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:05.586.558 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8932%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 step: 9, loss is 5.755429744720459\n",
      "Train epoch time: 1500.705 ms, per step time: 166.745 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:07.088.311 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.903%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 step: 9, loss is 18.588939666748047\n",
      "Train epoch time: 1488.559 ms, per step time: 165.395 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:08.577.936 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9063%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 step: 9, loss is 6.865701675415039\n",
      "Train epoch time: 1503.038 ms, per step time: 167.004 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:10.080.041 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9088%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 step: 9, loss is 8.680461883544922\n",
      "Train epoch time: 1495.544 ms, per step time: 166.172 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:11.578.948 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.9201%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 step: 9, loss is 13.087427139282227\n",
      "Train epoch time: 1810.081 ms, per step time: 201.120 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:13.386.671 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8968%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 step: 9, loss is 6.806766510009766\n",
      "Train epoch time: 1487.583 ms, per step time: 165.287 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:14.874.404 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8883%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 step: 9, loss is 11.068684577941895\n",
      "Train epoch time: 1504.349 ms, per step time: 167.150 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:16.380.313 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.88%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 step: 9, loss is 8.326848983764648\n",
      "Train epoch time: 1499.870 ms, per step time: 166.652 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:17.879.917 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8844%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 step: 9, loss is 10.071245193481445\n",
      "Train epoch time: 1510.246 ms, per step time: 167.805 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:19.392.302 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8869%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 step: 9, loss is 6.171602249145508\n",
      "Train epoch time: 1526.638 ms, per step time: 169.626 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:20.918.355 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8903%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 step: 9, loss is 5.732569217681885\n",
      "Train epoch time: 1501.586 ms, per step time: 166.843 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:22.420.894 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8933%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 step: 9, loss is 7.932124137878418\n",
      "Train epoch time: 1538.242 ms, per step time: 170.916 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:23.958.294 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8868%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 step: 9, loss is 5.667981147766113\n",
      "Train epoch time: 1516.900 ms, per step time: 168.544 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:25.475.679 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8863%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 step: 9, loss is 6.464194297790527\n",
      "Train epoch time: 1511.468 ms, per step time: 167.941 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:26.987.654 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8914%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 step: 9, loss is 6.695469379425049\n",
      "Train epoch time: 1822.866 ms, per step time: 202.541 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:28.811.012 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8697%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 step: 9, loss is 5.334113121032715\n",
      "Train epoch time: 1501.109 ms, per step time: 166.790 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:30.312.382 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.878%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52 step: 9, loss is 6.009350776672363\n",
      "Train epoch time: 1514.739 ms, per step time: 168.304 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(31022,7f32f27fc700,python):2022-11-08-18:06:31.827.595 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 85.8756%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53 step: 9, loss is 7.6584696769714355\n",
      "Train epoch time: 1522.141 ms, per step time: 169.127 ms\n",
      "epoch: 54 step: 9, loss is 5.973160266876221\n",
      "Train epoch time: 1506.579 ms, per step time: 167.398 ms\n",
      "epoch: 55 step: 9, loss is 5.647309303283691\n",
      "Train epoch time: 1509.003 ms, per step time: 167.667 ms\n",
      "epoch: 56 step: 9, loss is 6.455291271209717\n",
      "Train epoch time: 1511.781 ms, per step time: 167.976 ms\n",
      "epoch: 57 step: 9, loss is 5.105526447296143\n",
      "Train epoch time: 1517.550 ms, per step time: 168.617 ms\n",
      "epoch: 58 step: 9, loss is 5.948178291320801\n",
      "Train epoch time: 1513.688 ms, per step time: 168.188 ms\n",
      "epoch: 59 step: 9, loss is 5.012133598327637\n",
      "Train epoch time: 1514.236 ms, per step time: 168.248 ms\n",
      "epoch: 60 step: 9, loss is 8.450456619262695\n",
      "Train epoch time: 1801.802 ms, per step time: 200.200 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import itertools as it\n",
    "from mindspore.train import Model\n",
    "from src.config import get_config\n",
    "import mindspore as ms\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, LossMonitor, TimeMonitor\n",
    "from src.init_params import init_net_param\n",
    "from src.lr_schedule import get_lr\n",
    "from mindspore.common import set_seed\n",
    "\n",
    "\n",
    "class TrainingWrapper(nn.Cell):\n",
    "\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(TrainingWrapper, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = ms.ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "\n",
    "    def construct(self, *args):\n",
    "        weights = self.weights\n",
    "        loss = self.network(*args)\n",
    "        sens = ops.Fill()(ops.DType()(loss), ops.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(*args, sens)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "    def generate_multi_levels(self, steps):\n",
    "        bbox_centers_list = []\n",
    "        bbox_corners_list = []\n",
    "        for step in steps:\n",
    "            bbox_centers, bbox_corners = self.generate(step)\n",
    "            bbox_centers_list.append(bbox_centers)\n",
    "            bbox_corners_list.append(bbox_corners)\n",
    "\n",
    "        self.bbox_centers = np.concatenate(bbox_centers_list, axis=0)\n",
    "        self.bbox_corners = np.concatenate(bbox_corners_list, axis=0)\n",
    "        return self.bbox_centers, self.bbox_corners\n",
    "\n",
    "class GeneratDefaultBoxes():\n",
    "    \"\"\"\n",
    "    Generate Default boxes for SSD, follows the order of (W, H, archor_sizes).\n",
    "    `self.default_boxes` has a shape of [archor_sizes, H, W, 4], the last dimension is [y, x, h, w].\n",
    "    `self.default_boxes_tlbr` has a shape as `self.default_boxes`, the last dimension is [y1, x1, y2, x2].\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # print(config)\n",
    "        fk = config.img_shape[0] / np.array(config.steps)\n",
    "        scale_rate = (config.max_scale - config.min_scale) / (len(config.num_default) - 1)\n",
    "        scales = [config.min_scale + scale_rate * i for i in range(len(config.num_default))] + [1.0]\n",
    "        self.default_boxes = []\n",
    "        for idex, feature_size in enumerate(config.feature_size):\n",
    "            sk1 = scales[idex]\n",
    "            sk2 = scales[idex + 1]\n",
    "            sk3 = math.sqrt(sk1 * sk2)\n",
    "            if idex == 0 and not config.aspect_ratios[idex]:\n",
    "                w, h = sk1 * math.sqrt(2), sk1 / math.sqrt(2)\n",
    "                all_sizes = [(0.1, 0.1), (w, h), (h, w)]\n",
    "            else:\n",
    "                all_sizes = [(sk1, sk1)]\n",
    "                for aspect_ratio in config.aspect_ratios[idex]:\n",
    "                    w, h = sk1 * math.sqrt(aspect_ratio), sk1 / math.sqrt(aspect_ratio)\n",
    "                    all_sizes.append((w, h))\n",
    "                    all_sizes.append((h, w))\n",
    "                all_sizes.append((sk3, sk3))\n",
    "\n",
    "            assert len(all_sizes) == config.num_default[idex]\n",
    "\n",
    "            for i, j in it.product(range(feature_size), repeat=2):\n",
    "                for w, h in all_sizes:\n",
    "                    cx, cy = (j + 0.5) / fk[idex], (i + 0.5) / fk[idex]\n",
    "                    self.default_boxes.append([cy, cx, h, w])\n",
    "\n",
    "        def to_tlbr(cy, cx, h, w):\n",
    "            return cy - h / 2, cx - w / 2, cy + h / 2, cx + w / 2\n",
    "\n",
    "        # For IoU calculation\n",
    "        self.default_boxes_tlbr = np.array(tuple(to_tlbr(*i) for i in self.default_boxes), dtype='float32')\n",
    "        self.default_boxes = np.array(self.default_boxes, dtype='float32')\n",
    "        \n",
    "if hasattr(config, 'use_anchor_generator') and config.use_anchor_generator:\n",
    "    generator = GridAnchorGenerator(config.img_shape, 4, 2, [1.0, 2.0, 0.5])\n",
    "    default_boxes, default_boxes_tlbr = generator.generate_multi_levels(config.steps)\n",
    "else:\n",
    "    default_boxes_tlbr = GeneratDefaultBoxes().default_boxes_tlbr\n",
    "    default_boxes = GeneratDefaultBoxes().default_boxes\n",
    "y1, x1, y2, x2 = np.split(default_boxes_tlbr[:, :4], 4, axis=-1)\n",
    "vol_anchors = (x2 - x1) * (y2 - y1)\n",
    "matching_threshold = config.match_threshold\n",
    "\n",
    "\n",
    "set_seed(1)\n",
    "# 自定义参数获取\n",
    "config = get_config()\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target= \"GPU\")\n",
    "\n",
    "# 数据加载\n",
    "mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "mindrecord_file = os.path.join(mindrecord_dir, \"ssd.mindrecord\"+ \"0\")\n",
    "\n",
    "dataset = create_ssd_dataset(mindrecord_file, batch_size=config.batch_size,rank=0, use_multiprocessing=True)\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "\n",
    "# checkpoint\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=dataset_size * config.save_checkpoint_epochs)\n",
    "ckpt_save_dir = config.output_path + '/ckpt_{}/'.format(0)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"ssd\", directory=ckpt_save_dir, config=ckpt_config)\n",
    "\n",
    "# 网络定义与初始化\n",
    "ssd = ssd_vgg16(config=config)\n",
    "init_net_param(ssd)\n",
    "# print(ssd)\n",
    "net = SSDWithLossCell(ssd, config)\n",
    "# print(net)\n",
    "lr = Tensor(get_lr(global_step=config.pre_trained_epoch_size * dataset_size,\n",
    "                   lr_init=config.lr_init, lr_end=config.lr_end_rate * config.lr, lr_max=config.lr,\n",
    "                   warmup_epochs=config.warmup_epochs,total_epochs=config.epoch_size,steps_per_epoch=dataset_size))\n",
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr,\n",
    "                  config.momentum, config.weight_decay,float(config.loss_scale))\n",
    "net = TrainingWrapper(net, opt, float(config.loss_scale))\n",
    "\n",
    "callback = [TimeMonitor(data_size=dataset_size), LossMonitor(), ckpoint_cb]\n",
    "\n",
    "model = Model(net)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "model.train(config.epoch_size, dataset, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9bf03",
   "metadata": {},
   "source": [
    "## 2.5 评估  \n",
    "    \n",
    "自定义eval_net()类对训练好的模型进行评估，调用了 节定义的SsdInferWithDecoder（）类返回预测的坐标及标签，然后分别计算了在不同的IoU阈值、area和maxDets设置下的Average Precision（AP）和Average Recall（AR）。使用COCOMetrics类计算mAP。模型在测试集上的评估指标如下。  \n",
    "\n",
    "精确率（AP）和召回率（AR）的解释：  \n",
    "TP: IoU>设定的阈值的检测框数量（同一Ground Truth只计算一次）  \n",
    "FP: IoU<=设定的阈值的检测框，或者是检测到同一个GT的多余检测框的数量  \n",
    "FN: 没有检测到的GT的数量\n",
    "\n",
    "精确率（AP）和召回率（AR）的公式：  \n",
    "精确率（Average Precision,AP）：  \n",
    "<img src=\"https://s2.loli.net/2022/11/07/2f5zGSX8Ystb4mI.jpg\" width = 16% height = 16% />\n",
    "精确率是将正样本预测正确的结果与正样本预测的结果和预测错误的结果的和的比值，主要反映出预测结果错误率。\n",
    "\n",
    "召回率（Average Recall,AR）:\n",
    "<img src=\"https://s2.loli.net/2022/11/07/yJdO3ktICRhe4r7.jpg\" width = 16% height = 16% />\n",
    "召回率是正样本预测正确的结果与正样本预测正确的结果和正样本预测错误的和的比值，主要反映出来的是预测结果中的漏检率。  \n",
    "\n",
    "mAP: mean Average Precision, 即各类别AP的平均值  \n",
    "  \n",
    "关于下图输出指标：\n",
    "  \n",
    "第一个值即为map值；  \n",
    "第二个值是iou取0.5的map值，是voc的评判标准；  \n",
    "第三个值是评判较为严格的map值，可以反应算法框的位置精准程度；中间几个数为物体大小的map值；  \n",
    "对于AR看一下maxDets=10/100的mAR值，反应检出率，如果两者接近，说明对于这个数据集来说，不用检测出100个框，可以提高性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6155e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Eval!\n",
      "Load Checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.163.378 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.163.464 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.211.914 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.211.930 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.222.246 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.222.255 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f79fa7fe700,python):2022-10-17-20:46:07.224.848 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f79fa7fe700,python):2022-10-17-20:46:07.224.859 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "\n",
      "total images num:  9\n",
      "Processing, please wait a moment.\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.010\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.062\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.060\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.242\n",
      "\n",
      "========================================\n",
      "\n",
      "mAP: 0.0026560264153212814\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "#print(config)\n",
    "\n",
    "def ssd_eval(dataset_path, ckpt_path, anno_json):\n",
    "    \"\"\"SSD evaluation.\"\"\"\n",
    "    batch_size = 1\n",
    "    ds = create_ssd_dataset(dataset_path, batch_size=batch_size,\n",
    "                            is_training=False, use_multiprocessing=False)\n",
    "\n",
    "    net = ssd_vgg16(config=config)\n",
    "    \n",
    "    net = SsdInferWithDecoder(net, Tensor(default_boxes), config)\n",
    "\n",
    "    print(\"Load Checkpoint!\")\n",
    "    param_dict = ms.load_checkpoint(ckpt_path)\n",
    "    net.init_parameters_data()\n",
    "    ms.load_param_into_net(net, param_dict)\n",
    "\n",
    "    net.set_train(False)\n",
    "    total = ds.get_dataset_size() * batch_size\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(\"total images num: \", total)\n",
    "    print(\"Processing, please wait a moment.\")\n",
    "    eval_param_dict = {\"net\": net, \"dataset\": ds, \"anno_json\": anno_json}\n",
    "    mAP = apply_eval(eval_param_dict)\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(f\"mAP: {mAP}\")\n",
    "\n",
    "def eval_net():\n",
    "    if hasattr(config, 'num_ssd_boxes') and config.num_ssd_boxes == -1:\n",
    "        num = 0\n",
    "        h, w = config.img_shape\n",
    "        for i in range(len(config.steps)):\n",
    "            num += (h // config.steps[i]) * (w // config.steps[i]) * config.num_default[i]\n",
    "        config.num_ssd_boxes = num\n",
    "\n",
    "    coco_root = config.coco_root\n",
    "    json_path = os.path.join(coco_root, config.instances_set.format(config.val_data_type))\n",
    "\n",
    "    ms.set_context(mode=ms.GRAPH_MODE, device_target=config.device_target)\n",
    "    \n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, \"ssd_eval.mindrecord\"+ \"0\")\n",
    "\n",
    "    #mindrecord_file = create_mindrecord(config.dataset, \"ssd_eval.mindrecord\", False)\n",
    "\n",
    "    print(\"Start Eval!\")\n",
    "    ssd_eval(mindrecord_file, config.checkpoint_file_path, json_path)\n",
    "    \n",
    "eval_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a0cd1",
   "metadata": {},
   "source": [
    "### 参考\n",
    "[1] Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//European conference on computer vision. Springer, Cham, 2016: 21-37.<br />\n",
    "[2] http://t.csdn.cn/zycqp<br />\n",
    "[3] http://t.csdn.cn/rgoWz<br />\n",
    "[4] https://zhuanlan.zhihu.com/p/79854543<br />\n",
    "[5] https://zhuanlan.zhihu.com/p/33544892<br />\n",
    "[6] https://zhuanlan.zhihu.com/p/40968874<br />\n",
    "[7] https://blog.csdn.net/wind82465/article/details/118893589、https://blog.csdn.net/ThomasCai001/article/details/120097650<br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
