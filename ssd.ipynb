{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2a9205",
   "metadata": {},
   "source": [
    "# 基于MindSpore框架的SSD案例实现\n",
    "##  1 模型简介\n",
    "SSD，全称Single Shot MultiBox Detector，是Wei Liu在ECCV 2016上提出的一种目标检测算法。\n",
    "目标检测主流算法分成两个类型：\n",
    "1. two-stage方法：RCNN系列<br />\n",
    "通过算法产生候选框，然后再对这些候选框进行分类和回归。\n",
    "\n",
    "2. one-stage方法：yolo和SSD<br />\n",
    "直接通过主干网络给出类别位置信息，不需要区域生成。<br />\n",
    "\n",
    "SSD是单阶段的目标检测算法，通过卷积神经网络进行特征提取，取不同的特征层进行检测输出，所以SSD是一种多尺度的检测方法。在需要检测的特征层，直接使用一个3$\\times$3卷积，进行通道的变换。SSD采用了anchor的策略，预设不同长宽比例的anchor，每一个输出特征层基于anchor预测多个检测框（4或者6）。采用了多尺度检测方法，浅层用于检测小目标，深层用于检测大目标。\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 模型结构\n",
    "SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。SSD的网络结构如图1所示。上面是SSD模型，下面是Yolo模型，可以明显看到SSD利用了多尺度的特征图做检测。  \n",
    "  \n",
    "![图片](./src/img/v2-a43295a3e146008b2131b160eec09cd4_r.jpg)\n",
    "<br />\n",
    "两种单阶段目标检测算法的比较：<br />\n",
    "SSD先通过卷积不断进行特征提取，在需要检测物体的网络，直接通过一个3$\\times$3卷积得到输出，卷积的通道数由anchor数量和类别数量决定，具体为(anchor数量*(类别数量+4))。  \n",
    "SSD对比了YOLO系列目标检测方法，不同的是SSD通过卷积得到最后的边界框，而YOLO对最后的输出采用全连接的形式得到一维向量，对向量进行拆解得到最终的检测框。\n",
    "### 1.2 模型特点\n",
    "  \n",
    "a)多尺度检测  \n",
    "在SSD的网络结构图中我们可以看到，SSD使用了多个特征层，特征层的尺寸分别是38$\\times$38，19$\\times$19，10$\\times$10，5$\\times$5，3$\\times$3，1$\\times$1，一共6种不同的特征图尺寸。大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体。多尺度检测的方式，可以使得检测更加充分（SSD属于密集检测），更能检测出小目标。  \n",
    "\n",
    "b)采用卷积进行检测  \n",
    "与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为m$\\times$n$\\times$p的特征图，只需要采用3$\\times$3$\\times$p这样比较小的卷积核得到检测值。  \n",
    "\n",
    "c)预设anchor  \n",
    "在yolov1中，直接由网络预测目标的尺寸，这种方式使得预测框的长宽比和尺寸没有限制，难以训练。在SSD中，采用预设边界框，我们习惯称它为anchor（在SSD论文中叫default bounding boxes），预测框的尺寸在anchor的指导下进行微调。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28267d5",
   "metadata": {},
   "source": [
    "## 2 案例实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095ccaa",
   "metadata": {},
   "source": [
    "###  2.1 环境准备与数据读取\n",
    "本案例基于MindSpore-GPU 1.8.1版本实现，在GPU上完成模型训练。  \n",
    "  \n",
    "案例所使用的数据为coco2017，考虑到原始数据集过大，从中随机划分出100张图像作为训练集tiny_train_coco2017，50张图像作为测试集tiny_val_coco2017，且将数据转换为了Mindrecord格式。  \n",
    "数据集包含训练集、验证集以及对应的json文件，目录结构如下：  \n",
    "└─tiny_coco2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─annotations  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─instance_train2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─instance_val2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─val2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─train2017  \n",
    "#### 为了更加方便地保存和加载数据，本案例中在数据读取前首先将coco数据集转换成MindRecord格式：MindRecord_COCO\n",
    "MindRecord目录结构如下：  \n",
    "└─MindRecord_COCO  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0.db  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord1.db   \n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0.db  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord1.db   \n",
    " \n",
    "MindSpore中可以把用于训练网络模型的数据集，转换为MindSpore特定的格式数据（MindSpore Record格式），从而更加方便地保存和加载数据。\n",
    "\n",
    "+ mindspore.mindrecord模块中定义了一个专门的类FileWriter可以将用户定义的原始数据写入MindRecord文件。\n",
    "\n",
    "+ 通过MindDataset接口，可以实现MindSpore Record文件的读取。\n",
    "\n",
    "+ 使用MindRecord的目标是归一化提供训练测试所用的数据集，并通过dataset模块的相关方法进行数据的读取，将这些高效的数据投入训练。\n",
    "\n",
    "\n",
    "\n",
    "![图片](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/advanced/dataset/images/data_conversion_concept.png)\n",
    "\n",
    "MindRecord具备的特征如下：\n",
    "\n",
    "1. 实现多变的用户数据统一存储、访问，训练数据读取更加简便。\n",
    "2. 数据聚合存储，高效读取，且方便管理、移动。\n",
    "3. 高效的数据编解码操作，对用户透明、无感知。\n",
    "4. 可以灵活控制分区的大小，实现分布式训练。\n",
    "\n",
    "使用MindSpore Record数据格式可以减少磁盘IO、网络IO开销，从而获得更好的使用体验和性能提升。\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64a3a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ssd.mindrecord Mindrecord exists.\n",
      " ssd_eval.mindrecord Mindrecord exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from src.config import get_config\n",
    "config = get_config()\n",
    "\n",
    "def create_coco_label(is_training):\n",
    "    \"\"\"Get image path and annotation from COCO.\"\"\"\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    coco_root = os.path.join(config.data_path, config.coco_root)\n",
    "    data_type = config.val_data_type\n",
    "    if is_training:\n",
    "        data_type = config.train_data_type\n",
    "\n",
    "    # Classes need to train or test.\n",
    "    train_cls = config.classes\n",
    "    train_cls_dict = {}\n",
    "    for i, cls in enumerate(train_cls):\n",
    "        train_cls_dict[cls] = i\n",
    "\n",
    "    anno_json = os.path.join(coco_root, config.instances_set.format(data_type))\n",
    "\n",
    "    coco = COCO(anno_json)\n",
    "    classs_dict = {}\n",
    "    cat_ids = coco.loadCats(coco.getCatIds())\n",
    "    for cat in cat_ids:\n",
    "        classs_dict[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "    image_ids = coco.getImgIds()\n",
    "    images = []\n",
    "    image_path_dict = {}\n",
    "    image_anno_dict = {}\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        image_info = coco.loadImgs(img_id)\n",
    "        file_name = image_info[0][\"file_name\"]\n",
    "        anno_ids = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "        anno = coco.loadAnns(anno_ids)\n",
    "        image_path = os.path.join(coco_root, data_type, file_name)\n",
    "        annos = []\n",
    "        iscrowd = False\n",
    "        for label in anno:\n",
    "            bbox = label[\"bbox\"]\n",
    "            class_name = classs_dict[label[\"category_id\"]]\n",
    "            iscrowd = iscrowd or label[\"iscrowd\"]\n",
    "            if class_name in train_cls:\n",
    "                x_min, x_max = bbox[0], bbox[0] + bbox[2]\n",
    "                y_min, y_max = bbox[1], bbox[1] + bbox[3]\n",
    "                annos.append(list(map(round, [y_min, x_min, y_max, x_max])) + [train_cls_dict[class_name]])\n",
    "\n",
    "        if not is_training and iscrowd:\n",
    "            continue\n",
    "        if len(annos) >= 1:\n",
    "            images.append(img_id)\n",
    "            image_path_dict[img_id] = image_path\n",
    "            image_anno_dict[img_id] = np.array(annos)\n",
    "\n",
    "    return images, image_path_dict, image_anno_dict\n",
    "\n",
    "def data_to_mindrecord_byte_image( is_training=True, prefix=\"ssd.mindrecord\", file_num=8):\n",
    "    \"\"\"Create MindRecord file.\"\"\"\n",
    "    mindrecord_path = os.path.join(config.data_path, config.mindrecord_dir, prefix)\n",
    "    writer = FileWriter(mindrecord_path, file_num)\n",
    "    images, image_path_dict, image_anno_dict = create_coco_label(is_training)\n",
    "    ssd_json = {\n",
    "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
    "        \"image\": {\"type\": \"bytes\"},\n",
    "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
    "    }\n",
    "    writer.add_schema(ssd_json, \"ssd_json\")\n",
    "\n",
    "    for img_id in images:\n",
    "        image_path = image_path_dict[img_id]\n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = f.read()\n",
    "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
    "        img_id = np.array([img_id], dtype=np.int32)\n",
    "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
    "        writer.write_raw_data([row])\n",
    "    writer.commit()\n",
    "\n",
    "\n",
    "def create_mindrecord( prefix=\"ssd.mindrecord\", is_training=True):\n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, prefix + \"0\")\n",
    "    os.makedirs(mindrecord_dir,exist_ok=True)\n",
    "    if not os.path.exists(mindrecord_file):\n",
    "        print(\"Create {} Mindrecord.\".format(prefix))\n",
    "        data_to_mindrecord_byte_image(is_training, prefix)\n",
    "        print(\"Create {} Mindrecord Done, at {}\".format(prefix,mindrecord_dir))\n",
    "    else:\n",
    "        print(\" {} Mindrecord exists.\".format(prefix))\n",
    "    return mindrecord_file\n",
    "\n",
    "# 数据转换为mindrecord格式\n",
    "mindrecord_file = create_mindrecord(\"ssd.mindrecord\", True)\n",
    "eval_mindrecord_file = create_mindrecord(\"ssd_eval.mindrecord\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f834043",
   "metadata": {},
   "source": [
    "### 数据预处理  \n",
    "数据统一resize为300$\\times$300大小  \n",
    "SSD算法中采用了以下几种数据增强的方法：  \n",
    "随机裁剪：随机裁剪一个部分，每个采样部分的大小为原图的[0.3,1]，长宽比在1/2和2之间。如果真实标签框的中心位于采样部分内，则保留真实框与图片重叠的部分。  \n",
    "水平翻转：对采样后的小图片进行0.5概率的随机水平翻转 .  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a23487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def _rand(a=0., b=1.):\n",
    "    return np.random.rand() * (b - a) + a\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\"Compute the intersect of two sets of boxes.\"\"\"\n",
    "    max_yx = np.minimum(box_a[:, 2:4], box_b[2:4])\n",
    "    min_yx = np.maximum(box_a[:, :2], box_b[:2])\n",
    "    inter = np.clip((max_yx - min_yx), a_min=0, a_max=np.inf)\n",
    "    return inter[:, 0] * inter[:, 1]\n",
    "\n",
    "def jaccard_numpy(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.\"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n",
    "              (box_a[:, 3] - box_a[:, 1]))\n",
    "    area_b = ((box_b[2] - box_b[0]) *\n",
    "              (box_b[3] - box_b[1]))\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union\n",
    "\n",
    "# 随机裁剪图像和box\n",
    "def random_sample_crop(image, boxes):\n",
    "    height, width, _ = image.shape\n",
    "    min_iou = np.random.choice([None, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "\n",
    "    if min_iou is None:\n",
    "        return image, boxes\n",
    "\n",
    "    # max trails (50)\n",
    "    for _ in range(50):\n",
    "        image_t = image\n",
    "        w = _rand(0.3, 1.0) * width\n",
    "        h = _rand(0.3, 1.0) * height\n",
    "        # aspect ratio constraint b/t .5 & 2\n",
    "        if h / w < 0.5 or h / w > 2:\n",
    "            continue\n",
    "\n",
    "        left = _rand() * (width - w)\n",
    "        top = _rand() * (height - h)\n",
    "        rect = np.array([int(top), int(left), int(top + h), int(left + w)])\n",
    "        overlap = jaccard_numpy(boxes, rect)\n",
    "\n",
    "        # dropout some boxes\n",
    "        drop_mask = overlap > 0\n",
    "        if not drop_mask.any():\n",
    "            continue\n",
    "\n",
    "        if overlap[drop_mask].min() < min_iou and overlap[drop_mask].max() > (min_iou + 0.2):\n",
    "            continue\n",
    "\n",
    "        image_t = image_t[rect[0]:rect[2], rect[1]:rect[3], :]\n",
    "        centers = (boxes[:, :2] + boxes[:, 2:4]) / 2.0\n",
    "        m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n",
    "        m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n",
    "\n",
    "        # mask in that both m1 and m2 are true\n",
    "        mask = m1 * m2 * drop_mask\n",
    "\n",
    "        # have any valid boxes? try again if not\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        # take only matching gt boxes\n",
    "        boxes_t = boxes[mask, :].copy()\n",
    "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], rect[:2])\n",
    "        boxes_t[:, :2] -= rect[:2]\n",
    "        boxes_t[:, 2:4] = np.minimum(boxes_t[:, 2:4], rect[2:4])\n",
    "        boxes_t[:, 2:4] -= rect[:2]\n",
    "\n",
    "        return image_t, boxes_t\n",
    "    return image, boxes\n",
    "\n",
    "def ssd_bboxes_encode(boxes):\n",
    "    \"\"\"\n",
    "    Labels anchors with ground truth inputs.\n",
    "\n",
    "    Args:\n",
    "        boxex: ground truth with shape [N, 5], for each row, it stores [y, x, h, w, cls].\n",
    "\n",
    "    Returns:\n",
    "        gt_loc: location ground truth with shape [num_anchors, 4].\n",
    "        gt_label: class ground truth with shape [num_anchors, 1].\n",
    "        num_matched_boxes: number of positives in an image.\n",
    "    \"\"\"\n",
    "\n",
    "    def jaccard_with_anchors(bbox):\n",
    "        \"\"\"Compute jaccard score a box and the anchors.\"\"\"\n",
    "        # Intersection bbox and volume.\n",
    "        ymin = np.maximum(y1, bbox[0])\n",
    "        xmin = np.maximum(x1, bbox[1])\n",
    "        ymax = np.minimum(y2, bbox[2])\n",
    "        xmax = np.minimum(x2, bbox[3])\n",
    "        w = np.maximum(xmax - xmin, 0.)\n",
    "        h = np.maximum(ymax - ymin, 0.)\n",
    "\n",
    "        # Volumes.\n",
    "        inter_vol = h * w\n",
    "        union_vol = vol_anchors + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) - inter_vol\n",
    "        jaccard = inter_vol / union_vol\n",
    "        return np.squeeze(jaccard)\n",
    "\n",
    "    pre_scores = np.zeros((config.num_ssd_boxes), dtype=np.float32)\n",
    "    t_boxes = np.zeros((config.num_ssd_boxes, 4), dtype=np.float32)\n",
    "    t_label = np.zeros((config.num_ssd_boxes), dtype=np.int64)\n",
    "    for bbox in boxes:\n",
    "        label = int(bbox[4])\n",
    "        scores = jaccard_with_anchors(bbox)\n",
    "        idx = np.argmax(scores)\n",
    "        scores[idx] = 2.0\n",
    "        mask = (scores > matching_threshold)\n",
    "        mask = mask & (scores > pre_scores)\n",
    "        pre_scores = np.maximum(pre_scores, scores * mask)\n",
    "        t_label = mask * label + (1 - mask) * t_label\n",
    "        for i in range(4):\n",
    "            t_boxes[:, i] = mask * bbox[i] + (1 - mask) * t_boxes[:, i]\n",
    "\n",
    "    index = np.nonzero(t_label)\n",
    "\n",
    "    # Transform to tlbr.\n",
    "    bboxes = np.zeros((config.num_ssd_boxes, 4), dtype=np.float32)\n",
    "    bboxes[:, [0, 1]] = (t_boxes[:, [0, 1]] + t_boxes[:, [2, 3]]) / 2\n",
    "    bboxes[:, [2, 3]] = t_boxes[:, [2, 3]] - t_boxes[:, [0, 1]]\n",
    "\n",
    "    # Encode features.\n",
    "    bboxes_t = bboxes[index]\n",
    "    default_boxes_t = default_boxes[index]\n",
    "    bboxes_t[:, :2] = (bboxes_t[:, :2] - default_boxes_t[:, :2]) / (default_boxes_t[:, 2:] * config.prior_scaling[0])\n",
    "    tmp = np.maximum(bboxes_t[:, 2:4] / default_boxes_t[:, 2:4], 0.000001)\n",
    "    bboxes_t[:, 2:4] = np.log(tmp) / config.prior_scaling[1]\n",
    "    bboxes[index] = bboxes_t\n",
    "\n",
    "    num_match = np.array([len(np.nonzero(t_label)[0])], dtype=np.int32)\n",
    "    return bboxes, t_label.astype(np.int32), num_match\n",
    "\n",
    "def preprocess_fn(img_id, image, box, is_training):\n",
    "    \"\"\"Preprocess function for dataset.\"\"\"\n",
    "    cv2.setNumThreads(2)\n",
    "\n",
    "    def _infer_data(image, input_shape):\n",
    "        img_h, img_w, _ = image.shape\n",
    "        input_h, input_w = input_shape\n",
    "\n",
    "        image = cv2.resize(image, (input_w, input_h))\n",
    "\n",
    "        # When the channels of image is 1\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "            image = np.concatenate([image, image, image], axis=-1)\n",
    "\n",
    "        return img_id, image, np.array((img_h, img_w), np.float32)\n",
    "\n",
    "    def _data_aug(image, box, is_training, image_size=(300, 300)):\n",
    "        ih, iw, _ = image.shape\n",
    "        h, w = image_size\n",
    "        if not is_training:\n",
    "            return _infer_data(image, image_size)\n",
    "        # Random crop\n",
    "        box = box.astype(np.float32)\n",
    "        image, box = random_sample_crop(image, box)\n",
    "        ih, iw, _ = image.shape\n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (w, h))\n",
    "        # Flip image or not\n",
    "        flip = _rand() < .5\n",
    "        if flip:\n",
    "            image = cv2.flip(image, 1, dst=None)\n",
    "        # When the channels of image is 1\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "            image = np.concatenate([image, image, image], axis=-1)\n",
    "        box[:, [0, 2]] = box[:, [0, 2]] / ih\n",
    "        box[:, [1, 3]] = box[:, [1, 3]] / iw\n",
    "        if flip:\n",
    "            box[:, [1, 3]] = 1 - box[:, [3, 1]]\n",
    "        box, label, num_match = ssd_bboxes_encode(box)\n",
    "        return image, box, label, num_match\n",
    "    return _data_aug(image, box, is_training, image_size=config.img_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d684",
   "metadata": {},
   "source": [
    "### 数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec706a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import mindspore.dataset as de\n",
    "\n",
    "def create_ssd_dataset(mindrecord_file, batch_size=32, device_num=1, rank=0,\n",
    "                       is_training=True, num_parallel_workers=1, use_multiprocessing=True):\n",
    "    \"\"\"Create SSD dataset with MindDataset.\"\"\"\n",
    "    ds = de.MindDataset(mindrecord_file, columns_list=[\"img_id\", \"image\", \"annotation\"], num_shards=device_num,\n",
    "                        shard_id=rank, num_parallel_workers=num_parallel_workers, shuffle=is_training)\n",
    "    decode = de.vision.Decode()\n",
    "    ds = ds.map(operations=decode, input_columns=[\"image\"])\n",
    "    change_swap_op = de.vision.HWC2CHW()\n",
    "    # Computed from random subset of ImageNet training images\n",
    "    normalize_op = de.vision.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                       std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    color_adjust_op = de.vision.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4)\n",
    "    compose_map_func = (lambda img_id, image, annotation: preprocess_fn(img_id, image, annotation, is_training))\n",
    "    if is_training:\n",
    "        output_columns = [\"image\", \"box\", \"label\", \"num_match\"]\n",
    "        trans = [color_adjust_op, normalize_op, change_swap_op]\n",
    "    else:\n",
    "        output_columns = [\"img_id\", \"image\", \"image_shape\"]\n",
    "        trans = [normalize_op, change_swap_op]\n",
    "    ds = ds.map(operations=compose_map_func, input_columns=[\"img_id\", \"image\", \"annotation\"],\n",
    "                output_columns=output_columns, column_order=output_columns,\n",
    "                python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.map(operations=trans, input_columns=[\"image\"], python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f75e8f",
   "metadata": {},
   "source": [
    "##  2.2 模型构建\n",
    "SSD的网络结构主要分为以下几个部分：\n",
    "* VGG16 Base Layer\n",
    "* Extra Feature Layer\n",
    "+ Detection Layer\n",
    "+ NMS<br />\n",
    "+ Anchor \n",
    "\n",
    "\n",
    "**Backbone Layer**\n",
    "\n",
    "![图片](./src/img/441a293cb82c1ebecc4f67b0e03c2b05.png)  \n",
    "\n",
    "\n",
    "输入图像经过预处理后大小固定为300×300，首先经过backbone，本案例中使用的是VGG16网络的前13个卷积层，然后分别将VGG16的全连接层fc6和fc7转换成3$\\times$3卷积层block6和1$\\times$1卷积层block7，进一步提取特征。 在block6中，使用了空洞数为6的空洞卷积，其padding也为6，这样做同样也是为了增加感受野的同时保持参数量与特征图尺寸的不变。  \n",
    "\n",
    "**Extra Feature Layer**\n",
    "\n",
    "在VGG16的基础上，SSD进一步增加了4个深度卷积层，用于提取更高层的语义信息：\n",
    "![图片](./src/img/conv.png)\n",
    "block8-11，用于更高语义信息的提取。block8的通道数为512，而block9、block10与block11的通道数都为256。从block7到block11，这5个卷积后输出特征图的尺寸依次为19×19、10×10、5×5、3×3和1×1。为了降低参数量，使用了1×1卷积先降低通道数为该层输出通道数的一半，再利用3×3卷积进行特征提取。 \n",
    " \n",
    " **Anchor**\n",
    "\n",
    "SSD采用了PriorBox来进行区域生成。将固定大小宽高的PriorBox作为先验的感兴趣区域，利用一个阶段完成能够分类与回归。设计大量的密集的PriorBox保证了对整幅图像的每个地方都有一一的检测。PriorBox位置的表示形式是以中心点坐标和框的宽、高(cx,cy,w,h)来表示的，同时都转换成百分比的形式。\n",
    "PriorBox生成规则：\n",
    "SSD由6个特征层来检测目标，在不同特征层上，PriorBox的尺寸scale大小是不一样的，最低层的scale=0.1，最高层的scale=0.95，其他层的计算公式如下\n",
    "![图片](./src/img/sk.png)\n",
    "在某个特征层上其scale一定，那么会设置不同长宽比ratio的PriorBox，其长和宽的计算公式如下：\n",
    "![图片](./src/img/sk1.png)\n",
    "在ratio=1的时候，还会根据该特征层和下一个特征层计算一个特定scale的pPriorBox(长宽比ratio=1)，计算公式如下：\n",
    "![图片](./src/img/sk3.png)\n",
    "每个特征层的每个点都会以上述规则生成PriorBox，(cx,cy)由当前点的中心点来确定，由此每个特征层都生成大量密集的PriorBox，如下图：\n",
    "![图片](./src/img/box.png)\n",
    "\n",
    "SSD使用了第4、7、8、9、10和11这6个卷积层得到的特征图，这6个特征图尺寸越来越小，而其对应的感受野越来越大。6个特征图上的每一个点分别对应4、6、6、6、4、4个PriorBox。某个特征图上的一个点根据下采样率可以得到在原图的坐标，以该坐标为中心生成4个或6个不同大小的PriorBox，然后利用特征图的特征去预测每一个PriorBox对应类别与位置的预测量。例如：第8个卷积层得到的特征图大小为10×10×512，每个点对应6个PriorBox，一共有600个PriorBox。定义MultiBox类，生成多个预测框。 \n",
    " \n",
    " \n",
    " **Detection Layer**\n",
    "![图片](./src/img/dec.jpg)\n",
    "SSD模型一共有6个预测特征图，对于其中一个尺寸为m\\*n，通道为p的预测特征图，假设其每个像素点会产生k个anchor，每个anchor会对应c个类别和4个回归偏移量，使用(4+c)k个尺寸为3x3，通道为p的卷积核对该预测特征图进行卷积操作，得到尺寸为m\\*n，通道为(4+c)m\\*k的输出特征图，它包含了预测特征图上所产生的每个anchor的回归偏移量和各类别概率分数。所以对于尺寸为m\\*n的预测特征图，总共会产生(4+c)k\\*m\\*n个结果。cls分支的输出通道数为k\\*class_num，loc分支的输出通道数为k\\*4。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11ad0e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354a9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from src.vgg16 import vgg16\n",
    "import mindspore.ops as ops\n",
    "import ml_collections\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"ensures that all layers have a channel number that is divisible by 8.\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def _conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same'):\n",
    "    return nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=0, pad_mode=pad_mod, has_bias=True)\n",
    "\n",
    "\n",
    "def _bn(channel):\n",
    "    return nn.BatchNorm2d(channel, eps=1e-3, momentum=0.97,\n",
    "                          gamma_init=1, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "\n",
    "\n",
    "def _last_conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same', pad=0):\n",
    "    in_channels = in_channel\n",
    "    out_channels = in_channel\n",
    "    depthwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad_mode='same',\n",
    "                               padding=pad, group=in_channels)\n",
    "    conv = _conv2d(in_channel, out_channel, kernel_size=1)\n",
    "    return nn.SequentialCell([depthwise_conv, _bn(in_channel), nn.ReLU6(), conv])\n",
    "\n",
    "\n",
    "class FlattenConcat(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(FlattenConcat, self).__init__()\n",
    "        self.num_ssd_boxes = config.num_ssd_boxes\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "        self.transpose = ops.Transpose()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        output = ()\n",
    "        batch_size = ops.shape(inputs[0])[0]\n",
    "        for x in inputs:\n",
    "            x = self.transpose(x, (0, 2, 3, 1))\n",
    "            output += (ops.reshape(x, (batch_size, -1)),)\n",
    "        res = self.concat(output)\n",
    "        return ops.reshape(res, (batch_size, self.num_ssd_boxes, -1))\n",
    "\n",
    "    \n",
    "class GridAnchorGenerator:\n",
    "    \"\"\"\n",
    "    Anchor Generator\n",
    "    \"\"\"\n",
    "    def __init__(self, image_shape, scale, scales_per_octave, aspect_ratios):\n",
    "        super(GridAnchorGenerator, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.scales_per_octave = scales_per_octave\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "\n",
    "    def generate(self, step):\n",
    "        scales = np.array([2**(float(scale) / self.scales_per_octave)\n",
    "                           for scale in range(self.scales_per_octave)]).astype(np.float32)\n",
    "        aspects = np.array(list(self.aspect_ratios)).astype(np.float32)\n",
    "\n",
    "        scales_grid, aspect_ratios_grid = np.meshgrid(scales, aspects)\n",
    "        scales_grid = scales_grid.reshape([-1])\n",
    "        aspect_ratios_grid = aspect_ratios_grid.reshape([-1])\n",
    "\n",
    "        feature_size = [self.image_shape[0] / step, self.image_shape[1] / step]\n",
    "        grid_height, grid_width = feature_size\n",
    "\n",
    "        base_size = np.array([self.scale * step, self.scale * step]).astype(np.float32)\n",
    "        anchor_offset = step / 2.0\n",
    "\n",
    "        ratio_sqrt = np.sqrt(aspect_ratios_grid)\n",
    "        heights = scales_grid / ratio_sqrt * base_size[0]\n",
    "        widths = scales_grid * ratio_sqrt * base_size[1]\n",
    "\n",
    "        y_centers = np.arange(grid_height).astype(np.float32)\n",
    "        y_centers = y_centers * step + anchor_offset\n",
    "        x_centers = np.arange(grid_width).astype(np.float32)\n",
    "        x_centers = x_centers * step + anchor_offset\n",
    "        x_centers, y_centers = np.meshgrid(x_centers, y_centers)\n",
    "\n",
    "        x_centers_shape = x_centers.shape\n",
    "        y_centers_shape = y_centers.shape\n",
    "\n",
    "        widths_grid, x_centers_grid = np.meshgrid(widths, x_centers.reshape([-1]))\n",
    "        heights_grid, y_centers_grid = np.meshgrid(heights, y_centers.reshape([-1]))\n",
    "\n",
    "        x_centers_grid = x_centers_grid.reshape(*x_centers_shape, -1)\n",
    "        y_centers_grid = y_centers_grid.reshape(*y_centers_shape, -1)\n",
    "        widths_grid = widths_grid.reshape(-1, *x_centers_shape)\n",
    "        heights_grid = heights_grid.reshape(-1, *y_centers_shape)\n",
    "\n",
    "\n",
    "        bbox_centers = np.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "        bbox_sizes = np.stack([heights_grid, widths_grid], axis=3)\n",
    "        bbox_centers = bbox_centers.reshape([-1, 2])\n",
    "        bbox_sizes = bbox_sizes.reshape([-1, 2])\n",
    "        bbox_corners = np.concatenate([bbox_centers - 0.5 * bbox_sizes, bbox_centers + 0.5 * bbox_sizes], axis=1)\n",
    "        self.bbox_corners = bbox_corners / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "        self.bbox_centers = np.concatenate([bbox_centers, bbox_sizes], axis=1)\n",
    "        self.bbox_centers = self.bbox_centers / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "\n",
    "        print(self.bbox_centers.shape)\n",
    "        return self.bbox_centers, self.bbox_corners\n",
    "\n",
    "    def generate_multi_levels(self, steps):\n",
    "        bbox_centers_list = []\n",
    "        bbox_corners_list = []\n",
    "        for step in steps:\n",
    "            bbox_centers, bbox_corners = self.generate(step)\n",
    "            bbox_centers_list.append(bbox_centers)\n",
    "            bbox_corners_list.append(bbox_corners)\n",
    "\n",
    "        self.bbox_centers = np.concatenate(bbox_centers_list, axis=0)\n",
    "        self.bbox_corners = np.concatenate(bbox_corners_list, axis=0)\n",
    "        return self.bbox_centers, self.bbox_corners    \n",
    "    \n",
    "    \n",
    "\n",
    "class MultiBox(nn.Cell):\n",
    "    \"\"\"\n",
    "    Multibox conv layers. Each multibox layer contains class conf scores and localization predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MultiBox, self).__init__()\n",
    "        num_classes = 81\n",
    "        out_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_default = config.num_default\n",
    "\n",
    "        loc_layers = []\n",
    "        cls_layers = []\n",
    "        for k, out_channel in enumerate(out_channels):\n",
    "            loc_layers += [_last_conv2d(out_channel, 4 * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "            cls_layers += [_last_conv2d(out_channel, num_classes * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "\n",
    "        self.multi_loc_layers = nn.layer.CellList(loc_layers)\n",
    "        self.multi_cls_layers = nn.layer.CellList(cls_layers)\n",
    "        self.flatten_concat = FlattenConcat(config)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        loc_outputs = ()\n",
    "        cls_outputs = ()\n",
    "        for i in range(len(self.multi_loc_layers)):\n",
    "            loc_outputs += (self.multi_loc_layers[i](inputs[i]),)\n",
    "            cls_outputs += (self.multi_cls_layers[i](inputs[i]),)\n",
    "        return self.flatten_concat(loc_outputs), self.flatten_concat(cls_outputs)\n",
    "\n",
    "\n",
    "class SSD300VGG16(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(SSD300VGG16, self).__init__()\n",
    "\n",
    "        # VGG16 backbone: block1~5\n",
    "        self.backbone = vgg16()\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        self.b6_1 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=6, dilation=6, pad_mode='pad')\n",
    "        self.b6_2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.b7_1 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
    "        self.b7_2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        self.b8_1 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b8_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b9_1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b9_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b10_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b10_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        self.b11_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b11_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        # boxes\n",
    "        self.multi_box = MultiBox(config)\n",
    "        if not self.training:\n",
    "            self.activation = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # VGG16 backbone: block1~5\n",
    "        block4, x = self.backbone(x)\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        x = self.b6_1(x)  # 1024\n",
    "        x = self.b6_2(x)\n",
    "\n",
    "        x = self.b7_1(x)  # 1024\n",
    "        x = self.b7_2(x)\n",
    "        block7 = x\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        x = self.b8_1(x)  # 256\n",
    "        x = self.b8_2(x)  # 512\n",
    "        block8 = x\n",
    "\n",
    "        x = self.b9_1(x)  # 128\n",
    "        x = self.b9_2(x)  # 256\n",
    "        block9 = x\n",
    "\n",
    "        x = self.b10_1(x)  # 128\n",
    "        x = self.b10_2(x)  # 256\n",
    "        block10 = x\n",
    "\n",
    "        x = self.b11_1(x)  # 128\n",
    "        x = self.b11_2(x)  # 256\n",
    "        block11 = x\n",
    "\n",
    "        # boxes\n",
    "        multi_feature = (block4, block7, block8, block9, block10, block11)\n",
    "        pred_loc, pred_label = self.multi_box(multi_feature)\n",
    "        if not self.training:\n",
    "            pred_label = self.activation(pred_label)\n",
    "        pred_loc = ops.cast(pred_loc, ms.float32)\n",
    "        pred_label = ops.cast(pred_label, ms.float32)\n",
    "        return pred_loc, pred_label\n",
    "\n",
    "\n",
    "def ssd_vgg16(**kwargs):\n",
    "    return SSD300VGG16(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a19ba3",
   "metadata": {},
   "source": [
    "## 2.3 损失函数\n",
    "损失函数定义为位置误差（locatization loss， loc）与置信度误差（confidence loss, conf）的加权和：\n",
    "![图片](./src/img/loss.png)  \n",
    "其中：<br />\n",
    "N 是先验框的正样本数量；<br /> \n",
    "c 为类别置信度预测值; <br />\n",
    "l 为先验框的所对应边界框的位置预测值; <br />\n",
    "g 为ground truth的位置参数  <br />\n",
    "\n",
    "**对于位置损失函数：**\n",
    "针对所有的正样本，采用 Smooth L1 Loss, 位置信息都是 encode 之后的位置信息。\n",
    "![图片](./src/img/smooth.png)  \n",
    "**对于置信度损失函数：**\n",
    "置信度损失是多类置信度(c)上的softmax损失。\n",
    "![图片](./src/img/conf.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72446fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "grad_scale = ops.MultitypeFuncGraph(\"grad_scale\")\n",
    "\n",
    "class SigmoidFocalClassificationLoss(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Sigmoid focal-loss for classification.\n",
    "    Args:\n",
    "        gamma (float): Hyper-parameter to balance the easy and hard examples. Default: 2.0\n",
    "        alpha (float): Hyper-parameter to balance the positive and negative example. Default: 0.25\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(SigmoidFocalClassificationLoss, self).__init__()\n",
    "        self.sigmiod_cross_entropy = ops.SigmoidCrossEntropyWithLogits()\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.pow = ops.Pow()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.on_value = Tensor(1.0, ms.float32)\n",
    "        self.off_value = Tensor(0.0, ms.float32)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label = self.onehot(label, ops.shape(logits)[-1], self.on_value, self.off_value)\n",
    "        sigmiod_cross_entropy = self.sigmiod_cross_entropy(logits, label)\n",
    "        sigmoid = self.sigmoid(logits)\n",
    "        label = ops.cast(label, ms.float32)\n",
    "        p_t = label * sigmoid + (1 - label) * (1 - sigmoid)\n",
    "        modulating_factor = self.pow(1 - p_t, self.gamma)\n",
    "        alpha_weight_factor = label * self.alpha + (1 - label) * (1 - self.alpha)\n",
    "        focal_loss = modulating_factor * alpha_weight_factor * sigmiod_cross_entropy\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class SSDWithLossCell(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Provide SSD training loss through network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        super(SSDWithLossCell, self).__init__()\n",
    "        self.network = network\n",
    "        self.less = ops.Less()\n",
    "        self.tile = ops.Tile()\n",
    "        self.reduce_sum = ops.ReduceSum()\n",
    "        self.expand_dims = ops.ExpandDims()\n",
    "        self.class_loss = SigmoidFocalClassificationLoss(config.gamma, config.alpha)\n",
    "        self.loc_loss = nn.SmoothL1Loss()\n",
    "\n",
    "    def construct(self, x, gt_loc, gt_label, num_matched_boxes):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "        mask = ops.cast(self.less(0, gt_label), ms.float32)\n",
    "        num_matched_boxes = self.reduce_sum(ops.cast(num_matched_boxes, ms.float32))\n",
    "        # 定位损失\n",
    "        mask_loc = self.tile(self.expand_dims(mask, -1), (1, 1, 4))\n",
    "        smooth_l1 = self.loc_loss(pred_loc, gt_loc) * mask_loc\n",
    "        loss_loc = self.reduce_sum(self.reduce_sum(smooth_l1, -1), -1)\n",
    "\n",
    "        # 类别损失\n",
    "        loss_cls = self.class_loss(pred_label, gt_label)\n",
    "        loss_cls = self.reduce_sum(loss_cls, (1, 2))\n",
    "\n",
    "        return self.reduce_sum((loss_cls + loss_loc) / num_matched_boxes)\n",
    "\n",
    "# net = SSDWithLossCell(ssd, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b2cc1",
   "metadata": {},
   "source": [
    "### Metric\n",
    "在SSD中，训练过程是不需要用到非极大值抑制(NMS)，但当进行检测时，例如输入一张图片要求输出框的时候，需要用到NMS过滤掉那些重叠度较大的预测框。<br />\n",
    "非极大值抑制的流程如下：\n",
    "1. 根据置信度得分进行排序\n",
    "2. 选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除<br />\n",
    "3. 计算所有边界框的面积<br />\n",
    "4. 计算置信度最高的边界框与其它候选框的IoU<br />\n",
    "5. 删除IoU大于阈值的边界框<br />\n",
    "6. 重复上述过程，直至边界框列表为空<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590d1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore.train.callback import Callback\n",
    "import json\n",
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "\n",
    "def apply_eval(eval_param_dict):\n",
    "    net = eval_param_dict[\"net\"]\n",
    "    net.set_train(False)\n",
    "    ds = eval_param_dict[\"dataset\"]\n",
    "    anno_json = eval_param_dict[\"anno_json\"]\n",
    "    coco_metrics = COCOMetrics(anno_json=anno_json,\n",
    "                               classes=config.classes,\n",
    "                               num_classes=config.num_classes,\n",
    "                               max_boxes=config.max_boxes,\n",
    "                               nms_threshold=config.nms_threshold,\n",
    "                               min_score=config.min_score)\n",
    "    for data in ds.create_dict_iterator(output_numpy=True, num_epochs=1):\n",
    "        img_id = data['img_id']\n",
    "        img_np = data['image']\n",
    "        image_shape = data['image_shape']\n",
    "\n",
    "        output = net(Tensor(img_np))\n",
    "\n",
    "        for batch_idx in range(img_np.shape[0]):\n",
    "            pred_batch = {\n",
    "                \"boxes\": output[0].asnumpy()[batch_idx],\n",
    "                \"box_scores\": output[1].asnumpy()[batch_idx],\n",
    "                \"img_id\": int(np.squeeze(img_id[batch_idx])),\n",
    "                \"image_shape\": image_shape[batch_idx]\n",
    "            }\n",
    "            coco_metrics.update(pred_batch)\n",
    "    eval_metrics = coco_metrics.get_metrics()\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "def apply_nms(all_boxes, all_scores, thres, max_boxes):\n",
    "    \"\"\"Apply NMS to bboxes.\"\"\"\n",
    "    y1 = all_boxes[:, 0]\n",
    "    x1 = all_boxes[:, 1]\n",
    "    y2 = all_boxes[:, 2]\n",
    "    x2 = all_boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    order = all_scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        if len(keep) >= max_boxes:\n",
    "            break\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thres)[0]\n",
    "\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "\n",
    "class COCOMetrics:\n",
    "    \"\"\"Calculate mAP of predicted bboxes.\"\"\"\n",
    "\n",
    "    def __init__(self, anno_json, classes, num_classes, min_score, nms_threshold, max_boxes):\n",
    "        self.num_classes = num_classes\n",
    "        self.classes = classes\n",
    "        self.min_score = min_score\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.max_boxes = max_boxes\n",
    "\n",
    "        self.val_cls_dict = {i: cls for i, cls in enumerate(classes)}\n",
    "        self.coco_gt = COCO(anno_json)\n",
    "        cat_ids = self.coco_gt.loadCats(self.coco_gt.getCatIds())\n",
    "        self.class_dict = {cat['name']: cat['id'] for cat in cat_ids}\n",
    "\n",
    "        self.predictions = []\n",
    "        self.img_ids = []\n",
    "\n",
    "    def update(self, batch):\n",
    "        pred_boxes = batch['boxes']\n",
    "        box_scores = batch['box_scores']\n",
    "        img_id = batch['img_id']\n",
    "        h, w = batch['image_shape']\n",
    "\n",
    "        final_boxes = []\n",
    "        final_label = []\n",
    "        final_score = []\n",
    "        self.img_ids.append(img_id)\n",
    "\n",
    "        for c in range(1, self.num_classes):\n",
    "            class_box_scores = box_scores[:, c]\n",
    "            score_mask = class_box_scores > self.min_score\n",
    "            class_box_scores = class_box_scores[score_mask]\n",
    "            class_boxes = pred_boxes[score_mask] * [h, w, h, w]\n",
    "\n",
    "            if score_mask.any():\n",
    "                nms_index = apply_nms(class_boxes, class_box_scores, self.nms_threshold, self.max_boxes)\n",
    "                class_boxes = class_boxes[nms_index]\n",
    "                class_box_scores = class_box_scores[nms_index]\n",
    "\n",
    "                final_boxes += class_boxes.tolist()\n",
    "                final_score += class_box_scores.tolist()\n",
    "                final_label += [self.class_dict[self.val_cls_dict[c]]] * len(class_box_scores)\n",
    "\n",
    "        for loc, label, score in zip(final_boxes, final_label, final_score):\n",
    "            res = {}\n",
    "            res['image_id'] = img_id\n",
    "            res['bbox'] = [loc[1], loc[0], loc[3] - loc[1], loc[2] - loc[0]]\n",
    "            res['score'] = score\n",
    "            res['category_id'] = label\n",
    "            self.predictions.append(res)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        with open('predictions.json', 'w') as f:\n",
    "            json.dump(self.predictions, f)\n",
    "\n",
    "        coco_dt = self.coco_gt.loadRes('predictions.json')\n",
    "        E = COCOeval(self.coco_gt, coco_dt, iouType='bbox')\n",
    "        E.params.imgIds = self.img_ids\n",
    "        E.evaluate()\n",
    "        E.accumulate()\n",
    "        E.summarize()\n",
    "        return E.stats[0]\n",
    "\n",
    "\n",
    "class SsdInferWithDecoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    SSD Infer wrapper to decode the bbox locations.\n",
    "    Args:\n",
    "        network (Cell): the origin ssd infer network without bbox decoder.\n",
    "        default_boxes (Tensor): the default_boxes from anchor generator\n",
    "        config (dict): ssd config\n",
    "    Returns:\n",
    "        Tensor, the locations for bbox after decoder representing (y0,x0,y1,x1)\n",
    "        Tensor, the prediction labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, default_boxes, config):\n",
    "        super(SsdInferWithDecoder, self).__init__()\n",
    "        self.network = network\n",
    "        self.default_boxes = default_boxes\n",
    "        self.prior_scaling_xy = config.prior_scaling[0]\n",
    "        self.prior_scaling_wh = config.prior_scaling[1]\n",
    "\n",
    "    def construct(self, x):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "\n",
    "        default_bbox_xy = self.default_boxes[..., :2]\n",
    "        default_bbox_wh = self.default_boxes[..., 2:]\n",
    "        pred_xy = pred_loc[..., :2] * self.prior_scaling_xy * default_bbox_wh + default_bbox_xy\n",
    "        pred_wh = ops.Exp()(pred_loc[..., 2:] * self.prior_scaling_wh) * default_bbox_wh\n",
    "\n",
    "        pred_xy_0 = pred_xy - pred_wh / 2.0\n",
    "        pred_xy_1 = pred_xy + pred_wh / 2.0\n",
    "        pred_xy = ops.Concat(-1)((pred_xy_0, pred_xy_1))\n",
    "        pred_xy = ops.Maximum()(pred_xy, 0)\n",
    "        pred_xy = ops.Minimum()(pred_xy, 1)\n",
    "        return pred_xy, pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e45a95",
   "metadata": {},
   "source": [
    "## 2.4 训练过程\n",
    "**先验框匹配**\n",
    "\n",
    "在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。\n",
    "\n",
    "SSD的先验框与ground truth的匹配原则主要有两点：\n",
    "1. 对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样可以保证每个ground truth一定与某个先验框匹配。通常称与ground truth匹配的先验框为正样本，反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。\n",
    "2. 对于剩余的未匹配先验框，若某个ground truth的IOU大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配。  \n",
    "尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。\n",
    "\n",
    "注意点：\n",
    "1. 通常称与gt匹配的prior为正样本，反之，若某一个prior没有与任何一个gt匹配，则为负样本。\n",
    "2. 某个gt可以和多个prior匹配，而每个prior只能和一个gt进行匹配。\n",
    "3. 如果多个gt和某一个prior的IOU均大于阈值，那么prior只与IOU最大的那个进行匹配  \n",
    "\n",
    "在模型训练时，首先是设置模型训练的epoch次数为60，然后读取转化为mindrecord格式的训练集数据。训练集batch_size大小为32，图像尺寸统一调整为300×300；损失函数使用BCELoss，优化器使用Adam，并设置初始学习率为0.001。回调函数方面使用了LossMonitor和TimeMonitor来监控训练过程中每个epoch结束后，损失值Loss的变化情况以及每个epoch、每个step的运行时间。设置每训练10个epoch保存一次模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c5f05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 9, loss is 233.80543518066406\n",
      "Train epoch time: 9808.816 ms, per step time: 1089.868 ms\n",
      "epoch: 2 step: 9, loss is 40.247528076171875\n",
      "Train epoch time: 1461.455 ms, per step time: 162.384 ms\n",
      "epoch: 3 step: 9, loss is 40.54767608642578\n",
      "Train epoch time: 1465.138 ms, per step time: 162.793 ms\n",
      "epoch: 4 step: 9, loss is 121.43881225585938\n",
      "Train epoch time: 1467.775 ms, per step time: 163.086 ms\n",
      "epoch: 5 step: 9, loss is 153.93714904785156\n",
      "Train epoch time: 1460.328 ms, per step time: 162.259 ms\n",
      "epoch: 6 step: 9, loss is 88.31608581542969\n",
      "Train epoch time: 1467.715 ms, per step time: 163.079 ms\n",
      "epoch: 7 step: 9, loss is 38.1636962890625\n",
      "Train epoch time: 1458.762 ms, per step time: 162.085 ms\n",
      "epoch: 8 step: 9, loss is 41.95270538330078\n",
      "Train epoch time: 1482.331 ms, per step time: 164.703 ms\n",
      "epoch: 9 step: 9, loss is 26.810611724853516\n",
      "Train epoch time: 1492.504 ms, per step time: 165.834 ms\n",
      "epoch: 10 step: 9, loss is 48.34844207763672\n",
      "Train epoch time: 2475.931 ms, per step time: 275.103 ms\n",
      "epoch: 11 step: 9, loss is 31.83974838256836\n",
      "Train epoch time: 1451.048 ms, per step time: 161.228 ms\n",
      "epoch: 12 step: 9, loss is 53.46882629394531\n",
      "Train epoch time: 1465.175 ms, per step time: 162.797 ms\n",
      "epoch: 13 step: 9, loss is 35.60382843017578\n",
      "Train epoch time: 1457.248 ms, per step time: 161.916 ms\n",
      "epoch: 14 step: 9, loss is 67.38435363769531\n",
      "Train epoch time: 1467.971 ms, per step time: 163.108 ms\n",
      "epoch: 15 step: 9, loss is 41.290367126464844\n",
      "Train epoch time: 1474.129 ms, per step time: 163.792 ms\n",
      "epoch: 16 step: 9, loss is 16.410640716552734\n",
      "Train epoch time: 1469.988 ms, per step time: 163.332 ms\n",
      "epoch: 17 step: 9, loss is 21.002918243408203\n",
      "Train epoch time: 1463.248 ms, per step time: 162.583 ms\n",
      "epoch: 18 step: 9, loss is 33.2232666015625\n",
      "Train epoch time: 1480.834 ms, per step time: 164.537 ms\n",
      "epoch: 19 step: 9, loss is 27.007286071777344\n",
      "Train epoch time: 1483.737 ms, per step time: 164.860 ms\n",
      "epoch: 20 step: 9, loss is 41.071617126464844\n",
      "Train epoch time: 2583.792 ms, per step time: 287.088 ms\n",
      "epoch: 21 step: 9, loss is 24.626365661621094\n",
      "Train epoch time: 1475.154 ms, per step time: 163.906 ms\n",
      "epoch: 22 step: 9, loss is 16.457237243652344\n",
      "Train epoch time: 1501.492 ms, per step time: 166.832 ms\n",
      "epoch: 23 step: 9, loss is 22.130029678344727\n",
      "Train epoch time: 1490.548 ms, per step time: 165.616 ms\n",
      "epoch: 24 step: 9, loss is 47.68343734741211\n",
      "Train epoch time: 1483.815 ms, per step time: 164.868 ms\n",
      "epoch: 25 step: 9, loss is 20.253570556640625\n",
      "Train epoch time: 1471.871 ms, per step time: 163.541 ms\n",
      "epoch: 26 step: 9, loss is 31.227310180664062\n",
      "Train epoch time: 1694.372 ms, per step time: 188.264 ms\n",
      "epoch: 27 step: 9, loss is 47.794898986816406\n",
      "Train epoch time: 1832.508 ms, per step time: 203.612 ms\n",
      "epoch: 28 step: 9, loss is 30.15838623046875\n",
      "Train epoch time: 1641.671 ms, per step time: 182.408 ms\n",
      "epoch: 29 step: 9, loss is 5.724431037902832\n",
      "Train epoch time: 1510.849 ms, per step time: 167.872 ms\n",
      "epoch: 30 step: 9, loss is 35.56207275390625\n",
      "Train epoch time: 2534.496 ms, per step time: 281.611 ms\n",
      "epoch: 31 step: 9, loss is 27.5069580078125\n",
      "Train epoch time: 1544.610 ms, per step time: 171.623 ms\n",
      "epoch: 32 step: 9, loss is 9.36363697052002\n",
      "Train epoch time: 1479.715 ms, per step time: 164.413 ms\n",
      "epoch: 33 step: 9, loss is 21.65195083618164\n",
      "Train epoch time: 1588.668 ms, per step time: 176.519 ms\n",
      "epoch: 34 step: 9, loss is 16.62181854248047\n",
      "Train epoch time: 1714.323 ms, per step time: 190.480 ms\n",
      "epoch: 35 step: 9, loss is 34.14482498168945\n",
      "Train epoch time: 1532.654 ms, per step time: 170.295 ms\n",
      "epoch: 36 step: 9, loss is 6.122373580932617\n",
      "Train epoch time: 1707.319 ms, per step time: 189.702 ms\n",
      "epoch: 37 step: 9, loss is 18.494720458984375\n",
      "Train epoch time: 1551.808 ms, per step time: 172.423 ms\n",
      "epoch: 38 step: 9, loss is 12.783723831176758\n",
      "Train epoch time: 1508.483 ms, per step time: 167.609 ms\n",
      "epoch: 39 step: 9, loss is 13.811788558959961\n",
      "Train epoch time: 1497.016 ms, per step time: 166.335 ms\n",
      "epoch: 40 step: 9, loss is 20.23459243774414\n",
      "Train epoch time: 2498.295 ms, per step time: 277.588 ms\n",
      "epoch: 41 step: 9, loss is 9.193479537963867\n",
      "Train epoch time: 1682.733 ms, per step time: 186.970 ms\n",
      "epoch: 42 step: 9, loss is 14.361420631408691\n",
      "Train epoch time: 1675.429 ms, per step time: 186.159 ms\n",
      "epoch: 43 step: 9, loss is 10.456348419189453\n",
      "Train epoch time: 1546.899 ms, per step time: 171.878 ms\n",
      "epoch: 44 step: 9, loss is 17.647716522216797\n",
      "Train epoch time: 1543.062 ms, per step time: 171.451 ms\n",
      "epoch: 45 step: 9, loss is 8.5040922164917\n",
      "Train epoch time: 1576.231 ms, per step time: 175.137 ms\n",
      "epoch: 46 step: 9, loss is 7.399733543395996\n",
      "Train epoch time: 1524.132 ms, per step time: 169.348 ms\n",
      "epoch: 47 step: 9, loss is 14.182156562805176\n",
      "Train epoch time: 1514.601 ms, per step time: 168.289 ms\n",
      "epoch: 48 step: 9, loss is 6.025686740875244\n",
      "Train epoch time: 1662.768 ms, per step time: 184.752 ms\n",
      "epoch: 49 step: 9, loss is 7.949563026428223\n",
      "Train epoch time: 1681.440 ms, per step time: 186.827 ms\n",
      "epoch: 50 step: 9, loss is 7.4944329261779785\n",
      "Train epoch time: 2796.217 ms, per step time: 310.691 ms\n",
      "epoch: 51 step: 9, loss is 7.775108337402344\n",
      "Train epoch time: 1540.987 ms, per step time: 171.221 ms\n",
      "epoch: 52 step: 9, loss is 14.372739791870117\n",
      "Train epoch time: 1641.522 ms, per step time: 182.391 ms\n",
      "epoch: 53 step: 9, loss is 8.014837265014648\n",
      "Train epoch time: 1718.515 ms, per step time: 190.946 ms\n",
      "epoch: 54 step: 9, loss is 7.672669410705566\n",
      "Train epoch time: 1733.770 ms, per step time: 192.641 ms\n",
      "epoch: 55 step: 9, loss is 9.305501937866211\n",
      "Train epoch time: 1702.780 ms, per step time: 189.198 ms\n",
      "epoch: 56 step: 9, loss is 14.169563293457031\n",
      "Train epoch time: 1666.920 ms, per step time: 185.213 ms\n",
      "epoch: 57 step: 9, loss is 5.927455425262451\n",
      "Train epoch time: 1568.470 ms, per step time: 174.274 ms\n",
      "epoch: 58 step: 9, loss is 15.726490020751953\n",
      "Train epoch time: 1521.783 ms, per step time: 169.087 ms\n",
      "epoch: 59 step: 9, loss is 6.662771701812744\n",
      "Train epoch time: 1504.537 ms, per step time: 167.171 ms\n",
      "epoch: 60 step: 9, loss is 17.10586166381836\n",
      "Train epoch time: 2659.871 ms, per step time: 295.541 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import itertools as it\n",
    "from mindspore.train import Model\n",
    "from src.config import get_config\n",
    "import mindspore as ms\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, LossMonitor, TimeMonitor\n",
    "from src.init_params import init_net_param\n",
    "from src.lr_schedule import get_lr\n",
    "from mindspore.common import set_seed\n",
    "\n",
    "\n",
    "class TrainingWrapper(nn.Cell):\n",
    "\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(TrainingWrapper, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = ms.ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "\n",
    "    def construct(self, *args):\n",
    "        weights = self.weights\n",
    "        loss = self.network(*args)\n",
    "        sens = ops.Fill()(ops.DType()(loss), ops.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(*args, sens)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "    def generate_multi_levels(self, steps):\n",
    "        bbox_centers_list = []\n",
    "        bbox_corners_list = []\n",
    "        for step in steps:\n",
    "            bbox_centers, bbox_corners = self.generate(step)\n",
    "            bbox_centers_list.append(bbox_centers)\n",
    "            bbox_corners_list.append(bbox_corners)\n",
    "\n",
    "        self.bbox_centers = np.concatenate(bbox_centers_list, axis=0)\n",
    "        self.bbox_corners = np.concatenate(bbox_corners_list, axis=0)\n",
    "        return self.bbox_centers, self.bbox_corners\n",
    "\n",
    "class GeneratDefaultBoxes():\n",
    "    \"\"\"\n",
    "    Generate Default boxes for SSD, follows the order of (W, H, archor_sizes).\n",
    "    `self.default_boxes` has a shape of [archor_sizes, H, W, 4], the last dimension is [y, x, h, w].\n",
    "    `self.default_boxes_tlbr` has a shape as `self.default_boxes`, the last dimension is [y1, x1, y2, x2].\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # print(config)\n",
    "        fk = config.img_shape[0] / np.array(config.steps)\n",
    "        scale_rate = (config.max_scale - config.min_scale) / (len(config.num_default) - 1)\n",
    "        scales = [config.min_scale + scale_rate * i for i in range(len(config.num_default))] + [1.0]\n",
    "        self.default_boxes = []\n",
    "        for idex, feature_size in enumerate(config.feature_size):\n",
    "            sk1 = scales[idex]\n",
    "            sk2 = scales[idex + 1]\n",
    "            sk3 = math.sqrt(sk1 * sk2)\n",
    "            if idex == 0 and not config.aspect_ratios[idex]:\n",
    "                w, h = sk1 * math.sqrt(2), sk1 / math.sqrt(2)\n",
    "                all_sizes = [(0.1, 0.1), (w, h), (h, w)]\n",
    "            else:\n",
    "                all_sizes = [(sk1, sk1)]\n",
    "                for aspect_ratio in config.aspect_ratios[idex]:\n",
    "                    w, h = sk1 * math.sqrt(aspect_ratio), sk1 / math.sqrt(aspect_ratio)\n",
    "                    all_sizes.append((w, h))\n",
    "                    all_sizes.append((h, w))\n",
    "                all_sizes.append((sk3, sk3))\n",
    "\n",
    "            assert len(all_sizes) == config.num_default[idex]\n",
    "\n",
    "            for i, j in it.product(range(feature_size), repeat=2):\n",
    "                for w, h in all_sizes:\n",
    "                    cx, cy = (j + 0.5) / fk[idex], (i + 0.5) / fk[idex]\n",
    "                    self.default_boxes.append([cy, cx, h, w])\n",
    "\n",
    "        def to_tlbr(cy, cx, h, w):\n",
    "            return cy - h / 2, cx - w / 2, cy + h / 2, cx + w / 2\n",
    "\n",
    "        # For IoU calculation\n",
    "        self.default_boxes_tlbr = np.array(tuple(to_tlbr(*i) for i in self.default_boxes), dtype='float32')\n",
    "        self.default_boxes = np.array(self.default_boxes, dtype='float32')\n",
    "        \n",
    "if hasattr(config, 'use_anchor_generator') and config.use_anchor_generator:\n",
    "    generator = GridAnchorGenerator(config.img_shape, 4, 2, [1.0, 2.0, 0.5])\n",
    "    default_boxes, default_boxes_tlbr = generator.generate_multi_levels(config.steps)\n",
    "else:\n",
    "    default_boxes_tlbr = GeneratDefaultBoxes().default_boxes_tlbr\n",
    "    default_boxes = GeneratDefaultBoxes().default_boxes\n",
    "y1, x1, y2, x2 = np.split(default_boxes_tlbr[:, :4], 4, axis=-1)\n",
    "vol_anchors = (x2 - x1) * (y2 - y1)\n",
    "matching_threshold = config.match_threshold\n",
    "\n",
    "\n",
    "set_seed(1)\n",
    "# 自定义参数获取\n",
    "config = get_config()\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target= \"GPU\")\n",
    "\n",
    "# 数据加载\n",
    "mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "mindrecord_file = os.path.join(mindrecord_dir, \"ssd.mindrecord\"+ \"0\")\n",
    "\n",
    "dataset = create_ssd_dataset(mindrecord_file, batch_size=config.batch_size,rank=0, use_multiprocessing=True)\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "\n",
    "# checkpoint\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=dataset_size * config.save_checkpoint_epochs)\n",
    "ckpt_save_dir = config.output_path + '/ckpt_{}/'.format(0)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"ssd\", directory=ckpt_save_dir, config=ckpt_config)\n",
    "\n",
    "# 网络定义与初始化\n",
    "ssd = ssd_vgg16(config=config)\n",
    "init_net_param(ssd)\n",
    "# print(ssd)\n",
    "net = SSDWithLossCell(ssd, config)\n",
    "# print(net)\n",
    "lr = Tensor(get_lr(global_step=config.pre_trained_epoch_size * dataset_size,\n",
    "                   lr_init=config.lr_init, lr_end=config.lr_end_rate * config.lr, lr_max=config.lr,\n",
    "                   warmup_epochs=config.warmup_epochs,total_epochs=config.epoch_size,steps_per_epoch=dataset_size))\n",
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr,\n",
    "                  config.momentum, config.weight_decay,float(config.loss_scale))\n",
    "net = TrainingWrapper(net, opt, float(config.loss_scale))\n",
    "\n",
    "callback = [TimeMonitor(data_size=dataset_size), LossMonitor(), ckpoint_cb]\n",
    "\n",
    "model = Model(net)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "model.train(config.epoch_size, dataset, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f4d52",
   "metadata": {},
   "source": [
    "## 2.5 评估  \n",
    "#### 预测过程  \n",
    "对于每个预测框，首先根据类别置信度确定其类别（置信度最大者）与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数（解码后一般还需要做clip，防止预测框位置超出图片）。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k个预测框。最后再通过NMS算法得到预测框即检测结果。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eee955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Eval!\n",
      "Load Checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.163.378 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.163.464 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.211.914 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.211.930 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.222.246 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f7afbc6e740,python):2022-10-17-20:46:07.222.255 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f79fa7fe700,python):2022-10-17-20:46:07.224.848 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(2397,7f79fa7fe700,python):2022-10-17-20:46:07.224.859 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /media/su/sdisk/mindspore/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord3 does not contain any samples, pls remove it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "\n",
      "total images num:  9\n",
      "Processing, please wait a moment.\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.010\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.062\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.060\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.242\n",
      "\n",
      "========================================\n",
      "\n",
      "mAP: 0.0026560264153212814\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "#print(config)\n",
    "\n",
    "def ssd_eval(dataset_path, ckpt_path, anno_json):\n",
    "    \"\"\"SSD evaluation.\"\"\"\n",
    "    batch_size = 1\n",
    "    ds = create_ssd_dataset(dataset_path, batch_size=batch_size,\n",
    "                            is_training=False, use_multiprocessing=False)\n",
    "\n",
    "    net = ssd_vgg16(config=config)\n",
    "    \n",
    "    net = SsdInferWithDecoder(net, Tensor(default_boxes), config)\n",
    "\n",
    "    print(\"Load Checkpoint!\")\n",
    "    param_dict = ms.load_checkpoint(ckpt_path)\n",
    "    net.init_parameters_data()\n",
    "    ms.load_param_into_net(net, param_dict)\n",
    "\n",
    "    net.set_train(False)\n",
    "    total = ds.get_dataset_size() * batch_size\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(\"total images num: \", total)\n",
    "    print(\"Processing, please wait a moment.\")\n",
    "    eval_param_dict = {\"net\": net, \"dataset\": ds, \"anno_json\": anno_json}\n",
    "    mAP = apply_eval(eval_param_dict)\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(f\"mAP: {mAP}\")\n",
    "\n",
    "def eval_net():\n",
    "    if hasattr(config, 'num_ssd_boxes') and config.num_ssd_boxes == -1:\n",
    "        num = 0\n",
    "        h, w = config.img_shape\n",
    "        for i in range(len(config.steps)):\n",
    "            num += (h // config.steps[i]) * (w // config.steps[i]) * config.num_default[i]\n",
    "        config.num_ssd_boxes = num\n",
    "\n",
    "    coco_root = config.coco_root\n",
    "    json_path = os.path.join(coco_root, config.instances_set.format(config.val_data_type))\n",
    "\n",
    "    ms.set_context(mode=ms.GRAPH_MODE, device_target=config.device_target)\n",
    "    \n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, \"ssd_eval.mindrecord\"+ \"0\")\n",
    "\n",
    "    #mindrecord_file = create_mindrecord(config.dataset, \"ssd_eval.mindrecord\", False)\n",
    "\n",
    "    print(\"Start Eval!\")\n",
    "    ssd_eval(mindrecord_file, config.checkpoint_file_path, json_path)\n",
    "    \n",
    "eval_net()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
