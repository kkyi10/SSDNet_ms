{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2a9205",
   "metadata": {},
   "source": [
    "# 基于MindSpore框架的SSD案例实现\n",
    "##  1 模型简介\n",
    "SSD，全称Single Shot MultiBox Detector，是Wei Liu在ECCV 2016上提出的一种目标检测算法。\n",
    "目标检测主流算法分成两个类型：\n",
    "1. two-stage方法：RCNN系列<br />\n",
    "通过算法产生候选框，然后再对这些候选框进行分类和回归\n",
    "\n",
    "2. one-stage方法：yolo和SSD<br />\n",
    "直接通过主干网络给出类别位置信息，不需要区域生成<br />\n",
    "\n",
    "SSD是单阶段的目标检测算法，通过卷积神经网络进行特征提取，取不同的特征层进行检测输出，所以SSD是一种多尺度的检测方法。在需要检测的特征层，直接使用一个3$\\times$3卷积，进行通道的变换。SSD采用了anchor的策略，预设不同长宽比例的anchor，每一个输出特征层基于anchor预测多个检测框（4或者6）。采用了多尺度检测方法，浅层用于检测小目标，深层用于检测大目标。\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 模型结构\n",
    "SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。SSD的网络结构如图1所示。上面是SSD模型，下面是Yolo模型，可以明显看到SSD利用了多尺度的特征图做检测。  \n",
    "  \n",
    "![图片](./src/img/v2-a43295a3e146008b2131b160eec09cd4_r.jpg)\n",
    "<br />\n",
    "两种单阶段目标检测算法的比较：<br />\n",
    "SSD先通过卷积不断进行特征提取，在需要检测物体的网络，直接通过一个3$\\times$3卷积得到输出，卷积的通道数由anchor数量和类别数量决定，具体为(anchor数量*(类别数量+4))。  \n",
    "SSD对比了YOLO系列目标检测方法，不同的是SSD通过卷积得到最后的边界框，而YOLO对最后的输出采用全连接的形式得到一维向量，对向量进行拆解得到最终的检测框。\n",
    "### 1.2 模型特点\n",
    "  \n",
    "a)多尺度检测  \n",
    "在SSD的网络结构图中我们可以看到，SSD使用了多个特征层，特征层的尺寸分别是38$\\times$38，19$\\times$19，10$\\times$10，5$\\times$5，3$\\times$3，1$\\times$1，一共6种不同的特征图尺寸。大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体。多尺度检测的方式，可以使得检测更加充分（SSD属于密集检测），更能检测出小目标。  \n",
    "\n",
    "b)采用卷积进行检测  \n",
    "与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为m$\\times$n$\\times$p的特征图，只需要采用3$\\times$3$\\times$p这样比较小的卷积核得到检测值。  \n",
    "\n",
    "c)预设anchor  \n",
    "在yolov1中，直接由网络预测目标的尺寸，这种方式使得预测框的长宽比和尺寸没有限制，难以训练。在SSD中，采用预设边界框，我们习惯称它为anchor（在SSD论文中叫default bounding boxes），预测框的尺寸在anchor的指导下进行微调。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28267d5",
   "metadata": {},
   "source": [
    "## 2 案例实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095ccaa",
   "metadata": {},
   "source": [
    "###  2.1 环境准备与数据读取\n",
    "本案例基于MindSpore-GPU 1.8.1版本实现，在GPU上完成模型训练。  \n",
    "  \n",
    "案例所使用的数据为coco2017，考虑到原始数据集过大，从中随机划分出100张图像作为训练集tiny_train_coco2017，50张图像作为测试集tiny_val_coco2017，且将数据转换为了Mindrecord格式。  \n",
    "数据集包含训练集、验证集以及对应的json文件，目录结构如下：  \n",
    "└─tiny_coco2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─annotations  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─instance_train2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─instance_val2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─val2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─train2017  \n",
    "#### 为了更加方便地保存和加载数据，本案例中在数据读取前首先将coco数据集转换成MindRecord格式：MindRecord_COCO\n",
    "MindRecord目录结构如下：  \n",
    "└─MindRecord_COCO  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0.db  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord1.db   \n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0.db  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord1.db   \n",
    " \n",
    "MindSpore中可以把用于训练网络模型的数据集，转换为MindSpore特定的格式数据（MindSpore Record格式），从而更加方便地保存和加载数据。\n",
    "\n",
    "+ mindspore.mindrecord模块中定义了一个专门的类FileWriter可以将用户定义的原始数据写入MindRecord文件。\n",
    "\n",
    "+ 通过MindDataset接口，可以实现MindSpore Record文件的读取。\n",
    "\n",
    "+ 使用MindRecord的目标是归一化提供训练测试所用的数据集，并通过dataset模块的相关方法进行数据的读取，将这些高效的数据投入训练。\n",
    "\n",
    "\n",
    "\n",
    "![图片](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/advanced/dataset/images/data_conversion_concept.png)\n",
    "\n",
    "MindRecord具备的特征如下：\n",
    "\n",
    "1. 实现多变的用户数据统一存储、访问，训练数据读取更加简便。\n",
    "2. 数据聚合存储，高效读取，且方便管理、移动。\n",
    "3. 高效的数据编解码操作，对用户透明、无感知。\n",
    "4. 可以灵活控制分区的大小，实现分布式训练。\n",
    "\n",
    "使用MindSpore Record数据格式可以减少磁盘IO、网络IO开销，从而获得更好的使用体验和性能提升。\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64a3a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mindspore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dfd14a011c2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmindspore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmindrecord\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_coco_label\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_coco_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mindspore'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from src.config import get_config\n",
    "from src.create_coco_label import create_coco_label\n",
    "config = get_config()\n",
    "#print(config)\n",
    "\n",
    "def data_to_mindrecord_byte_image( is_training=True, prefix=\"ssd.mindrecord\", file_num=8):\n",
    "    \"\"\"Create MindRecord file.\"\"\"\n",
    "    mindrecord_path = os.path.join(config.data_path, config.mindrecord_dir, prefix)\n",
    "    writer = FileWriter(mindrecord_path, file_num)\n",
    "    images, image_path_dict, image_anno_dict = create_coco_label(is_training)\n",
    "    ssd_json = {\n",
    "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
    "        \"image\": {\"type\": \"bytes\"},\n",
    "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
    "    }\n",
    "    writer.add_schema(ssd_json, \"ssd_json\")\n",
    "\n",
    "    for img_id in images:\n",
    "        image_path = image_path_dict[img_id]\n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = f.read()\n",
    "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
    "        img_id = np.array([img_id], dtype=np.int32)\n",
    "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
    "        writer.write_raw_data([row])\n",
    "    writer.commit()\n",
    "\n",
    "\n",
    "def create_mindrecord( prefix=\"ssd.mindrecord\", is_training=True):\n",
    "\n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, prefix + \"0\")\n",
    "    os.makedirs(mindrecord_dir,exist_ok=True)\n",
    "    if not os.path.exists(mindrecord_file):\n",
    "        print(\"Create {} Mindrecord.\".format(prefix))\n",
    "        data_to_mindrecord_byte_image(is_training, prefix)\n",
    "        print(\"Create {} Mindrecord Done, at {}\".format(prefix,mindrecord_dir))\n",
    "    else:\n",
    "        print(\" {} Mindrecord exists.\".format(prefix))\n",
    "    return mindrecord_file\n",
    "#print(config)\n",
    "# 数据转换为mindrecord格式\n",
    "mindrecord_file = create_mindrecord(\"ssd.mindrecord\", True)\n",
    "eval_mindrecord_file = create_mindrecord(\"ssd_eval.mindrecord\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f834043",
   "metadata": {},
   "source": [
    "## 数据预处理  \n",
    "数据统一resize为300$\\times$300大小  \n",
    "SSD算法中采用了以下几种数据增强的方法：  \n",
    "随机裁剪：随机裁剪一个部分，每个采样部分的大小为原图的[0.3,1]，长宽比在1/2和2之间。如果真实标签框的中心位于采样部分内，则保留真实框与图片重叠的部分。  \n",
    "水平翻转：对采样后的小图片进行0.5概率的随机水平翻转 .  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a23487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from src.box_utils import jaccard_numpy, ssd_bboxes_encode\n",
    "\n",
    "def _rand(a=0., b=1.):\n",
    "    return np.random.rand() * (b - a) + a\n",
    "\n",
    "# 随机裁剪图像和box\n",
    "def random_sample_crop(image, boxes):\n",
    "    height, width, _ = image.shape\n",
    "    min_iou = np.random.choice([None, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "\n",
    "    if min_iou is None:\n",
    "        return image, boxes\n",
    "\n",
    "    # max trails (50)\n",
    "    for _ in range(50):\n",
    "        image_t = image\n",
    "        w = _rand(0.3, 1.0) * width\n",
    "        h = _rand(0.3, 1.0) * height\n",
    "        # aspect ratio constraint b/t .5 & 2\n",
    "        if h / w < 0.5 or h / w > 2:\n",
    "            continue\n",
    "\n",
    "        left = _rand() * (width - w)\n",
    "        top = _rand() * (height - h)\n",
    "        rect = np.array([int(top), int(left), int(top + h), int(left + w)])\n",
    "        overlap = jaccard_numpy(boxes, rect)\n",
    "\n",
    "        # dropout some boxes\n",
    "        drop_mask = overlap > 0\n",
    "        if not drop_mask.any():\n",
    "            continue\n",
    "\n",
    "        if overlap[drop_mask].min() < min_iou and overlap[drop_mask].max() > (min_iou + 0.2):\n",
    "            continue\n",
    "\n",
    "        image_t = image_t[rect[0]:rect[2], rect[1]:rect[3], :]\n",
    "        centers = (boxes[:, :2] + boxes[:, 2:4]) / 2.0\n",
    "        m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n",
    "        m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n",
    "\n",
    "        # mask in that both m1 and m2 are true\n",
    "        mask = m1 * m2 * drop_mask\n",
    "\n",
    "        # have any valid boxes? try again if not\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        # take only matching gt boxes\n",
    "        boxes_t = boxes[mask, :].copy()\n",
    "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], rect[:2])\n",
    "        boxes_t[:, :2] -= rect[:2]\n",
    "        boxes_t[:, 2:4] = np.minimum(boxes_t[:, 2:4], rect[2:4])\n",
    "        boxes_t[:, 2:4] -= rect[:2]\n",
    "\n",
    "        return image_t, boxes_t\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def preprocess_fn(img_id, image, box, is_training):\n",
    "    \"\"\"Preprocess function for dataset.\"\"\"\n",
    "    cv2.setNumThreads(2)\n",
    "\n",
    "    def _infer_data(image, input_shape):\n",
    "        img_h, img_w, _ = image.shape\n",
    "        input_h, input_w = input_shape\n",
    "\n",
    "        image = cv2.resize(image, (input_w, input_h))\n",
    "\n",
    "        # When the channels of image is 1\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "            image = np.concatenate([image, image, image], axis=-1)\n",
    "\n",
    "        return img_id, image, np.array((img_h, img_w), np.float32)\n",
    "\n",
    "    def _data_aug(image, box, is_training, image_size=(300, 300)):\n",
    "        ih, iw, _ = image.shape\n",
    "        h, w = image_size\n",
    "        if not is_training:\n",
    "            return _infer_data(image, image_size)\n",
    "        # Random crop\n",
    "        box = box.astype(np.float32)\n",
    "        image, box = random_sample_crop(image, box)\n",
    "        ih, iw, _ = image.shape\n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (w, h))\n",
    "        # Flip image or not\n",
    "        flip = _rand() < .5\n",
    "        if flip:\n",
    "            image = cv2.flip(image, 1, dst=None)\n",
    "        # When the channels of image is 1\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "            image = np.concatenate([image, image, image], axis=-1)\n",
    "        box[:, [0, 2]] = box[:, [0, 2]] / ih\n",
    "        box[:, [1, 3]] = box[:, [1, 3]] / iw\n",
    "        if flip:\n",
    "            box[:, [1, 3]] = 1 - box[:, [3, 1]]\n",
    "        box, label, num_match = ssd_bboxes_encode(box)\n",
    "        return image, box, label, num_match\n",
    "    return _data_aug(image, box, is_training, image_size=config.img_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d684",
   "metadata": {},
   "source": [
    "### 数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec706a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import mindspore.dataset as de\n",
    "\n",
    "def create_ssd_dataset(mindrecord_file, batch_size=32, device_num=1, rank=0,\n",
    "                       is_training=True, num_parallel_workers=1, use_multiprocessing=True):\n",
    "    \"\"\"Create SSD dataset with MindDataset.\"\"\"\n",
    "    ds = de.MindDataset(mindrecord_file, columns_list=[\"img_id\", \"image\", \"annotation\"], num_shards=device_num,\n",
    "                        shard_id=rank, num_parallel_workers=num_parallel_workers, shuffle=is_training)\n",
    "    decode = de.vision.Decode()\n",
    "    ds = ds.map(operations=decode, input_columns=[\"image\"])\n",
    "    change_swap_op = de.vision.HWC2CHW()\n",
    "    # Computed from random subset of ImageNet training images\n",
    "    normalize_op = de.vision.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                       std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    color_adjust_op = de.vision.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4)\n",
    "    compose_map_func = (lambda img_id, image, annotation: preprocess_fn(img_id, image, annotation, is_training))\n",
    "    if is_training:\n",
    "        output_columns = [\"image\", \"box\", \"label\", \"num_match\"]\n",
    "        trans = [color_adjust_op, normalize_op, change_swap_op]\n",
    "    else:\n",
    "        output_columns = [\"img_id\", \"image\", \"image_shape\"]\n",
    "        trans = [normalize_op, change_swap_op]\n",
    "    ds = ds.map(operations=compose_map_func, input_columns=[\"img_id\", \"image\", \"annotation\"],\n",
    "                output_columns=output_columns, column_order=output_columns,\n",
    "                python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.map(operations=trans, input_columns=[\"image\"], python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f75e8f",
   "metadata": {},
   "source": [
    "##  2.2 模型构建\n",
    "SSD的网络结构主要分为以下几个部分：\n",
    "* VGG16 Base Layer\n",
    "* Extra Feature Layer\n",
    "+ Detection Layer\n",
    "+ NMS<br />\n",
    "\n",
    "在整个SSD网络中，还隐藏了两个重要的部分：\n",
    "+ Anchor \n",
    "+ MultiBoxLoss\n",
    "\n",
    "**Backbone Layer**\n",
    "\n",
    "![图片](./src/img/441a293cb82c1ebecc4f67b0e03c2b05.png)  \n",
    "\n",
    "\n",
    "输入图像经过预处理后大小固定为300×300，首先经过backbone，本案例中使用的是VGG16网络的前13个卷积层，然后分别将VGG16的全连接层fc6和fc7转换成3$\\times$3卷积层block6和1$\\times$1卷积层block7，进一步提取特征。 在block6中，使用了空洞数为6的空洞卷积，其padding也为6，这样做同样也是为了增加感受野的同时保持参数量与特征图尺寸的不变。  \n",
    "\n",
    "**Extra Feature Layer**\n",
    "\n",
    "在VGG16的基础上，SSD进一步增加了4个深度卷积层，用于提取更高层的语义信息：\n",
    "![图片](./src/img/conv.png)\n",
    "block8-11，用于更高语义信息的提取。block8的通道数为512，而block9、block10与block11的通道数都为256。从block7到block11，这5个卷积后输出特征图的尺寸依次为19×19、10×10、5×5、3×3和1×1。为了降低参数量，使用了1×1卷积先降低通道数为该层输出通道数的一半，再利用3×3卷积进行特征提取。 \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11ad0e",
   "metadata": {},
   "source": [
    "**Anchor**\n",
    "\n",
    "SSD采用了PriorBox来进行区域生成。将固定大小宽高的PriorBox作为先验的感兴趣区域，利用一个阶段完成了分类与回归。PriorBox本质上是在原图上的一系列矩形框。SSD先验性地提供了以该坐标为中心的4个或6个不同大小的PriorBox，然后利用特征图的特征去预测这4个PriorBox的类别与位置偏移量。Proirbox位置的表示形式是以中心点坐标和框的宽、高(cx,cy,w,h)来表示的，同时都转换成百分比的形式。\n",
    "ProirBox生成规则：\n",
    "SSD由6个特征层来检测目标，在不同特征层上，proir box的尺寸scale大小是不一样的，最低层的scale=0.1，最高层的scale=0.95，其他层的计算公式如下：\n",
    "![图片](./src/img/sk.png)\n",
    "在某个特征层上其scale一定，那么会设置不同长宽比ratio的proir box，其长和宽的计算公式如下：\n",
    "![图片](./src/img/sk1.png)\n",
    "在ratio=1的时候，还会根据该特征层和下一个特征层计算一个特定scale的proir box(长宽比ratio=1)，计算公式如下：\n",
    "![图片](./src/img/sk3.png)\n",
    "每个特征层的每个点都会以上述规则生成proir box，(cx,cy)由当前点的中心点来确定，由此每个特征层都生成大量密集的proir box，类似于下图的效果。\n",
    "![图片](./src/img/box.png)\n",
    "\n",
    "SSD使用了第4、7、8、9、10和11这6个卷积层得到的特征图，这6个特征图尺寸越来越小，而其对应的感受野越来越大。6个特征图上的每一个点分别对应4、6、6、6、4、4个PriorBox。接下来分别利用3×3的卷积，即可得到每一个PriorBox对应的类别与位置预测量。 举个例子，第8个卷积层得到的特征图大小为10×10×512，每个点对应6个PriorBox，一共有600个PriorBox。 \n",
    "定义MultiBox类,生成多个预测框。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Anchor Generator\"\"\"\n",
    "import numpy as np\n",
    "class GridAnchorGenerator:\n",
    "    def __init__(self, image_shape, scale, scales_per_octave, aspect_ratios):\n",
    "        super(GridAnchorGenerator, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.scales_per_octave = scales_per_octave\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "\n",
    "    def generate(self, step):\n",
    "        scales = np.array([2**(float(scale) / self.scales_per_octave)\n",
    "                           for scale in range(self.scales_per_octave)]).astype(np.float32)\n",
    "        aspects = np.array(list(self.aspect_ratios)).astype(np.float32)\n",
    "\n",
    "        scales_grid, aspect_ratios_grid = np.meshgrid(scales, aspects)\n",
    "        scales_grid = scales_grid.reshape([-1])\n",
    "        aspect_ratios_grid = aspect_ratios_grid.reshape([-1])\n",
    "\n",
    "        feature_size = [self.image_shape[0] / step, self.image_shape[1] / step]\n",
    "        grid_height, grid_width = feature_size\n",
    "\n",
    "        base_size = np.array([self.scale * step, self.scale * step]).astype(np.float32)\n",
    "        anchor_offset = step / 2.0\n",
    "\n",
    "        ratio_sqrt = np.sqrt(aspect_ratios_grid)\n",
    "        heights = scales_grid / ratio_sqrt * base_size[0]\n",
    "        widths = scales_grid * ratio_sqrt * base_size[1]\n",
    "\n",
    "        y_centers = np.arange(grid_height).astype(np.float32)\n",
    "        y_centers = y_centers * step + anchor_offset\n",
    "        x_centers = np.arange(grid_width).astype(np.float32)\n",
    "        x_centers = x_centers * step + anchor_offset\n",
    "        x_centers, y_centers = np.meshgrid(x_centers, y_centers)\n",
    "\n",
    "        x_centers_shape = x_centers.shape\n",
    "        y_centers_shape = y_centers.shape\n",
    "\n",
    "        widths_grid, x_centers_grid = np.meshgrid(widths, x_centers.reshape([-1]))\n",
    "        heights_grid, y_centers_grid = np.meshgrid(heights, y_centers.reshape([-1]))\n",
    "\n",
    "        x_centers_grid = x_centers_grid.reshape(*x_centers_shape, -1)\n",
    "        y_centers_grid = y_centers_grid.reshape(*y_centers_shape, -1)\n",
    "        widths_grid = widths_grid.reshape(-1, *x_centers_shape)\n",
    "        heights_grid = heights_grid.reshape(-1, *y_centers_shape)\n",
    "\n",
    "\n",
    "        bbox_centers = np.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "        bbox_sizes = np.stack([heights_grid, widths_grid], axis=3)\n",
    "        bbox_centers = bbox_centers.reshape([-1, 2])\n",
    "        bbox_sizes = bbox_sizes.reshape([-1, 2])\n",
    "        bbox_corners = np.concatenate([bbox_centers - 0.5 * bbox_sizes, bbox_centers + 0.5 * bbox_sizes], axis=1)\n",
    "        self.bbox_corners = bbox_corners / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "        self.bbox_centers = np.concatenate([bbox_centers, bbox_sizes], axis=1)\n",
    "        self.bbox_centers = self.bbox_centers / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "\n",
    "        print(self.bbox_centers.shape)\n",
    "        return self.bbox_centers, self.bbox_corners\n",
    "\n",
    "    def generate_multi_levels(self, steps):\n",
    "        bbox_centers_list = []\n",
    "        bbox_corners_list = []\n",
    "        for step in steps:\n",
    "            bbox_centers, bbox_corners = self.generate(step)\n",
    "            bbox_centers_list.append(bbox_centers)\n",
    "            bbox_corners_list.append(bbox_corners)\n",
    "\n",
    "        self.bbox_centers = np.concatenate(bbox_centers_list, axis=0)\n",
    "        self.bbox_corners = np.concatenate(bbox_corners_list, axis=0)\n",
    "        return self.bbox_centers, self.bbox_corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354a9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from src.vgg16 import vgg16\n",
    "import mindspore.ops as ops\n",
    "import ml_collections\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"ensures that all layers have a channel number that is divisible by 8.\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def _conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same'):\n",
    "    return nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=0, pad_mode=pad_mod, has_bias=True)\n",
    "\n",
    "\n",
    "def _bn(channel):\n",
    "    return nn.BatchNorm2d(channel, eps=1e-3, momentum=0.97,\n",
    "                          gamma_init=1, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "\n",
    "\n",
    "def _last_conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same', pad=0):\n",
    "    in_channels = in_channel\n",
    "    out_channels = in_channel\n",
    "    depthwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad_mode='same',\n",
    "                               padding=pad, group=in_channels)\n",
    "    conv = _conv2d(in_channel, out_channel, kernel_size=1)\n",
    "    return nn.SequentialCell([depthwise_conv, _bn(in_channel), nn.ReLU6(), conv])\n",
    "\n",
    "\n",
    "class FlattenConcat(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(FlattenConcat, self).__init__()\n",
    "        self.num_ssd_boxes = config.num_ssd_boxes\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "        self.transpose = ops.Transpose()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        output = ()\n",
    "        batch_size = ops.shape(inputs[0])[0]\n",
    "        for x in inputs:\n",
    "            x = self.transpose(x, (0, 2, 3, 1))\n",
    "            output += (ops.reshape(x, (batch_size, -1)),)\n",
    "        res = self.concat(output)\n",
    "        return ops.reshape(res, (batch_size, self.num_ssd_boxes, -1))\n",
    "\n",
    "\n",
    "class MultiBox(nn.Cell):\n",
    "    \"\"\"\n",
    "    Multibox conv layers. Each multibox layer contains class conf scores and localization predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MultiBox, self).__init__()\n",
    "        num_classes = 81\n",
    "        out_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_default = config.num_default\n",
    "\n",
    "        loc_layers = []\n",
    "        cls_layers = []\n",
    "        for k, out_channel in enumerate(out_channels):\n",
    "            loc_layers += [_last_conv2d(out_channel, 4 * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "            cls_layers += [_last_conv2d(out_channel, num_classes * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "\n",
    "        self.multi_loc_layers = nn.layer.CellList(loc_layers)\n",
    "        self.multi_cls_layers = nn.layer.CellList(cls_layers)\n",
    "        self.flatten_concat = FlattenConcat(config)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        loc_outputs = ()\n",
    "        cls_outputs = ()\n",
    "        for i in range(len(self.multi_loc_layers)):\n",
    "            loc_outputs += (self.multi_loc_layers[i](inputs[i]),)\n",
    "            cls_outputs += (self.multi_cls_layers[i](inputs[i]),)\n",
    "        return self.flatten_concat(loc_outputs), self.flatten_concat(cls_outputs)\n",
    "\n",
    "\n",
    "class SSD300VGG16(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(SSD300VGG16, self).__init__()\n",
    "\n",
    "        # VGG16 backbone: block1~5\n",
    "        self.backbone = vgg16()\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        self.b6_1 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=6, dilation=6, pad_mode='pad')\n",
    "        self.b6_2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.b7_1 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
    "        self.b7_2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        self.b8_1 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b8_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b9_1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b9_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b10_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b10_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        self.b11_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b11_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        # boxes\n",
    "        self.multi_box = MultiBox(config)\n",
    "        if not self.training:\n",
    "            self.activation = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # VGG16 backbone: block1~5\n",
    "        block4, x = self.backbone(x)\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        x = self.b6_1(x)  # 1024\n",
    "        x = self.b6_2(x)\n",
    "\n",
    "        x = self.b7_1(x)  # 1024\n",
    "        x = self.b7_2(x)\n",
    "        block7 = x\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        x = self.b8_1(x)  # 256\n",
    "        x = self.b8_2(x)  # 512\n",
    "        block8 = x\n",
    "\n",
    "        x = self.b9_1(x)  # 128\n",
    "        x = self.b9_2(x)  # 256\n",
    "        block9 = x\n",
    "\n",
    "        x = self.b10_1(x)  # 128\n",
    "        x = self.b10_2(x)  # 256\n",
    "        block10 = x\n",
    "\n",
    "        x = self.b11_1(x)  # 128\n",
    "        x = self.b11_2(x)  # 256\n",
    "        block11 = x\n",
    "\n",
    "        # boxes\n",
    "        multi_feature = (block4, block7, block8, block9, block10, block11)\n",
    "        pred_loc, pred_label = self.multi_box(multi_feature)\n",
    "        if not self.training:\n",
    "            pred_label = self.activation(pred_label)\n",
    "        pred_loc = ops.cast(pred_loc, ms.float32)\n",
    "        pred_label = ops.cast(pred_label, ms.float32)\n",
    "        return pred_loc, pred_label\n",
    "\n",
    "\n",
    "def ssd_vgg16(**kwargs):\n",
    "    return SSD300VGG16(**kwargs)\n",
    "\n",
    "\n",
    "# ssd = ssd_vgg16(config=config)\n",
    "# print(ssd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a19ba3",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "损失函数定义为位置误差（locatization loss， loc）与置信度误差（confidence loss, conf）的加权和：\n",
    "![图片](./src/img/loss.png)  \n",
    "其中：<br />\n",
    "N 是先验框的正样本数量；<br /> \n",
    "c 为类别置信度预测值; <br />\n",
    "l 为先验框的所对应边界框的位置预测值; <br />\n",
    "g 为ground truth的位置参数  <br />\n",
    "\n",
    "**对于位置损失函数：**\n",
    "针对所有的正样本，采用 Smooth L1 Loss, 位置信息都是 encode 之后的位置信息。\n",
    "![图片](./src/img/smooth.png)  \n",
    "**对于置信度损失函数：**\n",
    "置信度损失是多类置信度(c)上的softmax损失\n",
    "![图片](./src/img/conf.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72446fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "grad_scale = ops.MultitypeFuncGraph(\"grad_scale\")\n",
    "\n",
    "class SigmoidFocalClassificationLoss(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Sigmoid focal-loss for classification.\n",
    "    Args:\n",
    "        gamma (float): Hyper-parameter to balance the easy and hard examples. Default: 2.0\n",
    "        alpha (float): Hyper-parameter to balance the positive and negative example. Default: 0.25\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(SigmoidFocalClassificationLoss, self).__init__()\n",
    "        self.sigmiod_cross_entropy = ops.SigmoidCrossEntropyWithLogits()\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.pow = ops.Pow()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.on_value = Tensor(1.0, ms.float32)\n",
    "        self.off_value = Tensor(0.0, ms.float32)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label = self.onehot(label, ops.shape(logits)[-1], self.on_value, self.off_value)\n",
    "        sigmiod_cross_entropy = self.sigmiod_cross_entropy(logits, label)\n",
    "        sigmoid = self.sigmoid(logits)\n",
    "        label = ops.cast(label, ms.float32)\n",
    "        p_t = label * sigmoid + (1 - label) * (1 - sigmoid)\n",
    "        modulating_factor = self.pow(1 - p_t, self.gamma)\n",
    "        alpha_weight_factor = label * self.alpha + (1 - label) * (1 - self.alpha)\n",
    "        focal_loss = modulating_factor * alpha_weight_factor * sigmiod_cross_entropy\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class SSDWithLossCell(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Provide SSD training loss through network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        super(SSDWithLossCell, self).__init__()\n",
    "        self.network = network\n",
    "        self.less = ops.Less()\n",
    "        self.tile = ops.Tile()\n",
    "        self.reduce_sum = ops.ReduceSum()\n",
    "        self.expand_dims = ops.ExpandDims()\n",
    "        self.class_loss = SigmoidFocalClassificationLoss(config.gamma, config.alpha)\n",
    "        self.loc_loss = nn.SmoothL1Loss()\n",
    "\n",
    "    def construct(self, x, gt_loc, gt_label, num_matched_boxes):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "        mask = ops.cast(self.less(0, gt_label), ms.float32)\n",
    "        num_matched_boxes = self.reduce_sum(ops.cast(num_matched_boxes, ms.float32))\n",
    "        # 定位损失\n",
    "        mask_loc = self.tile(self.expand_dims(mask, -1), (1, 1, 4))\n",
    "        smooth_l1 = self.loc_loss(pred_loc, gt_loc) * mask_loc\n",
    "        loss_loc = self.reduce_sum(self.reduce_sum(smooth_l1, -1), -1)\n",
    "\n",
    "        # 类别损失\n",
    "        loss_cls = self.class_loss(pred_label, gt_label)\n",
    "        loss_cls = self.reduce_sum(loss_cls, (1, 2))\n",
    "\n",
    "        return self.reduce_sum((loss_cls + loss_loc) / num_matched_boxes)\n",
    "\n",
    "# net = SSDWithLossCell(ssd, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b2cc1",
   "metadata": {},
   "source": [
    "### Metric\n",
    "在SSD中，训练过程是不需要用到非极大值抑制(NMS)，但当进行检测时，例如输入一张图片要求输出框的时候，需要用到NMS过滤掉那些重叠度较大的预测框。<br />\n",
    "非极大值抑制的流程如下：\n",
    "1. 根据置信度得分进行排序\n",
    "2. 选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除<br />\n",
    "3. 计算所有边界框的面积<br />\n",
    "4. 计算置信度最高的边界框与其它候选框的IoU<br />\n",
    "5. 删除IoU大于阈值的边界框<br />\n",
    "6. 重复上述过程，直至边界框列表为空<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590d1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore.train.callback import Callback\n",
    "import json\n",
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "\n",
    "def apply_eval(eval_param_dict):\n",
    "    net = eval_param_dict[\"net\"]\n",
    "    net.set_train(False)\n",
    "    ds = eval_param_dict[\"dataset\"]\n",
    "    anno_json = eval_param_dict[\"anno_json\"]\n",
    "    coco_metrics = COCOMetrics(anno_json=anno_json,\n",
    "                               classes=config.classes,\n",
    "                               num_classes=config.num_classes,\n",
    "                               max_boxes=config.max_boxes,\n",
    "                               nms_threshold=config.nms_threshold,\n",
    "                               min_score=config.min_score)\n",
    "    for data in ds.create_dict_iterator(output_numpy=True, num_epochs=1):\n",
    "        img_id = data['img_id']\n",
    "        img_np = data['image']\n",
    "        image_shape = data['image_shape']\n",
    "\n",
    "        output = net(Tensor(img_np))\n",
    "\n",
    "        for batch_idx in range(img_np.shape[0]):\n",
    "            pred_batch = {\n",
    "                \"boxes\": output[0].asnumpy()[batch_idx],\n",
    "                \"box_scores\": output[1].asnumpy()[batch_idx],\n",
    "                \"img_id\": int(np.squeeze(img_id[batch_idx])),\n",
    "                \"image_shape\": image_shape[batch_idx]\n",
    "            }\n",
    "            coco_metrics.update(pred_batch)\n",
    "    eval_metrics = coco_metrics.get_metrics()\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "def apply_nms(all_boxes, all_scores, thres, max_boxes):\n",
    "    \"\"\"Apply NMS to bboxes.\"\"\"\n",
    "    y1 = all_boxes[:, 0]\n",
    "    x1 = all_boxes[:, 1]\n",
    "    y2 = all_boxes[:, 2]\n",
    "    x2 = all_boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    order = all_scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        if len(keep) >= max_boxes:\n",
    "            break\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thres)[0]\n",
    "\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "\n",
    "class COCOMetrics:\n",
    "    \"\"\"Calculate mAP of predicted bboxes.\"\"\"\n",
    "\n",
    "    def __init__(self, anno_json, classes, num_classes, min_score, nms_threshold, max_boxes):\n",
    "        self.num_classes = num_classes\n",
    "        self.classes = classes\n",
    "        self.min_score = min_score\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.max_boxes = max_boxes\n",
    "\n",
    "        self.val_cls_dict = {i: cls for i, cls in enumerate(classes)}\n",
    "        self.coco_gt = COCO(anno_json)\n",
    "        cat_ids = self.coco_gt.loadCats(self.coco_gt.getCatIds())\n",
    "        self.class_dict = {cat['name']: cat['id'] for cat in cat_ids}\n",
    "\n",
    "        self.predictions = []\n",
    "        self.img_ids = []\n",
    "\n",
    "    def update(self, batch):\n",
    "        pred_boxes = batch['boxes']\n",
    "        box_scores = batch['box_scores']\n",
    "        img_id = batch['img_id']\n",
    "        h, w = batch['image_shape']\n",
    "\n",
    "        final_boxes = []\n",
    "        final_label = []\n",
    "        final_score = []\n",
    "        self.img_ids.append(img_id)\n",
    "\n",
    "        for c in range(1, self.num_classes):\n",
    "            class_box_scores = box_scores[:, c]\n",
    "            score_mask = class_box_scores > self.min_score\n",
    "            class_box_scores = class_box_scores[score_mask]\n",
    "            class_boxes = pred_boxes[score_mask] * [h, w, h, w]\n",
    "\n",
    "            if score_mask.any():\n",
    "                nms_index = apply_nms(class_boxes, class_box_scores, self.nms_threshold, self.max_boxes)\n",
    "                class_boxes = class_boxes[nms_index]\n",
    "                class_box_scores = class_box_scores[nms_index]\n",
    "\n",
    "                final_boxes += class_boxes.tolist()\n",
    "                final_score += class_box_scores.tolist()\n",
    "                final_label += [self.class_dict[self.val_cls_dict[c]]] * len(class_box_scores)\n",
    "\n",
    "        for loc, label, score in zip(final_boxes, final_label, final_score):\n",
    "            res = {}\n",
    "            res['image_id'] = img_id\n",
    "            res['bbox'] = [loc[1], loc[0], loc[3] - loc[1], loc[2] - loc[0]]\n",
    "            res['score'] = score\n",
    "            res['category_id'] = label\n",
    "            self.predictions.append(res)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        with open('predictions.json', 'w') as f:\n",
    "            json.dump(self.predictions, f)\n",
    "\n",
    "        coco_dt = self.coco_gt.loadRes('predictions.json')\n",
    "        E = COCOeval(self.coco_gt, coco_dt, iouType='bbox')\n",
    "        E.params.imgIds = self.img_ids\n",
    "        E.evaluate()\n",
    "        E.accumulate()\n",
    "        E.summarize()\n",
    "        return E.stats[0]\n",
    "\n",
    "\n",
    "class SsdInferWithDecoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    SSD Infer wrapper to decode the bbox locations.\n",
    "    Args:\n",
    "        network (Cell): the origin ssd infer network without bbox decoder.\n",
    "        default_boxes (Tensor): the default_boxes from anchor generator\n",
    "        config (dict): ssd config\n",
    "    Returns:\n",
    "        Tensor, the locations for bbox after decoder representing (y0,x0,y1,x1)\n",
    "        Tensor, the prediction labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, default_boxes, config):\n",
    "        super(SsdInferWithDecoder, self).__init__()\n",
    "        self.network = network\n",
    "        self.default_boxes = default_boxes\n",
    "        self.prior_scaling_xy = config.prior_scaling[0]\n",
    "        self.prior_scaling_wh = config.prior_scaling[1]\n",
    "\n",
    "    def construct(self, x):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "\n",
    "        default_bbox_xy = self.default_boxes[..., :2]\n",
    "        default_bbox_wh = self.default_boxes[..., 2:]\n",
    "        pred_xy = pred_loc[..., :2] * self.prior_scaling_xy * default_bbox_wh + default_bbox_xy\n",
    "        pred_wh = ops.Exp()(pred_loc[..., 2:] * self.prior_scaling_wh) * default_bbox_wh\n",
    "\n",
    "        pred_xy_0 = pred_xy - pred_wh / 2.0\n",
    "        pred_xy_1 = pred_xy + pred_wh / 2.0\n",
    "        pred_xy = ops.Concat(-1)((pred_xy_0, pred_xy_1))\n",
    "        pred_xy = ops.Maximum()(pred_xy, 0)\n",
    "        pred_xy = ops.Minimum()(pred_xy, 1)\n",
    "        return pred_xy, pred_label\n",
    "\n",
    "\n",
    "class EvalCallBack(Callback):\n",
    "    \"\"\"\n",
    "    Evaluation callback when training.\n",
    "\n",
    "    Args:\n",
    "        eval_function (function): evaluation function.\n",
    "        eval_param_dict (dict): evaluation parameters' configure dict.\n",
    "        interval (int): run evaluation interval, default is 1.\n",
    "        eval_start_epoch (int): evaluation start epoch, default is 1.\n",
    "        save_best_ckpt (bool): Whether to save best checkpoint, default is True.\n",
    "        besk_ckpt_name (str): bast checkpoint name, default is `best.ckpt`.\n",
    "        metrics_name (str): evaluation metrics name, default is `acc`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_function, eval_param_dict, interval=1, eval_start_epoch=1,\n",
    "                 ckpt_directory=\"./\", besk_ckpt_name=\"best.ckpt\", metrics_name=\"acc\"):\n",
    "        super(EvalCallBack, self).__init__()\n",
    "        self.eval_param_dict = eval_param_dict\n",
    "        self.eval_function = eval_function\n",
    "        self.eval_start_epoch = eval_start_epoch\n",
    "        self.interval = interval\n",
    "        self.best_res = 0\n",
    "        self.best_epoch = 0\n",
    "        if not os.path.isdir(ckpt_directory):\n",
    "            os.makedirs(ckpt_directory)\n",
    "        self.best_ckpt_path = os.path.join(ckpt_directory, besk_ckpt_name)\n",
    "        self.metrics_name = metrics_name\n",
    "\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"Callback when epoch end.\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        if cur_epoch >= self.eval_start_epoch and (cur_epoch - self.eval_start_epoch) % self.interval == 0:\n",
    "            res = self.eval_function(self.eval_param_dict)\n",
    "            print(\"epoch: {}, {}: {}\".format(cur_epoch, self.metrics_name, res), flush=True)\n",
    "            if res >= self.best_res:\n",
    "                self.best_res = res\n",
    "                self.best_epoch = cur_epoch\n",
    "                print(\"update best result: {}\".format(res), flush=True)\n",
    "                save_checkpoint(cb_params.train_network, self.best_ckpt_path)\n",
    "                print(\"update best checkpoint at: {}\".format(self.best_ckpt_path), flush=True)\n",
    "\n",
    "    def end(self, run_context):\n",
    "        print(\"End training, the best {0} is: {1}, the best {0} epoch is {2}\".format(self.metrics_name,\n",
    "                                                                                     self.best_res,\n",
    "                                                                                     self.best_epoch), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e45a95",
   "metadata": {},
   "source": [
    "### 训练过程\n",
    "**先验框匹配**\n",
    "\n",
    "在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。\n",
    "\n",
    "SSD的先验框与ground truth的匹配原则主要有两点：\n",
    "1. 对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样可以保证每个ground truth一定与某个先验框匹配。通常称与ground truth匹配的先验框为正样本，反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。\n",
    "2. 对于剩余的未匹配先验框，若某个ground truth的IOU大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配。  \n",
    "尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。\n",
    "\n",
    "注意点：\n",
    "1. 通常称与gt匹配的prior为正样本，反之，若某一个prior没有与任何一个gt匹配，则为负样本。\n",
    "2. 某个gt可以和多个prior匹配，而每个prior只能和一个gt进行匹配。\n",
    "3. 如果多个gt和某一个prior的IOU均大于阈值，那么prior只与IOU最大的那个进行匹配  \n",
    "\n",
    "在模型训练时，首先是设置模型训练的epoch次数为60，然后读取转化为mindrecord格式的训练集数据。训练集batch_size大小为32，图像尺寸统一调整为300×300；损失函数使用BCELoss，优化器使用Adam，并设置初始学习率为0.001。回调函数方面使用了LossMonitor和TimeMonitor来监控训练过程中每个epoch结束后，损失值Loss的变化情况以及每个epoch、每个step的运行时间。设置每训练10个epoch保存一次模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c5f05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f69fec21740,python):2022-10-14-17:24:28.217.794 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140088075867904 is not responding. Interrupt again\n",
      "[WARNING] MD(4277,7f69fec21740,python):2022-10-14-17:24:29.389.784 [mindspore/ccsrc/minddata/dataset/util/task.cc:162] Join] MapOp::WorkerEntry Thread ID 140088075867904 is not responding. Interrupt again\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:41.119.714 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 95.2311%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:41.346.099 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.0814%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:41.571.733 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.2353%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:41.800.502 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.8091%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:42.049.450 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.3708%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 9, loss is 114.43063354492188\n",
      "Train epoch time: 15357.247 ms, per step time: 1706.361 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:42.255.030 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.9258%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:42.503.591 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.4818%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:42.718.213 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.8214%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:42.915.182 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 99.2733%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 9, loss is 31.449960708618164\n",
      "Train epoch time: 2125.598 ms, per step time: 236.178 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:44.595.382 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 99.0368%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 9, loss is 53.507450103759766\n",
      "Train epoch time: 1259.129 ms, per step time: 139.903 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:45.804.705 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.8777%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 9, loss is 502.99468994140625\n",
      "Train epoch time: 1231.824 ms, per step time: 136.869 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:47.036.853 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.5597%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 9, loss is 82.18561553955078\n",
      "Train epoch time: 1204.924 ms, per step time: 133.880 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:48.242.018 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.9732%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 9, loss is 33.23680114746094\n",
      "Train epoch time: 1202.899 ms, per step time: 133.655 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:49.445.451 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4716%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 9, loss is 22.513893127441406\n",
      "Train epoch time: 1184.042 ms, per step time: 131.560 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:50.630.254 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4357%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 9, loss is 23.333145141601562\n",
      "Train epoch time: 1207.006 ms, per step time: 134.112 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:51.838.526 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4631%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 9, loss is 11.35791015625\n",
      "Train epoch time: 1205.015 ms, per step time: 133.891 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:53.043.673 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4952%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 9, loss is 9.789457321166992\n",
      "Train epoch time: 3247.890 ms, per step time: 360.877 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:56.446.108 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.4521%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 step: 9, loss is 8.991650581359863\n",
      "Train epoch time: 1492.878 ms, per step time: 165.875 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:57.818.152 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.5062%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 9, loss is 8.158019065856934\n",
      "Train epoch time: 1242.793 ms, per step time: 138.088 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:24:59.061.553 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.5693%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 9, loss is 7.13227653503418\n",
      "Train epoch time: 1228.481 ms, per step time: 136.498 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:00.290.291 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.6485%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 9, loss is 8.392068862915039\n",
      "Train epoch time: 1179.152 ms, per step time: 131.017 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:01.470.068 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.6519%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 9, loss is 8.414968490600586\n",
      "Train epoch time: 1191.986 ms, per step time: 132.443 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:02.662.393 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7112%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 9, loss is 6.670163154602051\n",
      "Train epoch time: 1224.498 ms, per step time: 136.055 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:03.887.750 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7539%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 9, loss is 6.927101135253906\n",
      "Train epoch time: 1210.542 ms, per step time: 134.505 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:05.099.596 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7818%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 9, loss is 8.05196475982666\n",
      "Train epoch time: 1267.803 ms, per step time: 140.867 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:06.368.059 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7722%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 9, loss is 6.5831732749938965\n",
      "Train epoch time: 1186.729 ms, per step time: 131.859 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:07.555.294 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7847%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 9, loss is 7.778858661651611\n",
      "Train epoch time: 1885.792 ms, per step time: 209.532 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:09.564.325 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.2986%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 9, loss is 7.211112976074219\n",
      "Train epoch time: 1316.847 ms, per step time: 146.316 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:10.798.948 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.5702%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 9, loss is 5.867620468139648\n",
      "Train epoch time: 1216.776 ms, per step time: 135.197 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:12.016.192 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.6056%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 9, loss is 9.66279411315918\n",
      "Train epoch time: 1227.932 ms, per step time: 136.437 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:13.244.554 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.6185%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 9, loss is 7.6253838539123535\n",
      "Train epoch time: 1218.398 ms, per step time: 135.378 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:14.463.552 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7088%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 step: 9, loss is 7.692754745483398\n",
      "Train epoch time: 1285.108 ms, per step time: 142.790 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:15.749.453 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.9637%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 9, loss is 6.751593589782715\n",
      "Train epoch time: 1229.062 ms, per step time: 136.562 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:16.978.836 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.6401%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 9, loss is 6.185293197631836\n",
      "Train epoch time: 1199.915 ms, per step time: 133.324 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:18.180.113 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.347%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 9, loss is 6.325072288513184\n",
      "Train epoch time: 1287.446 ms, per step time: 143.050 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:19.469.872 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.3391%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 9, loss is 5.667670249938965\n",
      "Train epoch time: 1215.463 ms, per step time: 135.051 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:20.683.339 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 98.2116%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 9, loss is 5.730759620666504\n",
      "Train epoch time: 1899.738 ms, per step time: 211.082 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:22.685.978 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.6307%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 step: 9, loss is 6.479983329772949\n",
      "Train epoch time: 1223.884 ms, per step time: 135.987 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:23.896.626 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.7657%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 step: 9, loss is 6.41501522064209\n",
      "Train epoch time: 1212.664 ms, per step time: 134.740 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:25.109.763 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 96.75%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 step: 9, loss is 5.922553062438965\n",
      "Train epoch time: 1245.807 ms, per step time: 138.423 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:26.356.614 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.284%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 step: 9, loss is 6.147653579711914\n",
      "Train epoch time: 1240.622 ms, per step time: 137.847 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:27.597.831 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4728%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 step: 9, loss is 5.500984191894531\n",
      "Train epoch time: 1226.605 ms, per step time: 136.289 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:28.826.071 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.6617%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 step: 9, loss is 5.793822288513184\n",
      "Train epoch time: 1194.948 ms, per step time: 132.772 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:30.020.379 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.7215%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 step: 9, loss is 5.918275833129883\n",
      "Train epoch time: 1188.918 ms, per step time: 132.102 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:31.209.631 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.7254%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 step: 9, loss is 5.80874490737915\n",
      "Train epoch time: 1181.769 ms, per step time: 131.308 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:32.391.724 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.8203%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 step: 9, loss is 5.888369083404541\n",
      "Train epoch time: 1185.665 ms, per step time: 131.741 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:33.577.958 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.782%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 step: 9, loss is 5.738338470458984\n",
      "Train epoch time: 1965.857 ms, per step time: 218.429 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:35.701.233 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.3039%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 step: 9, loss is 5.0232439041137695\n",
      "Train epoch time: 1498.748 ms, per step time: 166.528 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:37.080.898 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4184%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 step: 9, loss is 5.848567962646484\n",
      "Train epoch time: 1182.498 ms, per step time: 131.389 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:38.264.131 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4512%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 step: 9, loss is 6.699341773986816\n",
      "Train epoch time: 1183.761 ms, per step time: 131.529 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:39.448.486 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4674%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 step: 9, loss is 5.956066131591797\n",
      "Train epoch time: 1183.323 ms, per step time: 131.480 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:40.632.313 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4783%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 step: 9, loss is 5.5626540184021\n",
      "Train epoch time: 1183.412 ms, per step time: 131.490 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:41.815.897 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.4964%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 step: 9, loss is 5.833881378173828\n",
      "Train epoch time: 1183.187 ms, per step time: 131.465 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:42.999.675 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.5033%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 step: 9, loss is 5.306423187255859\n",
      "Train epoch time: 1183.205 ms, per step time: 131.467 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:44.183.335 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.5157%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 step: 9, loss is 5.971409320831299\n",
      "Train epoch time: 1181.023 ms, per step time: 131.225 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:45.364.764 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.513%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 step: 9, loss is 5.907222747802734\n",
      "Train epoch time: 1184.394 ms, per step time: 131.599 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:46.550.059 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.5155%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 step: 9, loss is 5.86525821685791\n",
      "Train epoch time: 1902.749 ms, per step time: 211.417 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:48.561.708 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.1579%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 step: 9, loss is 6.20849609375\n",
      "Train epoch time: 1478.397 ms, per step time: 164.266 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:49.948.147 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.5128%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52 step: 9, loss is 5.043522357940674\n",
      "Train epoch time: 1182.525 ms, per step time: 131.392 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(4277,7f67eb7fe700,python):2022-10-14-17:25:51.130.993 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:136] operator()] Memory consumption is more than 97.5091%, which may cause oom error. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53 step: 9, loss is 5.916744709014893\n",
      "Train epoch time: 1182.979 ms, per step time: 131.442 ms\n",
      "epoch: 54 step: 9, loss is 5.918503761291504\n",
      "Train epoch time: 1183.198 ms, per step time: 131.466 ms\n",
      "epoch: 55 step: 9, loss is 5.388006210327148\n",
      "Train epoch time: 1182.550 ms, per step time: 131.394 ms\n",
      "epoch: 56 step: 9, loss is 6.243959426879883\n",
      "Train epoch time: 1182.741 ms, per step time: 131.416 ms\n",
      "epoch: 57 step: 9, loss is 5.736234188079834\n",
      "Train epoch time: 1184.688 ms, per step time: 131.632 ms\n",
      "epoch: 58 step: 9, loss is 5.856206893920898\n",
      "Train epoch time: 1183.268 ms, per step time: 131.474 ms\n",
      "epoch: 59 step: 9, loss is 5.255853652954102\n",
      "Train epoch time: 1185.140 ms, per step time: 131.682 ms\n",
      "epoch: 60 step: 9, loss is 5.251133918762207\n",
      "Train epoch time: 1805.511 ms, per step time: 200.612 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mindspore.train import Model\n",
    "from src.config import get_config\n",
    "import mindspore as ms\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, LossMonitor, TimeMonitor\n",
    "from src.init_params import init_net_param\n",
    "from src.lr_schedule import get_lr\n",
    "from mindspore.common import set_seed\n",
    "from src.box_utils import default_boxes\n",
    "\n",
    "\n",
    "class TrainingWrapper(nn.Cell):\n",
    "\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(TrainingWrapper, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = ms.ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "\n",
    "    def construct(self, *args):\n",
    "        weights = self.weights\n",
    "        loss = self.network(*args)\n",
    "        sens = ops.Fill()(ops.DType()(loss), ops.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(*args, sens)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "\n",
    "set_seed(1)\n",
    "# 自定义参数获取\n",
    "config = get_config()\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target= \"GPU\")\n",
    "\n",
    "# 数据加载\n",
    "mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "mindrecord_file = os.path.join(mindrecord_dir, \"ssd.mindrecord\"+ \"0\")\n",
    "\n",
    "dataset = create_ssd_dataset(mindrecord_file, batch_size=config.batch_size,rank=0, use_multiprocessing=True)\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "\n",
    "# checkpoint\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=dataset_size * config.save_checkpoint_epochs)\n",
    "ckpt_save_dir = config.output_path + '/ckpt_{}/'.format(0)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"ssd\", directory=ckpt_save_dir, config=ckpt_config)\n",
    "\n",
    "# 网络定义与初始化\n",
    "ssd = ssd_vgg16(config=config)\n",
    "init_net_param(ssd)\n",
    "# print(ssd)\n",
    "net = SSDWithLossCell(ssd, config)\n",
    "# print(net)\n",
    "lr = Tensor(get_lr(global_step=config.pre_trained_epoch_size * dataset_size,\n",
    "                   lr_init=config.lr_init, lr_end=config.lr_end_rate * config.lr, lr_max=config.lr,\n",
    "                   warmup_epochs=config.warmup_epochs,total_epochs=config.epoch_size,steps_per_epoch=dataset_size))\n",
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr,\n",
    "                  config.momentum, config.weight_decay,float(config.loss_scale))\n",
    "net = TrainingWrapper(net, opt, float(config.loss_scale))\n",
    "\n",
    "callback = [TimeMonitor(data_size=dataset_size), LossMonitor(), ckpoint_cb]\n",
    "\n",
    "model = Model(net)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "model.train(config.epoch_size, dataset, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f4d52",
   "metadata": {},
   "source": [
    "### 评估  \n",
    "#### 预测过程  \n",
    "对于每个预测框，首先根据类别置信度确定其类别（置信度最大者）与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值（如0.5）过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数（解码后一般还需要做clip，防止预测框位置超出图片）。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k个预测框。最后再通过NMS算法得到预测框即检测结果。  \n",
    "\n",
    "实例化自定义的回调类EvalCallBack，实现计算每个epoch结束后的评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eee955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.75\n",
      "aspect_ratios:\n",
      "- - 2\n",
      "- - 2\n",
      "  - 3\n",
      "- - 2\n",
      "  - 3\n",
      "- - 2\n",
      "  - 3\n",
      "- - 2\n",
      "- - 2\n",
      "batch_size: 5\n",
      "checkpoint_file_path: ./cache/train/ckpt_0/ssd-60_9.ckpt\n",
      "checkpoint_path: ./checkpoint/\n",
      "classes:\n",
      "- background\n",
      "- person\n",
      "- bicycle\n",
      "- car\n",
      "- motorcycle\n",
      "- airplane\n",
      "- bus\n",
      "- train\n",
      "- truck\n",
      "- boat\n",
      "- traffic light\n",
      "- fire hydrant\n",
      "- stop sign\n",
      "- parking meter\n",
      "- bench\n",
      "- bird\n",
      "- cat\n",
      "- dog\n",
      "- horse\n",
      "- sheep\n",
      "- cow\n",
      "- elephant\n",
      "- bear\n",
      "- zebra\n",
      "- giraffe\n",
      "- backpack\n",
      "- umbrella\n",
      "- handbag\n",
      "- tie\n",
      "- suitcase\n",
      "- frisbee\n",
      "- skis\n",
      "- snowboard\n",
      "- sports ball\n",
      "- kite\n",
      "- baseball bat\n",
      "- baseball glove\n",
      "- skateboard\n",
      "- surfboard\n",
      "- tennis racket\n",
      "- bottle\n",
      "- wine glass\n",
      "- cup\n",
      "- fork\n",
      "- knife\n",
      "- spoon\n",
      "- bowl\n",
      "- banana\n",
      "- apple\n",
      "- sandwich\n",
      "- orange\n",
      "- broccoli\n",
      "- carrot\n",
      "- hot dog\n",
      "- pizza\n",
      "- donut\n",
      "- cake\n",
      "- chair\n",
      "- couch\n",
      "- potted plant\n",
      "- bed\n",
      "- dining table\n",
      "- toilet\n",
      "- tv\n",
      "- laptop\n",
      "- mouse\n",
      "- remote\n",
      "- keyboard\n",
      "- cell phone\n",
      "- microwave\n",
      "- oven\n",
      "- toaster\n",
      "- sink\n",
      "- refrigerator\n",
      "- book\n",
      "- clock\n",
      "- vase\n",
      "- scissors\n",
      "- teddy bear\n",
      "- hair drier\n",
      "- toothbrush\n",
      "coco_root: /home/ll/ms-redpanda/ssd_ms/mini_dataset/\n",
      "data_path: /home/ll/ms-redpanda/ssd_ms/mini_dataset/\n",
      "dataset: coco\n",
      "device_id: 0\n",
      "device_target: GPU\n",
      "epoch_size: 60\n",
      "eval_interval: 2\n",
      "eval_start_epoch: 3\n",
      "extras_ratio:\n",
      "- 0.2\n",
      "- 0.2\n",
      "- 0.2\n",
      "- 0.25\n",
      "- 0.5\n",
      "- 0.25\n",
      "feature_size:\n",
      "- 38\n",
      "- 19\n",
      "- 10\n",
      "- 5\n",
      "- 3\n",
      "- 1\n",
      "gamma: 2.0\n",
      "img_shape:\n",
      "- 300\n",
      "- 300\n",
      "instances_set: annotations/instances_{}.json\n",
      "load_path: ./cache/checkpoint_path/\n",
      "loss_scale: 1024\n",
      "lr: 0.05\n",
      "lr_end_rate: 0.001\n",
      "lr_init: 0.001\n",
      "match_threshold: 0.5\n",
      "max_boxes: 100\n",
      "max_scale: 0.95\n",
      "min_scale: 0.1\n",
      "min_score: 0.1\n",
      "mindrecord_dir: MindRecord_COCO\n",
      "momentum: 0.9\n",
      "nms_threshold: 0.6\n",
      "num_classes: 81\n",
      "num_default:\n",
      "- 4\n",
      "- 6\n",
      "- 6\n",
      "- 6\n",
      "- 4\n",
      "- 4\n",
      "num_ssd_boxes: 8732\n",
      "output_path: ./cache/train\n",
      "pre_trained_epoch_size: 0\n",
      "prior_scaling:\n",
      "- 0.1\n",
      "- 0.2\n",
      "run_eval: true\n",
      "save_checkpoint_epochs: 10\n",
      "steps:\n",
      "- 8\n",
      "- 16\n",
      "- 32\n",
      "- 64\n",
      "- 100\n",
      "- 300\n",
      "train_data_type: train2017\n",
      "val_data_type: val2017\n",
      "warmup_epochs: 2\n",
      "weight_decay: 0.00015\n",
      "\n",
      "Start Eval!\n",
      "Load Checkpoint!\n",
      "\n",
      "========================================\n",
      "\n",
      "total images num:  9\n",
      "Processing, please wait a moment.\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(8859,7efeedf7b740,python):2022-10-14-20:09:17.463.842 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efeedf7b740,python):2022-10-14-20:09:17.463.861 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord4 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efeedf7b740,python):2022-10-14-20:09:17.468.981 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efeedf7b740,python):2022-10-14-20:09:17.468.996 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord4 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efeedf7b740,python):2022-10-14-20:09:17.471.996 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efeedf7b740,python):2022-10-14-20:09:17.472.006 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord4 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efe01739700,python):2022-10-14-20:09:17.474.766 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord1 does not contain any samples, pls remove it.\n",
      "[WARNING] MD(8859,7efe01739700,python):2022-10-14-20:09:17.474.783 [mindspore/ccsrc/minddata/mindrecord/io/shard_reader.cc:360] ReadRowGroupSummary] The mindrecord file: /home/ll/ms-redpanda/ssd_ms/mini_dataset/MindRecord_COCO/ssd_eval.mindrecord4 does not contain any samples, pls remove it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.45s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.17s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.045\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.069\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.291\n",
      "\n",
      "========================================\n",
      "\n",
      "mAP: 0.0030771937954464077\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "from src.box_utils import default_boxes\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "#print(config)\n",
    "\n",
    "def ssd_eval(dataset_path, ckpt_path, anno_json):\n",
    "    \"\"\"SSD evaluation.\"\"\"\n",
    "    batch_size = 1\n",
    "    ds = create_ssd_dataset(dataset_path, batch_size=batch_size,\n",
    "                            is_training=False, use_multiprocessing=False)\n",
    "\n",
    "    net = ssd_vgg16(config=config)\n",
    "    \n",
    "    net = SsdInferWithDecoder(net, Tensor(default_boxes), config)\n",
    "\n",
    "    print(\"Load Checkpoint!\")\n",
    "    param_dict = ms.load_checkpoint(ckpt_path)\n",
    "    net.init_parameters_data()\n",
    "    ms.load_param_into_net(net, param_dict)\n",
    "\n",
    "    net.set_train(False)\n",
    "    total = ds.get_dataset_size() * batch_size\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(\"total images num: \", total)\n",
    "    print(\"Processing, please wait a moment.\")\n",
    "    eval_param_dict = {\"net\": net, \"dataset\": ds, \"anno_json\": anno_json}\n",
    "    mAP = apply_eval(eval_param_dict)\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(f\"mAP: {mAP}\")\n",
    "\n",
    "def eval_net():\n",
    "    if hasattr(config, 'num_ssd_boxes') and config.num_ssd_boxes == -1:\n",
    "        num = 0\n",
    "        h, w = config.img_shape\n",
    "        for i in range(len(config.steps)):\n",
    "            num += (h // config.steps[i]) * (w // config.steps[i]) * config.num_default[i]\n",
    "        config.num_ssd_boxes = num\n",
    "\n",
    "    coco_root = config.coco_root\n",
    "    json_path = os.path.join(coco_root, config.instances_set.format(config.val_data_type))\n",
    "\n",
    "    ms.set_context(mode=ms.GRAPH_MODE, device_target=config.device_target)\n",
    "    \n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, \"ssd_eval.mindrecord\"+ \"0\")\n",
    "\n",
    "    #mindrecord_file = create_mindrecord(config.dataset, \"ssd_eval.mindrecord\", False)\n",
    "\n",
    "    print(\"Start Eval!\")\n",
    "    ssd_eval(mindrecord_file, config.checkpoint_file_path, json_path)\n",
    "    \n",
    "eval_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23fe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
