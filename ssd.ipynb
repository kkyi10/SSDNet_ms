{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2a9205",
   "metadata": {},
   "source": [
    "# 基于MindSpore框架的SSD案例实现\n",
    "##  1 模型简介\n",
    "SSD，全称Single Shot MultiBox Detector，是Wei Liu在ECCV 2016上提出的一种目标检测算法。\n",
    "目标检测主流算法分成两个类型：\n",
    "1. two-stage方法：RCNN系列<br />\n",
    "通过算法产生候选框，然后再对这些候选框进行分类和回归\n",
    "\n",
    "2. one-stage方法：yolo和SSD<br />\n",
    "直接通过主干网络给出类别位置信息，不需要区域生成<br />\n",
    "\n",
    "SSD是单阶段的目标检测算法，通过卷积神经网络进行特征提取，取不同的特征层进行检测输出，所以SSD是一种多尺度的检测方法。在需要检测的特征层，直接使用一个3$\\times$3卷积，进行通道的变换。SSD采用了anchor的策略，预设不同长宽比例的anchor，每一个输出特征层基于anchor预测多个检测框（4或者6）。采用了多尺度检测方法，浅层用于检测小目标，深层用于检测大目标。\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 模型结构\n",
    "SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。SSD的网络结构如图1所示。上面是SSD模型，下面是Yolo模型，可以明显看到SSD利用了多尺度的特征图做检测。  \n",
    "  \n",
    "![图片](./src/img/v2-a43295a3e146008b2131b160eec09cd4_r.jpg)\n",
    "<br />\n",
    "两种单阶段目标检测算法的比较：<br />\n",
    "SSD先通过卷积不断进行特征提取，在需要检测物体的网络，直接通过一个3$\\times$3卷积得到输出，卷积的通道数由anchor数量和类别数量决定，具体为(anchor数量*(类别数量+4))。  \n",
    "SSD对比了YOLO系列目标检测方法，不同的是SSD通过卷积得到最后的边界框，而YOLO对最后的输出采用全连接的形式得到一维向量，对向量进行拆解得到最终的检测框。\n",
    "### 1.2 模型特点\n",
    "  \n",
    "a)多尺度检测  \n",
    "在SSD的网络结构图中我们可以看到，SSD使用了多个特征层，特征层的尺寸分别是38$\\times$38，19$\\times$19，10$\\times$10，5$\\times$5，3$\\times$3，1$\\times$1，一共6种不同的特征图尺寸。大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体。多尺度检测的方式，可以使得检测更加充分（SSD属于密集检测），更能检测出小目标。  \n",
    "\n",
    "b)采用卷积进行检测  \n",
    "与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为m$\\times$n$\\times$p的特征图，只需要采用3$\\times$3$\\times$p这样比较小的卷积核得到检测值。  \n",
    "\n",
    "c)预设anchor  \n",
    "在yolov1中，直接由网络预测目标的尺寸，这种方式使得预测框的长宽比和尺寸没有限制，难以训练。在SSD中，采用预设边界框，我们习惯称它为anchor（在SSD论文中叫default bounding boxes），预测框的尺寸在anchor的指导下进行微调。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28267d5",
   "metadata": {},
   "source": [
    "## 2 案例实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095ccaa",
   "metadata": {},
   "source": [
    "###  2.1 环境准备与数据读取\n",
    "本案例基于MindSpore-GPU 1.8.1版本实现，在GPU上完成模型训练。  \n",
    "  \n",
    "案例所使用的数据为coco2017，考虑到原始数据集过大，从中随机划分出100张图像作为训练集tiny_train_coco2017，50张图像作为测试集tiny_val_coco2017，且将数据转换为了Mindrecord格式。  \n",
    "数据集包含训练集、验证集以及对应的json文件，目录结构如下：  \n",
    "└─tiny_coco2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─annotations  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─instance_train2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─instance_val2017.json  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─val2017  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─train2017  \n",
    "#### 为了更加方便地保存和加载数据，本案例中在数据读取前首先将coco数据集转换成MindRecord格式：MindRecord_COCO\n",
    "MindRecord目录结构如下：  \n",
    "└─MindRecord_COCO  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord0.db  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd.mindrecord1.db   \n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord0.db  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├─ssd_eval.mindrecord1.db   \n",
    " \n",
    "MindSpore中可以把用于训练网络模型的数据集，转换为MindSpore特定的格式数据（MindSpore Record格式），从而更加方便地保存和加载数据。\n",
    "\n",
    "+ mindspore.mindrecord模块中定义了一个专门的类FileWriter可以将用户定义的原始数据写入MindRecord文件。\n",
    "\n",
    "+ 通过MindDataset接口，可以实现MindSpore Record文件的读取。\n",
    "\n",
    "+ 使用MindRecord的目标是归一化提供训练测试所用的数据集，并通过dataset模块的相关方法进行数据的读取，将这些高效的数据投入训练。\n",
    "\n",
    "\n",
    "\n",
    "![图片](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/tutorials/source_zh_cn/advanced/dataset/images/data_conversion_concept.png)\n",
    "\n",
    "MindRecord具备的特征如下：\n",
    "\n",
    "1. 实现多变的用户数据统一存储、访问，训练数据读取更加简便。\n",
    "2. 数据聚合存储，高效读取，且方便管理、移动。\n",
    "3. 高效的数据编解码操作，对用户透明、无感知。\n",
    "4. 可以灵活控制分区的大小，实现分布式训练。\n",
    "\n",
    "使用MindSpore Record数据格式可以减少磁盘IO、网络IO开销，从而获得更好的使用体验和性能提升。\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64a3a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ssd.mindrecord Mindrecord exists.\n",
      " ssd_eval.mindrecord Mindrecord exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from src.config import get_config\n",
    "from src.create_coco_label import create_coco_label\n",
    "config = get_config()\n",
    "#print(config)\n",
    "\n",
    "def data_to_mindrecord_byte_image( is_training=True, prefix=\"ssd.mindrecord\", file_num=8):\n",
    "    \"\"\"Create MindRecord file.\"\"\"\n",
    "    mindrecord_path = os.path.join(config.data_path, config.mindrecord_dir, prefix)\n",
    "    writer = FileWriter(mindrecord_path, file_num)\n",
    "    images, image_path_dict, image_anno_dict = create_coco_label(is_training)\n",
    "    ssd_json = {\n",
    "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
    "        \"image\": {\"type\": \"bytes\"},\n",
    "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
    "    }\n",
    "    writer.add_schema(ssd_json, \"ssd_json\")\n",
    "\n",
    "    for img_id in images:\n",
    "        image_path = image_path_dict[img_id]\n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = f.read()\n",
    "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
    "        img_id = np.array([img_id], dtype=np.int32)\n",
    "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
    "        writer.write_raw_data([row])\n",
    "    writer.commit()\n",
    "\n",
    "\n",
    "def create_mindrecord( prefix=\"ssd.mindrecord\", is_training=True):\n",
    "\n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, prefix + \"0\")\n",
    "    os.makedirs(mindrecord_dir,exist_ok=True)\n",
    "    if not os.path.exists(mindrecord_file):\n",
    "        print(\"Create {} Mindrecord.\".format(prefix))\n",
    "        data_to_mindrecord_byte_image(is_training, prefix)\n",
    "        print(\"Create {} Mindrecord Done, at {}\".format(prefix,mindrecord_dir))\n",
    "    else:\n",
    "        print(\" {} Mindrecord exists.\".format(prefix))\n",
    "    return mindrecord_file\n",
    "#print(config)\n",
    "# 数据转换为mindrecord格式\n",
    "mindrecord_file = create_mindrecord(\"ssd.mindrecord\", True)\n",
    "eval_mindrecord_file = create_mindrecord(\"ssd_eval.mindrecord\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f834043",
   "metadata": {},
   "source": [
    "## 数据预处理  \n",
    "数据统一resize为300$\\times$300大小  \n",
    "SSD算法中采用了以下几种数据增强的方法：  \n",
    "随机裁剪：随机裁剪一个部分，每个采样部分的大小为原图的[0.3,1]，长宽比在1/2和2之间。如果真实标签框的中心位于采样部分内，则保留真实框与图片重叠的部分。  \n",
    "水平翻转：对采样后的小图片进行0.5概率的随机水平翻转 .  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a23487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from src.box_utils import jaccard_numpy, ssd_bboxes_encode\n",
    "\n",
    "def _rand(a=0., b=1.):\n",
    "    return np.random.rand() * (b - a) + a\n",
    "\n",
    "# 随机裁剪图像和box\n",
    "def random_sample_crop(image, boxes):\n",
    "    height, width, _ = image.shape\n",
    "    min_iou = np.random.choice([None, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "\n",
    "    if min_iou is None:\n",
    "        return image, boxes\n",
    "\n",
    "    # max trails (50)\n",
    "    for _ in range(50):\n",
    "        image_t = image\n",
    "        w = _rand(0.3, 1.0) * width\n",
    "        h = _rand(0.3, 1.0) * height\n",
    "        # aspect ratio constraint b/t .5 & 2\n",
    "        if h / w < 0.5 or h / w > 2:\n",
    "            continue\n",
    "\n",
    "        left = _rand() * (width - w)\n",
    "        top = _rand() * (height - h)\n",
    "        rect = np.array([int(top), int(left), int(top + h), int(left + w)])\n",
    "        overlap = jaccard_numpy(boxes, rect)\n",
    "\n",
    "        # dropout some boxes\n",
    "        drop_mask = overlap > 0\n",
    "        if not drop_mask.any():\n",
    "            continue\n",
    "\n",
    "        if overlap[drop_mask].min() < min_iou and overlap[drop_mask].max() > (min_iou + 0.2):\n",
    "            continue\n",
    "\n",
    "        image_t = image_t[rect[0]:rect[2], rect[1]:rect[3], :]\n",
    "        centers = (boxes[:, :2] + boxes[:, 2:4]) / 2.0\n",
    "        m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n",
    "        m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n",
    "\n",
    "        # mask in that both m1 and m2 are true\n",
    "        mask = m1 * m2 * drop_mask\n",
    "\n",
    "        # have any valid boxes? try again if not\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        # take only matching gt boxes\n",
    "        boxes_t = boxes[mask, :].copy()\n",
    "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], rect[:2])\n",
    "        boxes_t[:, :2] -= rect[:2]\n",
    "        boxes_t[:, 2:4] = np.minimum(boxes_t[:, 2:4], rect[2:4])\n",
    "        boxes_t[:, 2:4] -= rect[:2]\n",
    "\n",
    "        return image_t, boxes_t\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def preprocess_fn(img_id, image, box, is_training):\n",
    "    \"\"\"Preprocess function for dataset.\"\"\"\n",
    "    cv2.setNumThreads(2)\n",
    "\n",
    "    def _infer_data(image, input_shape):\n",
    "        img_h, img_w, _ = image.shape\n",
    "        input_h, input_w = input_shape\n",
    "\n",
    "        image = cv2.resize(image, (input_w, input_h))\n",
    "\n",
    "        # When the channels of image is 1\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "            image = np.concatenate([image, image, image], axis=-1)\n",
    "\n",
    "        return img_id, image, np.array((img_h, img_w), np.float32)\n",
    "\n",
    "    def _data_aug(image, box, is_training, image_size=(300, 300)):\n",
    "        ih, iw, _ = image.shape\n",
    "        h, w = image_size\n",
    "        if not is_training:\n",
    "            return _infer_data(image, image_size)\n",
    "        # Random crop\n",
    "        box = box.astype(np.float32)\n",
    "        image, box = random_sample_crop(image, box)\n",
    "        ih, iw, _ = image.shape\n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (w, h))\n",
    "        # Flip image or not\n",
    "        flip = _rand() < .5\n",
    "        if flip:\n",
    "            image = cv2.flip(image, 1, dst=None)\n",
    "        # When the channels of image is 1\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "            image = np.concatenate([image, image, image], axis=-1)\n",
    "        box[:, [0, 2]] = box[:, [0, 2]] / ih\n",
    "        box[:, [1, 3]] = box[:, [1, 3]] / iw\n",
    "        if flip:\n",
    "            box[:, [1, 3]] = 1 - box[:, [3, 1]]\n",
    "        box, label, num_match = ssd_bboxes_encode(box)\n",
    "        return image, box, label, num_match\n",
    "    return _data_aug(image, box, is_training, image_size=config.img_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d684",
   "metadata": {},
   "source": [
    "### 数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec706a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import mindspore.dataset as de\n",
    "\n",
    "def create_ssd_dataset(mindrecord_file, batch_size=32, device_num=1, rank=0,\n",
    "                       is_training=True, num_parallel_workers=1, use_multiprocessing=True):\n",
    "    \"\"\"Create SSD dataset with MindDataset.\"\"\"\n",
    "    ds = de.MindDataset(mindrecord_file, columns_list=[\"img_id\", \"image\", \"annotation\"], num_shards=device_num,\n",
    "                        shard_id=rank, num_parallel_workers=num_parallel_workers, shuffle=is_training)\n",
    "    decode = de.vision.Decode()\n",
    "    ds = ds.map(operations=decode, input_columns=[\"image\"])\n",
    "    change_swap_op = de.vision.HWC2CHW()\n",
    "    # Computed from random subset of ImageNet training images\n",
    "    normalize_op = de.vision.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                       std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    color_adjust_op = de.vision.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4)\n",
    "    compose_map_func = (lambda img_id, image, annotation: preprocess_fn(img_id, image, annotation, is_training))\n",
    "    if is_training:\n",
    "        output_columns = [\"image\", \"box\", \"label\", \"num_match\"]\n",
    "        trans = [color_adjust_op, normalize_op, change_swap_op]\n",
    "    else:\n",
    "        output_columns = [\"img_id\", \"image\", \"image_shape\"]\n",
    "        trans = [normalize_op, change_swap_op]\n",
    "    ds = ds.map(operations=compose_map_func, input_columns=[\"img_id\", \"image\", \"annotation\"],\n",
    "                output_columns=output_columns, column_order=output_columns,\n",
    "                python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.map(operations=trans, input_columns=[\"image\"], python_multiprocessing=use_multiprocessing,\n",
    "                num_parallel_workers=num_parallel_workers)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f75e8f",
   "metadata": {},
   "source": [
    "##  2.2 模型构建\n",
    "SSD的网络结构主要分为以下几个部分：\n",
    "* VGG16 Base Layer\n",
    "* Extra Feature Layer\n",
    "+ Detection Layer\n",
    "+ NMS<br />\n",
    "+ Anchor \n",
    "\n",
    "\n",
    "**Backbone Layer**\n",
    "\n",
    "![图片](./src/img/441a293cb82c1ebecc4f67b0e03c2b05.png)  \n",
    "\n",
    "\n",
    "输入图像经过预处理后大小固定为300×300，首先经过backbone，本案例中使用的是VGG16网络的前13个卷积层，然后分别将VGG16的全连接层fc6和fc7转换成3$\\times$3卷积层block6和1$\\times$1卷积层block7，进一步提取特征。 在block6中，使用了空洞数为6的空洞卷积，其padding也为6，这样做同样也是为了增加感受野的同时保持参数量与特征图尺寸的不变。  \n",
    "\n",
    "**Extra Feature Layer**\n",
    "\n",
    "在VGG16的基础上，SSD进一步增加了4个深度卷积层，用于提取更高层的语义信息：\n",
    "![图片](./src/img/conv.png)\n",
    "block8-11，用于更高语义信息的提取。block8的通道数为512，而block9、block10与block11的通道数都为256。从block7到block11，这5个卷积后输出特征图的尺寸依次为19×19、10×10、5×5、3×3和1×1。为了降低参数量，使用了1×1卷积先降低通道数为该层输出通道数的一半，再利用3×3卷积进行特征提取。 \n",
    " \n",
    " **Detection Layer**\n",
    "![图片](./src/img/dec.jpg)\n",
    "Detection Layer首先分成cls分支和loc分支，每个分支中包含6个(因为有6个特征层)卷积层conv，conv的输出尺寸和输入尺寸相同，cls分支的输出通道数为k*class_num，loc分支的输出通道数为k*4，k表示proir box个数。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11ad0e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce3f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Anchor Generator\"\"\"\n",
    "import numpy as np\n",
    "class GridAnchorGenerator:\n",
    "    def __init__(self, image_shape, scale, scales_per_octave, aspect_ratios):\n",
    "        super(GridAnchorGenerator, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.scales_per_octave = scales_per_octave\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "\n",
    "    def generate(self, step):\n",
    "        scales = np.array([2**(float(scale) / self.scales_per_octave)\n",
    "                           for scale in range(self.scales_per_octave)]).astype(np.float32)\n",
    "        aspects = np.array(list(self.aspect_ratios)).astype(np.float32)\n",
    "\n",
    "        scales_grid, aspect_ratios_grid = np.meshgrid(scales, aspects)\n",
    "        scales_grid = scales_grid.reshape([-1])\n",
    "        aspect_ratios_grid = aspect_ratios_grid.reshape([-1])\n",
    "\n",
    "        feature_size = [self.image_shape[0] / step, self.image_shape[1] / step]\n",
    "        grid_height, grid_width = feature_size\n",
    "\n",
    "        base_size = np.array([self.scale * step, self.scale * step]).astype(np.float32)\n",
    "        anchor_offset = step / 2.0\n",
    "\n",
    "        ratio_sqrt = np.sqrt(aspect_ratios_grid)\n",
    "        heights = scales_grid / ratio_sqrt * base_size[0]\n",
    "        widths = scales_grid * ratio_sqrt * base_size[1]\n",
    "\n",
    "        y_centers = np.arange(grid_height).astype(np.float32)\n",
    "        y_centers = y_centers * step + anchor_offset\n",
    "        x_centers = np.arange(grid_width).astype(np.float32)\n",
    "        x_centers = x_centers * step + anchor_offset\n",
    "        x_centers, y_centers = np.meshgrid(x_centers, y_centers)\n",
    "\n",
    "        x_centers_shape = x_centers.shape\n",
    "        y_centers_shape = y_centers.shape\n",
    "\n",
    "        widths_grid, x_centers_grid = np.meshgrid(widths, x_centers.reshape([-1]))\n",
    "        heights_grid, y_centers_grid = np.meshgrid(heights, y_centers.reshape([-1]))\n",
    "\n",
    "        x_centers_grid = x_centers_grid.reshape(*x_centers_shape, -1)\n",
    "        y_centers_grid = y_centers_grid.reshape(*y_centers_shape, -1)\n",
    "        widths_grid = widths_grid.reshape(-1, *x_centers_shape)\n",
    "        heights_grid = heights_grid.reshape(-1, *y_centers_shape)\n",
    "\n",
    "\n",
    "        bbox_centers = np.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "        bbox_sizes = np.stack([heights_grid, widths_grid], axis=3)\n",
    "        bbox_centers = bbox_centers.reshape([-1, 2])\n",
    "        bbox_sizes = bbox_sizes.reshape([-1, 2])\n",
    "        bbox_corners = np.concatenate([bbox_centers - 0.5 * bbox_sizes, bbox_centers + 0.5 * bbox_sizes], axis=1)\n",
    "        self.bbox_corners = bbox_corners / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "        self.bbox_centers = np.concatenate([bbox_centers, bbox_sizes], axis=1)\n",
    "        self.bbox_centers = self.bbox_centers / np.array([*self.image_shape, *self.image_shape]).astype(np.float32)\n",
    "\n",
    "        print(self.bbox_centers.shape)\n",
    "        return self.bbox_centers, self.bbox_corners\n",
    "\n",
    "    def generate_multi_levels(self, steps):\n",
    "        bbox_centers_list = []\n",
    "        bbox_corners_list = []\n",
    "        for step in steps:\n",
    "            bbox_centers, bbox_corners = self.generate(step)\n",
    "            bbox_centers_list.append(bbox_centers)\n",
    "            bbox_corners_list.append(bbox_corners)\n",
    "\n",
    "        self.bbox_centers = np.concatenate(bbox_centers_list, axis=0)\n",
    "        self.bbox_corners = np.concatenate(bbox_corners_list, axis=0)\n",
    "        return self.bbox_centers, self.bbox_corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354a9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from src.vgg16 import vgg16\n",
    "import mindspore.ops as ops\n",
    "import ml_collections\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"ensures that all layers have a channel number that is divisible by 8.\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def _conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same'):\n",
    "    return nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=0, pad_mode=pad_mod, has_bias=True)\n",
    "\n",
    "\n",
    "def _bn(channel):\n",
    "    return nn.BatchNorm2d(channel, eps=1e-3, momentum=0.97,\n",
    "                          gamma_init=1, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "\n",
    "\n",
    "def _last_conv2d(in_channel, out_channel, kernel_size=3, stride=1, pad_mod='same', pad=0):\n",
    "    in_channels = in_channel\n",
    "    out_channels = in_channel\n",
    "    depthwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad_mode='same',\n",
    "                               padding=pad, group=in_channels)\n",
    "    conv = _conv2d(in_channel, out_channel, kernel_size=1)\n",
    "    return nn.SequentialCell([depthwise_conv, _bn(in_channel), nn.ReLU6(), conv])\n",
    "\n",
    "\n",
    "class FlattenConcat(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(FlattenConcat, self).__init__()\n",
    "        self.num_ssd_boxes = config.num_ssd_boxes\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "        self.transpose = ops.Transpose()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        output = ()\n",
    "        batch_size = ops.shape(inputs[0])[0]\n",
    "        for x in inputs:\n",
    "            x = self.transpose(x, (0, 2, 3, 1))\n",
    "            output += (ops.reshape(x, (batch_size, -1)),)\n",
    "        res = self.concat(output)\n",
    "        return ops.reshape(res, (batch_size, self.num_ssd_boxes, -1))\n",
    "\n",
    "\n",
    "class MultiBox(nn.Cell):\n",
    "    \"\"\"\n",
    "    Multibox conv layers. Each multibox layer contains class conf scores and localization predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MultiBox, self).__init__()\n",
    "        num_classes = 81\n",
    "        out_channels = [512, 1024, 512, 256, 256, 256]\n",
    "        num_default = config.num_default\n",
    "\n",
    "        loc_layers = []\n",
    "        cls_layers = []\n",
    "        for k, out_channel in enumerate(out_channels):\n",
    "            loc_layers += [_last_conv2d(out_channel, 4 * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "            cls_layers += [_last_conv2d(out_channel, num_classes * num_default[k],\n",
    "                                        kernel_size=3, stride=1, pad_mod='same', pad=0)]\n",
    "\n",
    "        self.multi_loc_layers = nn.layer.CellList(loc_layers)\n",
    "        self.multi_cls_layers = nn.layer.CellList(cls_layers)\n",
    "        self.flatten_concat = FlattenConcat(config)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        loc_outputs = ()\n",
    "        cls_outputs = ()\n",
    "        for i in range(len(self.multi_loc_layers)):\n",
    "            loc_outputs += (self.multi_loc_layers[i](inputs[i]),)\n",
    "            cls_outputs += (self.multi_cls_layers[i](inputs[i]),)\n",
    "        return self.flatten_concat(loc_outputs), self.flatten_concat(cls_outputs)\n",
    "\n",
    "\n",
    "class SSD300VGG16(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(SSD300VGG16, self).__init__()\n",
    "\n",
    "        # VGG16 backbone: block1~5\n",
    "        self.backbone = vgg16()\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        self.b6_1 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=6, dilation=6, pad_mode='pad')\n",
    "        self.b6_2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.b7_1 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
    "        self.b7_2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        self.b8_1 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b8_2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b9_1 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1, padding=1, pad_mode='pad')\n",
    "        self.b9_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, pad_mode='valid')\n",
    "\n",
    "        self.b10_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b10_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        self.b11_1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.b11_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, pad_mode='valid')\n",
    "\n",
    "        # boxes\n",
    "        self.multi_box = MultiBox(config)\n",
    "        if not self.training:\n",
    "            self.activation = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # VGG16 backbone: block1~5\n",
    "        block4, x = self.backbone(x)\n",
    "\n",
    "        # SSD blocks: block6~7\n",
    "        x = self.b6_1(x)  # 1024\n",
    "        x = self.b6_2(x)\n",
    "\n",
    "        x = self.b7_1(x)  # 1024\n",
    "        x = self.b7_2(x)\n",
    "        block7 = x\n",
    "\n",
    "        # Extra Feature Layers: block8~11\n",
    "        x = self.b8_1(x)  # 256\n",
    "        x = self.b8_2(x)  # 512\n",
    "        block8 = x\n",
    "\n",
    "        x = self.b9_1(x)  # 128\n",
    "        x = self.b9_2(x)  # 256\n",
    "        block9 = x\n",
    "\n",
    "        x = self.b10_1(x)  # 128\n",
    "        x = self.b10_2(x)  # 256\n",
    "        block10 = x\n",
    "\n",
    "        x = self.b11_1(x)  # 128\n",
    "        x = self.b11_2(x)  # 256\n",
    "        block11 = x\n",
    "\n",
    "        # boxes\n",
    "        multi_feature = (block4, block7, block8, block9, block10, block11)\n",
    "        pred_loc, pred_label = self.multi_box(multi_feature)\n",
    "        if not self.training:\n",
    "            pred_label = self.activation(pred_label)\n",
    "        pred_loc = ops.cast(pred_loc, ms.float32)\n",
    "        pred_label = ops.cast(pred_label, ms.float32)\n",
    "        return pred_loc, pred_label\n",
    "\n",
    "\n",
    "def ssd_vgg16(**kwargs):\n",
    "    return SSD300VGG16(**kwargs)\n",
    "\n",
    "\n",
    "# ssd = ssd_vgg16(config=config)\n",
    "# print(ssd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a19ba3",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "损失函数定义为位置误差（locatization loss， loc）与置信度误差（confidence loss, conf）的加权和：\n",
    "![图片](./src/img/loss.png)  \n",
    "其中：<br />\n",
    "N 是先验框的正样本数量；<br /> \n",
    "c 为类别置信度预测值; <br />\n",
    "l 为先验框的所对应边界框的位置预测值; <br />\n",
    "g 为ground truth的位置参数  <br />\n",
    "\n",
    "**对于位置损失函数：**\n",
    "针对所有的正样本，采用 Smooth L1 Loss, 位置信息都是 encode 之后的位置信息。\n",
    "![图片](./src/img/smooth.png)  \n",
    "**对于置信度损失函数：**\n",
    "置信度损失是多类置信度(c)上的softmax损失\n",
    "![图片](./src/img/conf.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72446fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "grad_scale = ops.MultitypeFuncGraph(\"grad_scale\")\n",
    "\n",
    "class SigmoidFocalClassificationLoss(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Sigmoid focal-loss for classification.\n",
    "    Args:\n",
    "        gamma (float): Hyper-parameter to balance the easy and hard examples. Default: 2.0\n",
    "        alpha (float): Hyper-parameter to balance the positive and negative example. Default: 0.25\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(SigmoidFocalClassificationLoss, self).__init__()\n",
    "        self.sigmiod_cross_entropy = ops.SigmoidCrossEntropyWithLogits()\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.pow = ops.Pow()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.on_value = Tensor(1.0, ms.float32)\n",
    "        self.off_value = Tensor(0.0, ms.float32)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label = self.onehot(label, ops.shape(logits)[-1], self.on_value, self.off_value)\n",
    "        sigmiod_cross_entropy = self.sigmiod_cross_entropy(logits, label)\n",
    "        sigmoid = self.sigmoid(logits)\n",
    "        label = ops.cast(label, ms.float32)\n",
    "        p_t = label * sigmoid + (1 - label) * (1 - sigmoid)\n",
    "        modulating_factor = self.pow(1 - p_t, self.gamma)\n",
    "        alpha_weight_factor = label * self.alpha + (1 - label) * (1 - self.alpha)\n",
    "        focal_loss = modulating_factor * alpha_weight_factor * sigmiod_cross_entropy\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class SSDWithLossCell(nn.Cell):\n",
    "    \"\"\"\"\n",
    "    Provide SSD training loss through network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        super(SSDWithLossCell, self).__init__()\n",
    "        self.network = network\n",
    "        self.less = ops.Less()\n",
    "        self.tile = ops.Tile()\n",
    "        self.reduce_sum = ops.ReduceSum()\n",
    "        self.expand_dims = ops.ExpandDims()\n",
    "        self.class_loss = SigmoidFocalClassificationLoss(config.gamma, config.alpha)\n",
    "        self.loc_loss = nn.SmoothL1Loss()\n",
    "\n",
    "    def construct(self, x, gt_loc, gt_label, num_matched_boxes):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "        mask = ops.cast(self.less(0, gt_label), ms.float32)\n",
    "        num_matched_boxes = self.reduce_sum(ops.cast(num_matched_boxes, ms.float32))\n",
    "        # 定位损失\n",
    "        mask_loc = self.tile(self.expand_dims(mask, -1), (1, 1, 4))\n",
    "        smooth_l1 = self.loc_loss(pred_loc, gt_loc) * mask_loc\n",
    "        loss_loc = self.reduce_sum(self.reduce_sum(smooth_l1, -1), -1)\n",
    "\n",
    "        # 类别损失\n",
    "        loss_cls = self.class_loss(pred_label, gt_label)\n",
    "        loss_cls = self.reduce_sum(loss_cls, (1, 2))\n",
    "\n",
    "        return self.reduce_sum((loss_cls + loss_loc) / num_matched_boxes)\n",
    "\n",
    "# net = SSDWithLossCell(ssd, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b2cc1",
   "metadata": {},
   "source": [
    "### Metric\n",
    "在SSD中，训练过程是不需要用到非极大值抑制(NMS)，但当进行检测时，例如输入一张图片要求输出框的时候，需要用到NMS过滤掉那些重叠度较大的预测框。<br />\n",
    "非极大值抑制的流程如下：\n",
    "1. 根据置信度得分进行排序\n",
    "2. 选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除<br />\n",
    "3. 计算所有边界框的面积<br />\n",
    "4. 计算置信度最高的边界框与其它候选框的IoU<br />\n",
    "5. 删除IoU大于阈值的边界框<br />\n",
    "6. 重复上述过程，直至边界框列表为空<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590d1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore.train.callback import Callback\n",
    "import json\n",
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "\n",
    "def apply_eval(eval_param_dict):\n",
    "    net = eval_param_dict[\"net\"]\n",
    "    net.set_train(False)\n",
    "    ds = eval_param_dict[\"dataset\"]\n",
    "    anno_json = eval_param_dict[\"anno_json\"]\n",
    "    coco_metrics = COCOMetrics(anno_json=anno_json,\n",
    "                               classes=config.classes,\n",
    "                               num_classes=config.num_classes,\n",
    "                               max_boxes=config.max_boxes,\n",
    "                               nms_threshold=config.nms_threshold,\n",
    "                               min_score=config.min_score)\n",
    "    for data in ds.create_dict_iterator(output_numpy=True, num_epochs=1):\n",
    "        img_id = data['img_id']\n",
    "        img_np = data['image']\n",
    "        image_shape = data['image_shape']\n",
    "\n",
    "        output = net(Tensor(img_np))\n",
    "\n",
    "        for batch_idx in range(img_np.shape[0]):\n",
    "            pred_batch = {\n",
    "                \"boxes\": output[0].asnumpy()[batch_idx],\n",
    "                \"box_scores\": output[1].asnumpy()[batch_idx],\n",
    "                \"img_id\": int(np.squeeze(img_id[batch_idx])),\n",
    "                \"image_shape\": image_shape[batch_idx]\n",
    "            }\n",
    "            coco_metrics.update(pred_batch)\n",
    "    eval_metrics = coco_metrics.get_metrics()\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "def apply_nms(all_boxes, all_scores, thres, max_boxes):\n",
    "    \"\"\"Apply NMS to bboxes.\"\"\"\n",
    "    y1 = all_boxes[:, 0]\n",
    "    x1 = all_boxes[:, 1]\n",
    "    y2 = all_boxes[:, 2]\n",
    "    x2 = all_boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    order = all_scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        if len(keep) >= max_boxes:\n",
    "            break\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thres)[0]\n",
    "\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "\n",
    "class COCOMetrics:\n",
    "    \"\"\"Calculate mAP of predicted bboxes.\"\"\"\n",
    "\n",
    "    def __init__(self, anno_json, classes, num_classes, min_score, nms_threshold, max_boxes):\n",
    "        self.num_classes = num_classes\n",
    "        self.classes = classes\n",
    "        self.min_score = min_score\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.max_boxes = max_boxes\n",
    "\n",
    "        self.val_cls_dict = {i: cls for i, cls in enumerate(classes)}\n",
    "        self.coco_gt = COCO(anno_json)\n",
    "        cat_ids = self.coco_gt.loadCats(self.coco_gt.getCatIds())\n",
    "        self.class_dict = {cat['name']: cat['id'] for cat in cat_ids}\n",
    "\n",
    "        self.predictions = []\n",
    "        self.img_ids = []\n",
    "\n",
    "    def update(self, batch):\n",
    "        pred_boxes = batch['boxes']\n",
    "        box_scores = batch['box_scores']\n",
    "        img_id = batch['img_id']\n",
    "        h, w = batch['image_shape']\n",
    "\n",
    "        final_boxes = []\n",
    "        final_label = []\n",
    "        final_score = []\n",
    "        self.img_ids.append(img_id)\n",
    "\n",
    "        for c in range(1, self.num_classes):\n",
    "            class_box_scores = box_scores[:, c]\n",
    "            score_mask = class_box_scores > self.min_score\n",
    "            class_box_scores = class_box_scores[score_mask]\n",
    "            class_boxes = pred_boxes[score_mask] * [h, w, h, w]\n",
    "\n",
    "            if score_mask.any():\n",
    "                nms_index = apply_nms(class_boxes, class_box_scores, self.nms_threshold, self.max_boxes)\n",
    "                class_boxes = class_boxes[nms_index]\n",
    "                class_box_scores = class_box_scores[nms_index]\n",
    "\n",
    "                final_boxes += class_boxes.tolist()\n",
    "                final_score += class_box_scores.tolist()\n",
    "                final_label += [self.class_dict[self.val_cls_dict[c]]] * len(class_box_scores)\n",
    "\n",
    "        for loc, label, score in zip(final_boxes, final_label, final_score):\n",
    "            res = {}\n",
    "            res['image_id'] = img_id\n",
    "            res['bbox'] = [loc[1], loc[0], loc[3] - loc[1], loc[2] - loc[0]]\n",
    "            res['score'] = score\n",
    "            res['category_id'] = label\n",
    "            self.predictions.append(res)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        with open('predictions.json', 'w') as f:\n",
    "            json.dump(self.predictions, f)\n",
    "\n",
    "        coco_dt = self.coco_gt.loadRes('predictions.json')\n",
    "        E = COCOeval(self.coco_gt, coco_dt, iouType='bbox')\n",
    "        E.params.imgIds = self.img_ids\n",
    "        E.evaluate()\n",
    "        E.accumulate()\n",
    "        E.summarize()\n",
    "        return E.stats[0]\n",
    "\n",
    "\n",
    "class SsdInferWithDecoder(nn.Cell):\n",
    "    \"\"\"\n",
    "    SSD Infer wrapper to decode the bbox locations.\n",
    "    Args:\n",
    "        network (Cell): the origin ssd infer network without bbox decoder.\n",
    "        default_boxes (Tensor): the default_boxes from anchor generator\n",
    "        config (dict): ssd config\n",
    "    Returns:\n",
    "        Tensor, the locations for bbox after decoder representing (y0,x0,y1,x1)\n",
    "        Tensor, the prediction labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, default_boxes, config):\n",
    "        super(SsdInferWithDecoder, self).__init__()\n",
    "        self.network = network\n",
    "        self.default_boxes = default_boxes\n",
    "        self.prior_scaling_xy = config.prior_scaling[0]\n",
    "        self.prior_scaling_wh = config.prior_scaling[1]\n",
    "\n",
    "    def construct(self, x):\n",
    "        pred_loc, pred_label = self.network(x)\n",
    "\n",
    "        default_bbox_xy = self.default_boxes[..., :2]\n",
    "        default_bbox_wh = self.default_boxes[..., 2:]\n",
    "        pred_xy = pred_loc[..., :2] * self.prior_scaling_xy * default_bbox_wh + default_bbox_xy\n",
    "        pred_wh = ops.Exp()(pred_loc[..., 2:] * self.prior_scaling_wh) * default_bbox_wh\n",
    "\n",
    "        pred_xy_0 = pred_xy - pred_wh / 2.0\n",
    "        pred_xy_1 = pred_xy + pred_wh / 2.0\n",
    "        pred_xy = ops.Concat(-1)((pred_xy_0, pred_xy_1))\n",
    "        pred_xy = ops.Maximum()(pred_xy, 0)\n",
    "        pred_xy = ops.Minimum()(pred_xy, 1)\n",
    "        return pred_xy, pred_label\n",
    "\n",
    "\n",
    "class EvalCallBack(Callback):\n",
    "    \"\"\"\n",
    "    Evaluation callback when training.\n",
    "\n",
    "    Args:\n",
    "        eval_function (function): evaluation function.\n",
    "        eval_param_dict (dict): evaluation parameters' configure dict.\n",
    "        interval (int): run evaluation interval, default is 1.\n",
    "        eval_start_epoch (int): evaluation start epoch, default is 1.\n",
    "        save_best_ckpt (bool): Whether to save best checkpoint, default is True.\n",
    "        besk_ckpt_name (str): bast checkpoint name, default is `best.ckpt`.\n",
    "        metrics_name (str): evaluation metrics name, default is `acc`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_function, eval_param_dict, interval=1, eval_start_epoch=1,\n",
    "                 ckpt_directory=\"./\", besk_ckpt_name=\"best.ckpt\", metrics_name=\"acc\"):\n",
    "        super(EvalCallBack, self).__init__()\n",
    "        self.eval_param_dict = eval_param_dict\n",
    "        self.eval_function = eval_function\n",
    "        self.eval_start_epoch = eval_start_epoch\n",
    "        self.interval = interval\n",
    "        self.best_res = 0\n",
    "        self.best_epoch = 0\n",
    "        if not os.path.isdir(ckpt_directory):\n",
    "            os.makedirs(ckpt_directory)\n",
    "        self.best_ckpt_path = os.path.join(ckpt_directory, besk_ckpt_name)\n",
    "        self.metrics_name = metrics_name\n",
    "\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"Callback when epoch end.\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        if cur_epoch >= self.eval_start_epoch and (cur_epoch - self.eval_start_epoch) % self.interval == 0:\n",
    "            res = self.eval_function(self.eval_param_dict)\n",
    "            print(\"epoch: {}, {}: {}\".format(cur_epoch, self.metrics_name, res), flush=True)\n",
    "            if res >= self.best_res:\n",
    "                self.best_res = res\n",
    "                self.best_epoch = cur_epoch\n",
    "                print(\"update best result: {}\".format(res), flush=True)\n",
    "                save_checkpoint(cb_params.train_network, self.best_ckpt_path)\n",
    "                print(\"update best checkpoint at: {}\".format(self.best_ckpt_path), flush=True)\n",
    "\n",
    "    def end(self, run_context):\n",
    "        print(\"End training, the best {0} is: {1}, the best {0} epoch is {2}\".format(self.metrics_name,\n",
    "                                                                                     self.best_res,\n",
    "                                                                                     self.best_epoch), flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e45a95",
   "metadata": {},
   "source": [
    "### 训练过程\n",
    "**先验框匹配**\n",
    "\n",
    "在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。\n",
    "\n",
    "SSD的先验框与ground truth的匹配原则主要有两点：\n",
    "1. 对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样可以保证每个ground truth一定与某个先验框匹配。通常称与ground truth匹配的先验框为正样本，反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。\n",
    "2. 对于剩余的未匹配先验框，若某个ground truth的IOU大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配。  \n",
    "尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。\n",
    "\n",
    "注意点：\n",
    "1. 通常称与gt匹配的prior为正样本，反之，若某一个prior没有与任何一个gt匹配，则为负样本。\n",
    "2. 某个gt可以和多个prior匹配，而每个prior只能和一个gt进行匹配。\n",
    "3. 如果多个gt和某一个prior的IOU均大于阈值，那么prior只与IOU最大的那个进行匹配  \n",
    "\n",
    "在模型训练时，首先是设置模型训练的epoch次数为60，然后读取转化为mindrecord格式的训练集数据。训练集batch_size大小为32，图像尺寸统一调整为300×300；损失函数使用BCELoss，优化器使用Adam，并设置初始学习率为0.001。回调函数方面使用了LossMonitor和TimeMonitor来监控训练过程中每个epoch结束后，损失值Loss的变化情况以及每个epoch、每个step的运行时间。设置每训练10个epoch保存一次模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c5f05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 9, loss is 115.22297668457031\n",
      "Train epoch time: 120289.073 ms, per step time: 13365.453 ms\n",
      "epoch: 2 step: 9, loss is 46.78498840332031\n",
      "Train epoch time: 1464.132 ms, per step time: 162.681 ms\n",
      "epoch: 3 step: 9, loss is 40.630332946777344\n",
      "Train epoch time: 1478.894 ms, per step time: 164.322 ms\n",
      "epoch: 4 step: 9, loss is 72.6512451171875\n",
      "Train epoch time: 1474.117 ms, per step time: 163.791 ms\n",
      "epoch: 5 step: 9, loss is 106.53961181640625\n",
      "Train epoch time: 1747.797 ms, per step time: 194.200 ms\n",
      "epoch: 6 step: 9, loss is 45.731834411621094\n",
      "Train epoch time: 1832.906 ms, per step time: 203.656 ms\n",
      "epoch: 7 step: 9, loss is 22.35395050048828\n",
      "Train epoch time: 1583.116 ms, per step time: 175.902 ms\n",
      "epoch: 8 step: 9, loss is 29.20132827758789\n",
      "Train epoch time: 1546.572 ms, per step time: 171.841 ms\n",
      "epoch: 9 step: 9, loss is 29.154678344726562\n",
      "Train epoch time: 1525.192 ms, per step time: 169.466 ms\n",
      "epoch: 10 step: 9, loss is 31.667152404785156\n",
      "Train epoch time: 1922.388 ms, per step time: 213.599 ms\n",
      "epoch: 11 step: 9, loss is 18.35234832763672\n",
      "Train epoch time: 1466.233 ms, per step time: 162.915 ms\n",
      "epoch: 12 step: 9, loss is 26.685890197753906\n",
      "Train epoch time: 1680.990 ms, per step time: 186.777 ms\n",
      "epoch: 13 step: 9, loss is 22.46505355834961\n",
      "Train epoch time: 1760.242 ms, per step time: 195.582 ms\n",
      "epoch: 14 step: 9, loss is 23.825969696044922\n",
      "Train epoch time: 1603.977 ms, per step time: 178.220 ms\n",
      "epoch: 15 step: 9, loss is 7.595627784729004\n",
      "Train epoch time: 1550.208 ms, per step time: 172.245 ms\n",
      "epoch: 16 step: 9, loss is 8.191256523132324\n",
      "Train epoch time: 1516.945 ms, per step time: 168.549 ms\n",
      "epoch: 17 step: 9, loss is 9.939350128173828\n",
      "Train epoch time: 1647.524 ms, per step time: 183.058 ms\n",
      "epoch: 18 step: 9, loss is 7.230021953582764\n",
      "Train epoch time: 1518.031 ms, per step time: 168.670 ms\n",
      "epoch: 19 step: 9, loss is 7.569142818450928\n",
      "Train epoch time: 1467.583 ms, per step time: 163.065 ms\n",
      "epoch: 20 step: 9, loss is 8.624747276306152\n",
      "Train epoch time: 1783.915 ms, per step time: 198.213 ms\n",
      "epoch: 21 step: 9, loss is 6.65357780456543\n",
      "Train epoch time: 1531.836 ms, per step time: 170.204 ms\n",
      "epoch: 22 step: 9, loss is 7.323695182800293\n",
      "Train epoch time: 1619.333 ms, per step time: 179.926 ms\n",
      "epoch: 23 step: 9, loss is 6.9050493240356445\n",
      "Train epoch time: 1496.302 ms, per step time: 166.256 ms\n",
      "epoch: 24 step: 9, loss is 6.693961143493652\n",
      "Train epoch time: 1492.344 ms, per step time: 165.816 ms\n",
      "epoch: 25 step: 9, loss is 6.994884490966797\n",
      "Train epoch time: 1534.629 ms, per step time: 170.514 ms\n",
      "epoch: 26 step: 9, loss is 7.7560882568359375\n",
      "Train epoch time: 1557.064 ms, per step time: 173.007 ms\n",
      "epoch: 27 step: 9, loss is 7.345478534698486\n",
      "Train epoch time: 1491.425 ms, per step time: 165.714 ms\n",
      "epoch: 28 step: 9, loss is 6.288608551025391\n",
      "Train epoch time: 1514.164 ms, per step time: 168.240 ms\n",
      "epoch: 29 step: 9, loss is 6.554647922515869\n",
      "Train epoch time: 1503.884 ms, per step time: 167.098 ms\n",
      "epoch: 30 step: 9, loss is 7.3065290451049805\n",
      "Train epoch time: 1803.971 ms, per step time: 200.441 ms\n",
      "epoch: 31 step: 9, loss is 8.516347885131836\n",
      "Train epoch time: 1480.057 ms, per step time: 164.451 ms\n",
      "epoch: 32 step: 9, loss is 5.713535308837891\n",
      "Train epoch time: 1486.977 ms, per step time: 165.220 ms\n",
      "epoch: 33 step: 9, loss is 6.153460502624512\n",
      "Train epoch time: 1478.275 ms, per step time: 164.253 ms\n",
      "epoch: 34 step: 9, loss is 6.598751068115234\n",
      "Train epoch time: 1493.444 ms, per step time: 165.938 ms\n",
      "epoch: 35 step: 9, loss is 6.109360694885254\n",
      "Train epoch time: 1479.689 ms, per step time: 164.410 ms\n",
      "epoch: 36 step: 9, loss is 4.9675116539001465\n",
      "Train epoch time: 1484.303 ms, per step time: 164.923 ms\n",
      "epoch: 37 step: 9, loss is 5.276495933532715\n",
      "Train epoch time: 1485.836 ms, per step time: 165.093 ms\n",
      "epoch: 38 step: 9, loss is 5.145185470581055\n",
      "Train epoch time: 1481.297 ms, per step time: 164.589 ms\n",
      "epoch: 39 step: 9, loss is 5.230655670166016\n",
      "Train epoch time: 1523.667 ms, per step time: 169.296 ms\n",
      "epoch: 40 step: 9, loss is 6.3112006187438965\n",
      "Train epoch time: 1823.756 ms, per step time: 202.640 ms\n",
      "epoch: 41 step: 9, loss is 5.643704891204834\n",
      "Train epoch time: 1495.631 ms, per step time: 166.181 ms\n",
      "epoch: 42 step: 9, loss is 5.99526834487915\n",
      "Train epoch time: 1559.393 ms, per step time: 173.266 ms\n",
      "epoch: 43 step: 9, loss is 4.8811469078063965\n",
      "Train epoch time: 1528.668 ms, per step time: 169.852 ms\n",
      "epoch: 44 step: 9, loss is 6.634915351867676\n",
      "Train epoch time: 1502.316 ms, per step time: 166.924 ms\n",
      "epoch: 45 step: 9, loss is 5.179366111755371\n",
      "Train epoch time: 1496.346 ms, per step time: 166.261 ms\n",
      "epoch: 46 step: 9, loss is 5.171003818511963\n",
      "Train epoch time: 1507.815 ms, per step time: 167.535 ms\n",
      "epoch: 47 step: 9, loss is 6.688290596008301\n",
      "Train epoch time: 1493.966 ms, per step time: 165.996 ms\n",
      "epoch: 48 step: 9, loss is 4.892411231994629\n",
      "Train epoch time: 1506.347 ms, per step time: 167.372 ms\n",
      "epoch: 49 step: 9, loss is 5.7277727127075195\n",
      "Train epoch time: 1515.306 ms, per step time: 168.367 ms\n",
      "epoch: 50 step: 9, loss is 5.062741279602051\n",
      "Train epoch time: 1857.396 ms, per step time: 206.377 ms\n",
      "epoch: 51 step: 9, loss is 5.477893829345703\n",
      "Train epoch time: 1487.955 ms, per step time: 165.328 ms\n",
      "epoch: 52 step: 9, loss is 5.7479658126831055\n",
      "Train epoch time: 1494.661 ms, per step time: 166.073 ms\n",
      "epoch: 53 step: 9, loss is 6.153087139129639\n",
      "Train epoch time: 1499.030 ms, per step time: 166.559 ms\n",
      "epoch: 54 step: 9, loss is 6.3101372718811035\n",
      "Train epoch time: 1512.343 ms, per step time: 168.038 ms\n",
      "epoch: 55 step: 9, loss is 6.674015998840332\n",
      "Train epoch time: 1510.218 ms, per step time: 167.802 ms\n",
      "epoch: 56 step: 9, loss is 6.25616979598999\n",
      "Train epoch time: 1510.914 ms, per step time: 167.879 ms\n",
      "epoch: 57 step: 9, loss is 4.876305103302002\n",
      "Train epoch time: 1510.185 ms, per step time: 167.798 ms\n",
      "epoch: 58 step: 9, loss is 5.240882396697998\n",
      "Train epoch time: 1512.829 ms, per step time: 168.092 ms\n",
      "epoch: 59 step: 9, loss is 4.57403039932251\n",
      "Train epoch time: 1545.766 ms, per step time: 171.752 ms\n",
      "epoch: 60 step: 9, loss is 4.942201614379883\n",
      "Train epoch time: 1896.290 ms, per step time: 210.699 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mindspore.train import Model\n",
    "from src.config import get_config\n",
    "import mindspore as ms\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, LossMonitor, TimeMonitor\n",
    "from src.init_params import init_net_param\n",
    "from src.lr_schedule import get_lr\n",
    "from mindspore.common import set_seed\n",
    "from src.box_utils import default_boxes\n",
    "\n",
    "\n",
    "class TrainingWrapper(nn.Cell):\n",
    "\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(TrainingWrapper, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = ms.ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "\n",
    "    def construct(self, *args):\n",
    "        weights = self.weights\n",
    "        loss = self.network(*args)\n",
    "        sens = ops.Fill()(ops.DType()(loss), ops.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(*args, sens)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "\n",
    "set_seed(1)\n",
    "# 自定义参数获取\n",
    "config = get_config()\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target= \"GPU\")\n",
    "\n",
    "# 数据加载\n",
    "mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "mindrecord_file = os.path.join(mindrecord_dir, \"ssd.mindrecord\"+ \"0\")\n",
    "\n",
    "dataset = create_ssd_dataset(mindrecord_file, batch_size=config.batch_size,rank=0, use_multiprocessing=True)\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "\n",
    "# checkpoint\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=dataset_size * config.save_checkpoint_epochs)\n",
    "ckpt_save_dir = config.output_path + '/ckpt_{}/'.format(0)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"ssd\", directory=ckpt_save_dir, config=ckpt_config)\n",
    "\n",
    "# 网络定义与初始化\n",
    "ssd = ssd_vgg16(config=config)\n",
    "init_net_param(ssd)\n",
    "# print(ssd)\n",
    "net = SSDWithLossCell(ssd, config)\n",
    "# print(net)\n",
    "lr = Tensor(get_lr(global_step=config.pre_trained_epoch_size * dataset_size,\n",
    "                   lr_init=config.lr_init, lr_end=config.lr_end_rate * config.lr, lr_max=config.lr,\n",
    "                   warmup_epochs=config.warmup_epochs,total_epochs=config.epoch_size,steps_per_epoch=dataset_size))\n",
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr,\n",
    "                  config.momentum, config.weight_decay,float(config.loss_scale))\n",
    "net = TrainingWrapper(net, opt, float(config.loss_scale))\n",
    "\n",
    "callback = [TimeMonitor(data_size=dataset_size), LossMonitor(), ckpoint_cb]\n",
    "\n",
    "model = Model(net)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "model.train(config.epoch_size, dataset, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f4d52",
   "metadata": {},
   "source": [
    "### 评估  \n",
    "#### 预测过程  \n",
    "对于每个预测框，首先根据类别置信度确定其类别（置信度最大者）与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值（如0.5）过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数（解码后一般还需要做clip，防止预测框位置超出图片）。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k个预测框。最后再通过NMS算法得到预测框即检测结果。  \n",
    "\n",
    "实例化自定义的回调类EvalCallBack，实现计算每个epoch结束后的评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "from src.box_utils import default_boxes\n",
    "from src.config import get_config\n",
    "\n",
    "config = get_config()\n",
    "#print(config)\n",
    "\n",
    "def ssd_eval(dataset_path, ckpt_path, anno_json):\n",
    "    \"\"\"SSD evaluation.\"\"\"\n",
    "    batch_size = 1\n",
    "    ds = create_ssd_dataset(dataset_path, batch_size=batch_size,\n",
    "                            is_training=False, use_multiprocessing=False)\n",
    "\n",
    "    net = ssd_vgg16(config=config)\n",
    "    \n",
    "    net = SsdInferWithDecoder(net, Tensor(default_boxes), config)\n",
    "\n",
    "    print(\"Load Checkpoint!\")\n",
    "    param_dict = ms.load_checkpoint(ckpt_path)\n",
    "    net.init_parameters_data()\n",
    "    ms.load_param_into_net(net, param_dict)\n",
    "\n",
    "    net.set_train(False)\n",
    "    total = ds.get_dataset_size() * batch_size\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(\"total images num: \", total)\n",
    "    print(\"Processing, please wait a moment.\")\n",
    "    eval_param_dict = {\"net\": net, \"dataset\": ds, \"anno_json\": anno_json}\n",
    "    mAP = apply_eval(eval_param_dict)\n",
    "    print(\"\\n========================================\\n\")\n",
    "    print(f\"mAP: {mAP}\")\n",
    "\n",
    "def eval_net():\n",
    "    if hasattr(config, 'num_ssd_boxes') and config.num_ssd_boxes == -1:\n",
    "        num = 0\n",
    "        h, w = config.img_shape\n",
    "        for i in range(len(config.steps)):\n",
    "            num += (h // config.steps[i]) * (w // config.steps[i]) * config.num_default[i]\n",
    "        config.num_ssd_boxes = num\n",
    "\n",
    "    coco_root = config.coco_root\n",
    "    json_path = os.path.join(coco_root, config.instances_set.format(config.val_data_type))\n",
    "\n",
    "    ms.set_context(mode=ms.GRAPH_MODE, device_target=config.device_target)\n",
    "    \n",
    "    mindrecord_dir = os.path.join(config.data_path, config.mindrecord_dir)\n",
    "    mindrecord_file = os.path.join(mindrecord_dir, \"ssd_eval.mindrecord\"+ \"0\")\n",
    "\n",
    "    #mindrecord_file = create_mindrecord(config.dataset, \"ssd_eval.mindrecord\", False)\n",
    "\n",
    "    print(\"Start Eval!\")\n",
    "    ssd_eval(mindrecord_file, config.checkpoint_file_path, json_path)\n",
    "    \n",
    "eval_net()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
